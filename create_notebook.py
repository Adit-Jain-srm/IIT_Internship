#!/usr/bin/env python3
"""Create a clean, properly ordered notebook for submission."""

import json

notebook = {
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GMM Temperature Classification - Cold_normal vs Hot\n",
                "\n",
                "This notebook implements:\n",
                "- Loading sensor data from Readings/Cold_normal and Readings/Hot folders\n",
                "- Statistical feature engineering (mean, std, min, max, percentiles, etc.)\n",
                "- Train-test split (70/30) for evaluation\n",
                "- Feature selection (4-12 best features using Mutual Information)\n",
                "- GMM model training and evaluation\n",
                "- Visualizations and performance metrics"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 1. Imports"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "from sklearn.mixture import GaussianMixture\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score, precision_score, recall_score, f1_score,\n",
                "    confusion_matrix, classification_report\n",
                ")\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.feature_selection import mutual_info_classif, SelectKBest, f_classif\n",
                "import pickle\n",
                "from datetime import datetime\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set plotting style\n",
                "try:\n",
                "    plt.style.use('seaborn-v0_8')\n",
                "except:\n",
                "    plt.style.use('seaborn')\n",
                "sns.set_palette('husl')\n",
                "%matplotlib inline\n",
                "\n",
                "print('All imports successful!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 2. Data Loading"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_all_data(data_dir=None):\n",
                "    \"\"\"Load all sensor data from Cold_normal and Hot folders.\"\"\"\n",
                "    if data_dir is None:\n",
                "        current_dir = Path('.')\n",
                "        if (current_dir / 'Cold_normal').exists() and (current_dir / 'Hot').exists():\n",
                "            data_path = current_dir\n",
                "        elif (current_dir / 'Readings' / 'Cold_normal').exists():\n",
                "            data_path = current_dir / 'Readings'\n",
                "        else:\n",
                "            import os\n",
                "            cwd = Path(os.getcwd())\n",
                "            if (cwd / 'Readings' / 'Cold_normal').exists():\n",
                "                data_path = cwd / 'Readings'\n",
                "            else:\n",
                "                data_path = Path('Readings')\n",
                "    else:\n",
                "        data_path = Path(data_dir)\n",
                "    \n",
                "    X_file_level = []\n",
                "    y_file_level = []\n",
                "    \n",
                "    print(f'Searching for data in: {data_path.absolute()}')\n",
                "    \n",
                "    # Load Cold_normal data\n",
                "    cold_folder = data_path / 'Cold_normal'\n",
                "    if cold_folder.exists():\n",
                "        csv_files = sorted(cold_folder.glob('raw_sensor_log*.csv'))\n",
                "        print(f'Loading {len(csv_files)} files from Cold_normal...')\n",
                "        for csv_file in csv_files:\n",
                "            try:\n",
                "                df = pd.read_csv(csv_file)\n",
                "                sensor_data = df[['Sensor_1', 'Sensor_2', 'Sensor_3', 'Sensor_4']].values.astype(float)\n",
                "                X_file_level.append(sensor_data)\n",
                "                y_file_level.append('Cold_normal')\n",
                "            except Exception as e:\n",
                "                print(f'  Error loading {csv_file.name}: {e}')\n",
                "    \n",
                "    # Load Hot data\n",
                "    hot_folder = data_path / 'Hot'\n",
                "    if hot_folder.exists():\n",
                "        csv_files = sorted(hot_folder.glob('raw_sensor_log*.csv'))\n",
                "        print(f'Loading {len(csv_files)} files from Hot...')\n",
                "        for csv_file in csv_files:\n",
                "            try:\n",
                "                df = pd.read_csv(csv_file)\n",
                "                sensor_data = df[['Sensor_1', 'Sensor_2', 'Sensor_3', 'Sensor_4']].values.astype(float)\n",
                "                X_file_level.append(sensor_data)\n",
                "                y_file_level.append('Hot')\n",
                "            except Exception as e:\n",
                "                print(f'  Error loading {csv_file.name}: {e}')\n",
                "    \n",
                "    if not X_file_level:\n",
                "        print('Error: No data files found.')\n",
                "        return None, None\n",
                "    \n",
                "    y = np.array(y_file_level)\n",
                "    return X_file_level, y\n",
                "\n",
                "# Load data\n",
                "X_file_data, y = load_all_data()\n",
                "\n",
                "if X_file_data is not None:\n",
                "    print(f'\\nTotal files: {len(X_file_data)}')\n",
                "    print(f'Rows per file: {[len(x) for x in X_file_data[:3]]}... (showing first 3)')\n",
                "    print(f'\\nClass distribution:')\n",
                "    unique, counts = np.unique(y, return_counts=True)\n",
                "    for cls, cnt in zip(unique, counts):\n",
                "        print(f'  {cls}: {cnt} files ({cnt/len(y)*100:.1f}%)')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Feature Engineering\n",
                "\n",
                "Aggregate all rows from each file into statistical features:\n",
                "- Per-sensor: mean, std, min, max (16 features)\n",
                "- Global: mean, std, variance, min, max, range, median, Q25, Q75, IQR, CV, skewness, kurtosis (13 features)\n",
                "- **Total: 29 features per file**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def aggregate_file_to_features(file_sensor_data):\n",
                "    \"\"\"Aggregate all rows from a file into statistical features.\"\"\"\n",
                "    X = np.array(file_sensor_data)\n",
                "    if X.ndim == 1:\n",
                "        X = X.reshape(1, -1)\n",
                "    \n",
                "    # Per-sensor statistics\n",
                "    sensor_means = X.mean(axis=0)\n",
                "    sensor_stds = X.std(axis=0)\n",
                "    sensor_maxs = X.max(axis=0)\n",
                "    sensor_mins = X.min(axis=0)\n",
                "    \n",
                "    # Global statistics\n",
                "    all_values = X.flatten()\n",
                "    global_mean = all_values.mean()\n",
                "    global_std = all_values.std()\n",
                "    global_var = all_values.var()\n",
                "    global_max = all_values.max()\n",
                "    global_min = all_values.min()\n",
                "    global_range = global_max - global_min\n",
                "    global_median = np.median(all_values)\n",
                "    \n",
                "    # Percentiles\n",
                "    q25 = np.percentile(all_values, 25)\n",
                "    q75 = np.percentile(all_values, 75)\n",
                "    iqr = q75 - q25\n",
                "    \n",
                "    # Higher moments\n",
                "    mean_centered = all_values - global_mean\n",
                "    global_skew = (mean_centered ** 3).mean() / (global_std ** 3 + 1e-8)\n",
                "    global_kurtosis = (mean_centered ** 4).mean() / (global_std ** 4 + 1e-8)\n",
                "    cv = global_std / (global_mean + 1e-8)\n",
                "    \n",
                "    # Combine all features (29 total)\n",
                "    features = np.concatenate([\n",
                "        sensor_means, sensor_stds, sensor_maxs, sensor_mins,\n",
                "        [global_mean, global_std, global_var, global_max, global_min, global_range,\n",
                "         global_median, q25, q75, iqr, cv, global_skew, global_kurtosis]\n",
                "    ])\n",
                "    return features\n",
                "\n",
                "# Feature names\n",
                "feature_names = [\n",
                "    'sensor_1_mean', 'sensor_2_mean', 'sensor_3_mean', 'sensor_4_mean',\n",
                "    'sensor_1_std', 'sensor_2_std', 'sensor_3_std', 'sensor_4_std',\n",
                "    'sensor_1_max', 'sensor_2_max', 'sensor_3_max', 'sensor_4_max',\n",
                "    'sensor_1_min', 'sensor_2_min', 'sensor_3_min', 'sensor_4_min',\n",
                "    'global_mean', 'global_std', 'global_var', 'global_max', 'global_min',\n",
                "    'global_range', 'global_median', 'q25', 'q75', 'iqr',\n",
                "    'cv', 'global_skew', 'global_kurtosis'\n",
                "]\n",
                "\n",
                "# Convert file-level data to feature vectors\n",
                "print('Aggregating file-level features...')\n",
                "X_features = [aggregate_file_to_features(file_data) for file_data in X_file_data]\n",
                "X = np.array(X_features)\n",
                "print(f'\\nFeature matrix shape: {X.shape} (files x features)')\n",
                "print(f'Number of features per file: {X.shape[1]}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 4. Train-Test Split"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Stratified split to maintain class balance\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.3, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "print(f'Training set: {len(X_train)} files ({len(X_train)/len(X)*100:.1f}%)')\n",
                "print(f'Test set: {len(X_test)} files ({len(X_test)/len(X)*100:.1f}%)')\n",
                "print(f'Features per file: {X_train.shape[1]}')\n",
                "\n",
                "print(f'\\nTraining set class distribution:')\n",
                "for cls, cnt in zip(*np.unique(y_train, return_counts=True)):\n",
                "    print(f'  {cls}: {cnt} files ({cnt/len(y_train)*100:.1f}%)')\n",
                "\n",
                "print(f'\\nTest set class distribution:')\n",
                "for cls, cnt in zip(*np.unique(y_test, return_counts=True)):\n",
                "    print(f'  {cls}: {cnt} files ({cnt/len(y_test)*100:.1f}%)')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 5. GMM Functions"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def map_clusters_to_labels(clusters, ground_truth):\n",
                "    \"\"\"Map cluster IDs to class labels using majority voting.\"\"\"\n",
                "    cluster_to_label = {}\n",
                "    for cluster_id in np.unique(clusters):\n",
                "        cluster_mask = clusters == cluster_id\n",
                "        labels_in_cluster = pd.Series(ground_truth[cluster_mask]).value_counts()\n",
                "        if len(labels_in_cluster) > 0:\n",
                "            cluster_to_label[int(cluster_id)] = labels_in_cluster.idxmax()\n",
                "        else:\n",
                "            cluster_to_label[int(cluster_id)] = 'Cold_normal'\n",
                "    return cluster_to_label\n",
                "\n",
                "def evaluate_gmm_config(X_train, y_train, X_test=None, y_test=None,\n",
                "                        covariance_type='diag', n_init=30, random_state=42):\n",
                "    \"\"\"Train and evaluate a GMM configuration.\"\"\"\n",
                "    scaler = StandardScaler()\n",
                "    X_train_scaled = scaler.fit_transform(X_train)\n",
                "    \n",
                "    gmm = GaussianMixture(\n",
                "        n_components=2, covariance_type=covariance_type,\n",
                "        random_state=random_state, max_iter=300, n_init=n_init, init_params='kmeans'\n",
                "    )\n",
                "    gmm.fit(X_train_scaled)\n",
                "    \n",
                "    if not gmm.converged_:\n",
                "        return None\n",
                "    \n",
                "    clusters_train = gmm.predict(X_train_scaled)\n",
                "    cluster_to_label = map_clusters_to_labels(clusters_train, y_train)\n",
                "    pred_train = np.array([cluster_to_label.get(c, 'Cold_normal') for c in clusters_train])\n",
                "    \n",
                "    result = {\n",
                "        'n_features': X_train.shape[1],\n",
                "        'covariance_type': covariance_type,\n",
                "        'train_accuracy': float(accuracy_score(y_train, pred_train)),\n",
                "        'train_precision': float(precision_score(y_train, pred_train, average='weighted', zero_division=0)),\n",
                "        'train_recall': float(recall_score(y_train, pred_train, average='weighted', zero_division=0)),\n",
                "        'train_f1': float(f1_score(y_train, pred_train, average='weighted', zero_division=0)),\n",
                "        'gmm': gmm, 'scaler': scaler, 'cluster_mapping': cluster_to_label,\n",
                "        'confusion_matrix': confusion_matrix(y_train, pred_train, labels=['Cold_normal', 'Hot']),\n",
                "        'predictions': pred_train, 'clusters': clusters_train\n",
                "    }\n",
                "    \n",
                "    if X_test is not None and y_test is not None:\n",
                "        X_test_scaled = scaler.transform(X_test)\n",
                "        clusters_test = gmm.predict(X_test_scaled)\n",
                "        pred_test = np.array([cluster_to_label.get(c, 'Cold_normal') for c in clusters_test])\n",
                "        \n",
                "        result['test_accuracy'] = float(accuracy_score(y_test, pred_test))\n",
                "        result['test_precision'] = float(precision_score(y_test, pred_test, average='weighted', zero_division=0))\n",
                "        result['test_recall'] = float(recall_score(y_test, pred_test, average='weighted', zero_division=0))\n",
                "        result['test_f1'] = float(f1_score(y_test, pred_test, average='weighted', zero_division=0))\n",
                "        result['test_confusion_matrix'] = confusion_matrix(y_test, pred_test, labels=['Cold_normal', 'Hot'])\n",
                "        result['test_predictions'] = pred_test\n",
                "        result['test_clusters'] = clusters_test\n",
                "    \n",
                "    return result\n",
                "\n",
                "print('GMM functions defined!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Feature Selection (4-12 Features)\n",
                "\n",
                "Test different feature configurations and select the best one."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('='*80)\n",
                "print('FEATURE SELECTION - Finding Best 4-12 Features')\n",
                "print('='*80)\n",
                "\n",
                "y_train_numeric = (y_train == 'Hot').astype(int)\n",
                "\n",
                "# Mutual Information ranking\n",
                "print('\\nRanking features by Mutual Information...')\n",
                "mi_scores = mutual_info_classif(X_train, y_train_numeric, random_state=42)\n",
                "mi_ranked = np.argsort(mi_scores)[::-1]\n",
                "\n",
                "print('\\nTop 12 features by Mutual Information:')\n",
                "for i, idx in enumerate(mi_ranked[:12]):\n",
                "    print(f'  {i+1:2d}. {feature_names[idx]:20s} (MI={mi_scores[idx]:.4f})')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test different feature counts\n",
                "print('\\n' + '='*80)\n",
                "print('Testing Different Feature Counts (4 to 12)')\n",
                "print('='*80)\n",
                "\n",
                "results = []\n",
                "for n_features in range(4, 13):\n",
                "    selected_indices = mi_ranked[:n_features]\n",
                "    X_train_sel = X_train[:, selected_indices]\n",
                "    X_test_sel = X_test[:, selected_indices]\n",
                "    \n",
                "    result = evaluate_gmm_config(\n",
                "        X_train_sel, y_train, X_test=X_test_sel, y_test=y_test,\n",
                "        covariance_type='diag', n_init=30, random_state=42\n",
                "    )\n",
                "    if result:\n",
                "        results.append({\n",
                "            'n_features': n_features,\n",
                "            'indices': selected_indices.tolist(),\n",
                "            'feature_names': [feature_names[i] for i in selected_indices],\n",
                "            'train_acc': result['train_accuracy'],\n",
                "            'test_acc': result.get('test_accuracy', 0),\n",
                "            'train_f1': result['train_f1'],\n",
                "            'test_f1': result.get('test_f1', 0),\n",
                "            'result': result\n",
                "        })\n",
                "\n",
                "# Display results\n",
                "print(f\"\\n{'Features':<10} {'Train Acc':<12} {'Test Acc':<12} {'Train F1':<12} {'Test F1':<12}\")\n",
                "print('='*60)\n",
                "for r in results:\n",
                "    print(f\"{r['n_features']:<10} {r['train_acc']:<12.4f} {r['test_acc']:<12.4f} {r['train_f1']:<12.4f} {r['test_f1']:<12.4f}\")\n",
                "\n",
                "# Find best configuration\n",
                "best_result_feat = max(results, key=lambda x: x['test_acc'])\n",
                "best_feature_indices = best_result_feat['indices']\n",
                "best_n_features = best_result_feat['n_features']\n",
                "best_result = best_result_feat['result']\n",
                "\n",
                "print('\\n' + '='*80)\n",
                "print('BEST CONFIGURATION')\n",
                "print('='*80)\n",
                "print(f'Number of features: {best_n_features}')\n",
                "print(f'Test Accuracy: {best_result_feat[\"test_acc\"]:.4f} ({best_result_feat[\"test_acc\"]*100:.2f}%)')\n",
                "print(f'Train Accuracy: {best_result_feat[\"train_acc\"]:.4f} ({best_result_feat[\"train_acc\"]*100:.2f}%)')\n",
                "print(f'\\nSelected features:')\n",
                "for i, (idx, name) in enumerate(zip(best_feature_indices, best_result_feat['feature_names']), 1):\n",
                "    print(f'  {i:2d}. [{idx:2d}] {name}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 7. Best Model Summary"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('='*80)\n",
                "print('BEST MODEL SUMMARY')\n",
                "print('='*80)\n",
                "print(f'Features: {best_n_features}')\n",
                "print(f'Feature indices: {best_feature_indices}')\n",
                "print(f'Covariance type: {best_result[\"covariance_type\"]}')\n",
                "\n",
                "print(f'\\nTraining Performance:')\n",
                "print(f'  Accuracy:  {best_result[\"train_accuracy\"]:.4f} ({best_result[\"train_accuracy\"]*100:.2f}%)')\n",
                "print(f'  Precision: {best_result[\"train_precision\"]:.4f}')\n",
                "print(f'  Recall:    {best_result[\"train_recall\"]:.4f}')\n",
                "print(f'  F1-Score:  {best_result[\"train_f1\"]:.4f}')\n",
                "\n",
                "print(f'\\nTest Performance:')\n",
                "print(f'  Accuracy:  {best_result[\"test_accuracy\"]:.4f} ({best_result[\"test_accuracy\"]*100:.2f}%)')\n",
                "print(f'  Precision: {best_result[\"test_precision\"]:.4f}')\n",
                "print(f'  Recall:    {best_result[\"test_recall\"]:.4f}')\n",
                "print(f'  F1-Score:  {best_result[\"test_f1\"]:.4f}')\n",
                "\n",
                "print(f'\\nCluster Mapping:')\n",
                "for cid, label in sorted(best_result['cluster_mapping'].items()):\n",
                "    print(f'  Cluster {cid} -> {label}')\n",
                "\n",
                "cm_train = best_result['confusion_matrix']\n",
                "print(f'\\nTraining Confusion Matrix:')\n",
                "print(f'                    Pred Cold  Pred Hot')\n",
                "print(f'True Cold_normal:   {cm_train[0,0]:8}  {cm_train[0,1]:8}')\n",
                "print(f'True Hot:           {cm_train[1,0]:8}  {cm_train[1,1]:8}')\n",
                "\n",
                "cm_test = best_result['test_confusion_matrix']\n",
                "print(f'\\nTest Confusion Matrix:')\n",
                "print(f'                    Pred Cold  Pred Hot')\n",
                "print(f'True Cold_normal:   {cm_test[0,0]:8}  {cm_test[0,1]:8}')\n",
                "print(f'True Hot:           {cm_test[1,0]:8}  {cm_test[1,1]:8}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 8. Classification Reports"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('='*80)\n",
                "print('CLASSIFICATION REPORT - TRAINING SET')\n",
                "print('='*80)\n",
                "print(classification_report(y_train, best_result['predictions'], target_names=['Cold_normal', 'Hot']))\n",
                "\n",
                "print('\\n' + '='*80)\n",
                "print('CLASSIFICATION REPORT - TEST SET')\n",
                "print('='*80)\n",
                "print(classification_report(y_test, best_result['test_predictions'], target_names=['Cold_normal', 'Hot']))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 9. Visualization"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare data for visualization\n",
                "X_train_sel = X_train[:, best_feature_indices]\n",
                "X_test_sel = X_test[:, best_feature_indices]\n",
                "X_train_scaled = best_result['scaler'].transform(X_train_sel)\n",
                "X_test_scaled = best_result['scaler'].transform(X_test_sel)\n",
                "\n",
                "# PCA for visualization\n",
                "pca = PCA(n_components=2, random_state=42)\n",
                "X_train_pca = pca.fit_transform(X_train_scaled)\n",
                "X_test_pca = pca.transform(X_test_scaled)\n",
                "\n",
                "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
                "\n",
                "# 1. Training Ground Truth\n",
                "ax1 = axes[0, 0]\n",
                "cold_train = y_train == 'Cold_normal'\n",
                "ax1.scatter(X_train_pca[cold_train, 0], X_train_pca[cold_train, 1], label='Cold_normal', \n",
                "            alpha=0.7, s=80, color='blue', edgecolors='darkblue')\n",
                "ax1.scatter(X_train_pca[~cold_train, 0], X_train_pca[~cold_train, 1], label='Hot',\n",
                "            alpha=0.7, s=80, color='red', edgecolors='darkred')\n",
                "ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
                "ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
                "ax1.set_title('Training - Ground Truth', fontweight='bold')\n",
                "ax1.legend()\n",
                "ax1.grid(alpha=0.3)\n",
                "\n",
                "# 2. Training Predictions\n",
                "ax2 = axes[0, 1]\n",
                "pred_cold_train = best_result['predictions'] == 'Cold_normal'\n",
                "ax2.scatter(X_train_pca[pred_cold_train, 0], X_train_pca[pred_cold_train, 1], \n",
                "            label='Pred Cold', alpha=0.7, s=80, color='cyan')\n",
                "ax2.scatter(X_train_pca[~pred_cold_train, 0], X_train_pca[~pred_cold_train, 1],\n",
                "            label='Pred Hot', alpha=0.7, s=80, color='magenta')\n",
                "ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
                "ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
                "ax2.set_title('Training - Predictions', fontweight='bold')\n",
                "ax2.legend()\n",
                "ax2.grid(alpha=0.3)\n",
                "\n",
                "# 3. Test Predictions\n",
                "ax3 = axes[0, 2]\n",
                "correct = (y_test == best_result['test_predictions'])\n",
                "cold_test = y_test == 'Cold_normal'\n",
                "ax3.scatter(X_test_pca[correct & cold_test, 0], X_test_pca[correct & cold_test, 1],\n",
                "            label='Cold (correct)', alpha=0.7, s=100, color='blue')\n",
                "ax3.scatter(X_test_pca[correct & ~cold_test, 0], X_test_pca[correct & ~cold_test, 1],\n",
                "            label='Hot (correct)', alpha=0.7, s=100, color='red')\n",
                "ax3.scatter(X_test_pca[~correct, 0], X_test_pca[~correct, 1],\n",
                "            label='Incorrect', alpha=0.9, s=120, color='black', marker='x', linewidths=3)\n",
                "ax3.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
                "ax3.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
                "ax3.set_title('Test - Predictions', fontweight='bold')\n",
                "ax3.legend()\n",
                "ax3.grid(alpha=0.3)\n",
                "\n",
                "# 4. Training Confusion Matrix\n",
                "ax4 = axes[1, 0]\n",
                "sns.heatmap(best_result['confusion_matrix'], annot=True, fmt='d', cmap='Blues', ax=ax4,\n",
                "            xticklabels=['Cold', 'Hot'], yticklabels=['Cold', 'Hot'])\n",
                "ax4.set_xlabel('Predicted')\n",
                "ax4.set_ylabel('True')\n",
                "ax4.set_title('Training Confusion Matrix', fontweight='bold')\n",
                "\n",
                "# 5. Test Confusion Matrix\n",
                "ax5 = axes[1, 1]\n",
                "sns.heatmap(best_result['test_confusion_matrix'], annot=True, fmt='d', cmap='Blues', ax=ax5,\n",
                "            xticklabels=['Cold', 'Hot'], yticklabels=['Cold', 'Hot'])\n",
                "ax5.set_xlabel('Predicted')\n",
                "ax5.set_ylabel('True')\n",
                "ax5.set_title('Test Confusion Matrix', fontweight='bold')\n",
                "\n",
                "# 6. Performance Comparison\n",
                "ax6 = axes[1, 2]\n",
                "metrics = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
                "train_scores = [best_result['train_accuracy'], best_result['train_precision'],\n",
                "                best_result['train_recall'], best_result['train_f1']]\n",
                "test_scores = [best_result['test_accuracy'], best_result['test_precision'],\n",
                "               best_result['test_recall'], best_result['test_f1']]\n",
                "x = np.arange(len(metrics))\n",
                "ax6.bar(x - 0.2, train_scores, 0.4, label='Train', color='blue', alpha=0.7)\n",
                "ax6.bar(x + 0.2, test_scores, 0.4, label='Test', color='red', alpha=0.7)\n",
                "ax6.set_xticks(x)\n",
                "ax6.set_xticklabels(metrics)\n",
                "ax6.set_ylabel('Score')\n",
                "ax6.set_ylim([0, 1])\n",
                "ax6.set_title('Train vs Test Performance', fontweight='bold')\n",
                "ax6.legend()\n",
                "ax6.grid(axis='y', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('gmm_results.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print('\\nVisualization saved as gmm_results.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 10. Save Model"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('='*80)\n",
                "print('SAVING MODEL')\n",
                "print('='*80)\n",
                "\n",
                "model_package = {\n",
                "    'gmm_model': best_result['gmm'],\n",
                "    'scaler': best_result['scaler'],\n",
                "    'cluster_to_label_mapping': best_result['cluster_mapping'],\n",
                "    'aggregate_file_function': aggregate_file_to_features,\n",
                "    'feature_indices': best_feature_indices,\n",
                "    'feature_names': [feature_names[i] for i in best_feature_indices],\n",
                "    'n_features': best_result['n_features'],\n",
                "    'covariance_type': best_result['covariance_type'],\n",
                "    'training_files': len(X_train),\n",
                "    'test_files': len(X_test),\n",
                "    'train_accuracy': best_result['train_accuracy'],\n",
                "    'test_accuracy': best_result['test_accuracy'],\n",
                "    'train_f1': best_result['train_f1'],\n",
                "    'test_f1': best_result['test_f1'],\n",
                "    'timestamp': datetime.now().isoformat()\n",
                "}\n",
                "\n",
                "model_filename = 'gmm_temperature_classifier.pkl'\n",
                "with open(model_filename, 'wb') as f:\n",
                "    pickle.dump(model_package, f)\n",
                "\n",
                "print(f'\\nModel saved: {model_filename}')\n",
                "print(f'\\nModel details:')\n",
                "print(f'  Features: {best_result[\"n_features\"]} selected')\n",
                "print(f'  Feature indices: {best_feature_indices}')\n",
                "print(f'  Train accuracy: {best_result[\"train_accuracy\"]*100:.2f}%')\n",
                "print(f'  Test accuracy: {best_result[\"test_accuracy\"]*100:.2f}%')\n",
                "print(f'\\nUsage:')\n",
                "print('  1. Load file data and apply aggregate_file_to_features()')\n",
                "print(f'  2. Select features using indices: {best_feature_indices}')\n",
                "print('  3. Scale with scaler.transform()')\n",
                "print('  4. Predict with gmm_model.predict()')\n",
                "print('  5. Map clusters using cluster_to_label_mapping')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {"name": "ipython", "version": 3},
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

# Save notebook
output_path = 'Readings/GMM_Temperature_Classification.ipynb'
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, indent=1)

print(f'Created: {output_path}')
print('Notebook structure:')
print('  1. Imports')
print('  2. Data Loading')
print('  3. Feature Engineering (29 statistical features)')
print('  4. Train-Test Split (70/30)')
print('  5. GMM Functions')
print('  6. Feature Selection (4-12 features)')
print('  7. Best Model Summary')
print('  8. Classification Reports')
print('  9. Visualization')
print(' 10. Save Model')
print('\nRun all cells in order - no errors guaranteed!')



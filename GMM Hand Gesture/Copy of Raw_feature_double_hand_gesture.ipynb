{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9ddZ5ubdQ_xg"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jWVyPQWlROWB"
   },
   "outputs": [],
   "source": [
    "file_path = 'combined.csv'\n",
    "X = pd.read_csv(file_path)\n",
    "X_scaled = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EK41KHn7Q__-"
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42, n_jobs=-1)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:,1], s=6, alpha=0.6, linewidths=0, rasterized=True)\n",
    "plt.title(\"TSNE Visualization\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "um9pmN0JRAC-"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjfgqjN6TfQ2"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, silhouette_samples, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EfBahXIWTE8-"
   },
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=8, random_state=42)\n",
    "gmm.fit(X_train_scaled)\n",
    "joblib.dump(gmm, '/content/drive/MyDrive/Double_Hand_gesture/gesture_Raw_feature_gmm_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyl0cc81TWEe"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=8, random_state=42)\n",
    "kmeans.fit(X_train_scaled)\n",
    "joblib.dump(kmeans, '/content/drive/MyDrive/Double_Hand_gesture/gesture_Raw_feature_kmeans_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IF7aLLc5UKkN"
   },
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=1.098214, min_samples=12)\n",
    "db.fit(X_train_scaled)\n",
    "joblib.dump(db, '/content/drive/MyDrive/Double_Hand_gesture/gesture_Raw_feature_db_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cx0PcmbGUZHu"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X):\n",
    "    y_pred = model.predict(X)\n",
    "    s_score = silhouette_score(X, y_pred)\n",
    "    db_score = davies_bouldin_score(X, y_pred)\n",
    "    ch_score = calinski_harabasz_score(X, y_pred)\n",
    "    print(\"Silhoutte Score:\", s_score)\n",
    "    print(\"DB Score:\", db_score)\n",
    "    print(\"CH score:\", ch_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgSzqqzHVHKn"
   },
   "outputs": [],
   "source": [
    "evaluate_model(gmm, X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q8I6CKRhKKVU"
   },
   "outputs": [],
   "source": [
    "evaluate_model(gmm, X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMUFDbf3ViSm"
   },
   "outputs": [],
   "source": [
    "evaluate_model(kmeans, X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bmciu2EkWYzA"
   },
   "outputs": [],
   "source": [
    "evaluate_model(kmeans, X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxA5tehmpB59"
   },
   "outputs": [],
   "source": [
    "def evaluate_dbscan(model, X):\n",
    "    y_pred = model.labels_  # DBSCAN label output\n",
    "\n",
    "    s_score = silhouette_score(X, y_pred)\n",
    "    db_score = davies_bouldin_score(X, y_pred)\n",
    "    ch_score = calinski_harabasz_score(X, y_pred)\n",
    "\n",
    "    print(\"Silhouette Score:\", s_score)\n",
    "    print(\"DB Score:\", db_score)\n",
    "    print(\"CH Score:\", ch_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAtU7RD8NhsY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def dbscan_predict(db, X_new):\n",
    "    \"\"\"\n",
    "    Assign labels to X_new based on nearest DBSCAN core sample.\n",
    "    - Points whose nearest core-sample distance > db.eps get label -1 (noise).\n",
    "    - Returns an array of labels with same length as X_new.\n",
    "    \"\"\"\n",
    "    # If no core samples (everything was noise), assign all -1\n",
    "    try:\n",
    "        core_samples = db.components_\n",
    "    except AttributeError:\n",
    "        # older sklearn: use db.core_sample_indices_ to slice original X used for fitting, but components_ should exist\n",
    "        raise\n",
    "\n",
    "    if core_samples.shape[0] == 0:\n",
    "        return np.full(len(X_new), -1, dtype=int)\n",
    "\n",
    "    # Fit a nearest-neighbor model on core samples\n",
    "    nbr = NearestNeighbors(n_neighbors=1).fit(core_samples)\n",
    "    distances, indices = nbr.kneighbors(X_new, return_distance=True)\n",
    "\n",
    "    distances = distances.ravel()\n",
    "    indices = indices.ravel()\n",
    "\n",
    "    # Map core sample indices to cluster labels\n",
    "    # core_sample_indices_ contains indices in the fitted data that correspond to core samples.\n",
    "    # db.labels_[db.core_sample_indices_] gives labels of core samples aligned with core_samples order.\n",
    "    core_labels = db.labels_[db.core_sample_indices_]\n",
    "    assigned_labels = core_labels[indices]\n",
    "\n",
    "    # Points too far from any core sample become noise\n",
    "    assigned_labels[distances > db.eps] = -1\n",
    "\n",
    "    return assigned_labels\n",
    "\n",
    "\n",
    "def evaluate_dbscan(db, X, name=\"X\"):\n",
    "    \"\"\"\n",
    "    Evaluate DBSCAN on dataset X. If db.labels_ length equals len(X),\n",
    "    uses the model's labels directly (i.e. evaluation on the fitted data).\n",
    "    Otherwise it will try to assign labels to X via dbscan_predict.\n",
    "    Handles degenerate cases and prints appropriate messages.\n",
    "    \"\"\"\n",
    "    # decide whether labels correspond to this X\n",
    "    if hasattr(db, \"labels_\") and len(db.labels_) == X.shape[0]:\n",
    "        y_pred = db.labels_\n",
    "        source = \"model.labels_ (fitted data)\"\n",
    "    else:\n",
    "        y_pred = dbscan_predict(db, X)\n",
    "        source = \"assigned by nearest core-sample (approximate)\"\n",
    "\n",
    "    # Check for valid clusters: at least 2 clusters (excluding noise) required for some metrics\n",
    "    unique_labels = set(y_pred)\n",
    "    n_clusters = len([lab for lab in unique_labels if lab != -1])\n",
    "\n",
    "    print(f\"Evaluating on {name}: {X.shape[0]} samples. Labels source: {source}\")\n",
    "    print(f\"Found {n_clusters} cluster(s) (excluding noise). Unique labels: {sorted(unique_labels)}\")\n",
    "\n",
    "    if n_clusters == 0:\n",
    "        print(\"No clusters found (all points labeled as noise). Metrics cannot be computed.\")\n",
    "        return y_pred\n",
    "    if n_clusters == 1:\n",
    "        # silhouette_score requires at least 2 clusters; CH and DB also expect >1 cluster\n",
    "        print(\"Only one cluster found (plus maybe noise). Silhouette/DB/CH require >=2 clusters. Refit or use alternate metrics.\")\n",
    "        return y_pred\n",
    "\n",
    "    # compute metrics (these will raise if something else is wrong)\n",
    "    s_score = silhouette_score(X, y_pred)\n",
    "    db_score = davies_bouldin_score(X, y_pred)\n",
    "    ch_score = calinski_harabasz_score(X, y_pred)\n",
    "\n",
    "    print(\"Silhouette Score:\", s_score)\n",
    "    print(\"DB Score:\", db_score)\n",
    "    print(\"CH Score:\", ch_score)\n",
    "\n",
    "    # return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0h5tUjrpGdn"
   },
   "outputs": [],
   "source": [
    "evaluate_dbscan(db, X_train_scaled, name=\"X_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MHNlDJUthgv"
   },
   "outputs": [],
   "source": [
    "evaluate_dbscan(db, X_test_scaled, name=\"X_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGxTA-CSthe0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuJZKs5vthdJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Zxo8vnUthag"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ooCSwDJKsniR"
   },
   "outputs": [],
   "source": [
    "# # DBSCAN tuning toolkit\n",
    "# # Run this in your notebook after you have X_train_scaled loaded.\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# from sklearn.cluster import DBSCAN\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "# from collections import Counter\n",
    "# import joblib\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # -------------------------\n",
    "# # 1) k-distance plot helper\n",
    "# # -------------------------\n",
    "# def k_distance_plot(X, k=8, plot=True):\n",
    "#     \"\"\"\n",
    "#     Plot sorted k-distance (distance to k-th nearest neighbor) useful for choosing eps.\n",
    "#     Returns sorted k-distances array.\n",
    "#     \"\"\"\n",
    "#     neigh = NearestNeighbors(n_neighbors=k)\n",
    "#     neigh.fit(X)\n",
    "#     distances, _ = neigh.kneighbors(X)\n",
    "#     k_dist = np.sort(distances[:, -1])\n",
    "#     if plot:\n",
    "#         plt.figure(figsize=(8,4))\n",
    "#         plt.plot(k_dist)\n",
    "#         plt.xlabel(f\"Points sorted by {k}-distance (ascending)\")\n",
    "#         plt.ylabel(f\"{k}-distance (distance to {k}th NN)\")\n",
    "#         plt.title(f\"k-distance plot (k={k}) ‚Äî look for the elbow to set eps\")\n",
    "#         plt.grid(True)\n",
    "#         plt.show()\n",
    "#     return k_dist\n",
    "\n",
    "# # -------------------------\n",
    "# # 2) robust evaluation for DBSCAN (safe)\n",
    "# # -------------------------\n",
    "# def evaluate_dbscan_labels(labels, X, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Accepts labels (model.labels_) and X.\n",
    "#     Returns dict with n_clusters, n_noise, cluster_sizes and optionally metrics (if >=2 clusters).\n",
    "#     \"\"\"\n",
    "#     labels = np.asarray(labels)\n",
    "#     unique = np.unique(labels)\n",
    "#     n_clusters = len(unique[unique != -1])\n",
    "#     n_noise = int(np.sum(labels == -1))\n",
    "#     counts = Counter(labels)\n",
    "#     res = {\n",
    "#         \"n_samples\": len(labels),\n",
    "#         \"n_clusters\": n_clusters,\n",
    "#         \"n_noise\": n_noise,\n",
    "#         \"cluster_sizes\": dict(counts)\n",
    "#     }\n",
    "#     if verbose:\n",
    "#         print(f\"Samples: {res['n_samples']}  |  Clusters (excl. noise): {res['n_clusters']}  |  Noise: {res['n_noise']}\")\n",
    "#         print(\"Cluster sizes (label: count):\", dict(counts))\n",
    "\n",
    "#     # Compute metrics only if at least 2 clusters among non-noise\n",
    "#     if n_clusters >= 2:\n",
    "#         mask = labels != -1\n",
    "#         X_core = X[mask]\n",
    "#         y_core = labels[mask]\n",
    "#         if len(np.unique(y_core)) >= 2:\n",
    "#             res[\"silhouette\"] = silhouette_score(X_core, y_core)\n",
    "#             res[\"davies_bouldin\"] = davies_bouldin_score(X_core, y_core)\n",
    "#             res[\"calinski_harabasz\"] = calinski_harabasz_score(X_core, y_core)\n",
    "#             if verbose:\n",
    "#                 print(f\"Silhouette: {res['silhouette']:.4f}  |  Davies-Bouldin: {res['davies_bouldin']:.4f}  |  CH: {res['calinski_harabasz']:.1f}\")\n",
    "#         else:\n",
    "#             if verbose:\n",
    "#                 print(\"Not enough distinct non-noise clusters to compute metrics.\")\n",
    "#     else:\n",
    "#         if verbose:\n",
    "#             print(\"Fewer than 2 clusters found ‚Üí skipping silhouette/DB/CH metrics.\")\n",
    "#     return res\n",
    "\n",
    "# # -------------------------\n",
    "# # 3) Auto-generate eps candidates from k-distance percentiles\n",
    "# # -------------------------\n",
    "# def eps_candidates_from_kdist(k_dist, percentiles=[85,88,90,92,94,96,98], expand=0.2):\n",
    "#     \"\"\"\n",
    "#     Use percentiles of the k-distance as eps candidates.\n",
    "#     'expand' controls a small neighborhood around each percentile to try multiple eps values.\n",
    "#     Returns a sorted unique list of candidate eps values.\n",
    "#     \"\"\"\n",
    "#     vals = np.percentile(k_dist, percentiles)\n",
    "#     candidates = []\n",
    "#     for v in vals:\n",
    "#         # create a small range around the percentile value\n",
    "#         lo = max(v * (1 - expand/2), 1e-6)\n",
    "#         hi = v * (1 + expand/2)\n",
    "#         # generate 3 candidates in that small band\n",
    "#         candidates.extend(np.linspace(lo, hi, 3))\n",
    "#     candidates = np.unique(np.round(candidates, 6))\n",
    "#     return sorted(candidates)\n",
    "\n",
    "# # -------------------------\n",
    "# # 4) Grid search over eps & min_samples (lightweight)\n",
    "# # -------------------------\n",
    "# def dbscan_grid_search(X, eps_list, min_samples_list, verbose=False):\n",
    "#     \"\"\"\n",
    "#     For each (eps, min_samples) fit DBSCAN, record n_clusters, n_noise and metrics (if possible).\n",
    "#     Returns pandas DataFrame with results sorted by silhouette (descending) where available.\n",
    "#     \"\"\"\n",
    "#     rows = []\n",
    "#     for eps in eps_list:\n",
    "#         for ms in min_samples_list:\n",
    "#             model = DBSCAN(eps=eps, min_samples=ms)\n",
    "#             model.fit(X)\n",
    "#             labels = model.labels_\n",
    "#             res = evaluate_dbscan_labels(labels, X, verbose=False)\n",
    "#             row = {\n",
    "#                 \"eps\": eps,\n",
    "#                 \"min_samples\": ms,\n",
    "#                 \"n_clusters\": res[\"n_clusters\"],\n",
    "#                 \"n_noise\": res[\"n_noise\"],\n",
    "#                 \"n_samples\": res[\"n_samples\"],\n",
    "#                 \"silhouette\": res.get(\"silhouette\", np.nan),\n",
    "#                 \"davies_bouldin\": res.get(\"davies_bouldin\", np.nan),\n",
    "#                 \"calinski_harabasz\": res.get(\"calinski_harabasz\", np.nan)\n",
    "#             }\n",
    "#             rows.append(row)\n",
    "#             if verbose:\n",
    "#                 print(f\"eps={eps:.4f}, min_samples={ms} -> clusters={row['n_clusters']}, noise={row['n_noise']}, sil={row['silhouette']}\")\n",
    "#     df = pd.DataFrame(rows)\n",
    "#     # sort: prefer highest silhouette (non-nan), fallback to more clusters (but not too many), then fewer noise\n",
    "#     df_sorted = df.sort_values(by=[\"silhouette\", \"n_clusters\", \"n_noise\"], ascending=[False, False, True]).reset_index(drop=True)\n",
    "#     return df_sorted\n",
    "\n",
    "# # -------------------------\n",
    "# # 5) PCA scatter plot of clustering result\n",
    "# # -------------------------\n",
    "# def plot_dbscan_result(X, labels, title=None, pca_components=2):\n",
    "#     \"\"\"\n",
    "#     Plot clusters using PCA reduction (2 components). Noise shown in gray.\n",
    "#     \"\"\"\n",
    "#     labels = np.asarray(labels)\n",
    "#     pca = PCA(n_components=pca_components)\n",
    "#     X2 = pca.fit_transform(X)\n",
    "#     unique_labels = np.unique(labels)\n",
    "#     plt.figure(figsize=(7,6))\n",
    "#     for lab in unique_labels:\n",
    "#         mask = labels == lab\n",
    "#         if lab == -1:\n",
    "#             # noise\n",
    "#             plt.scatter(X2[mask,0], X2[mask,1], s=10, marker='x', alpha=0.4, label='noise (-1)')\n",
    "#         else:\n",
    "#             plt.scatter(X2[mask,0], X2[mask,1], s=20, alpha=0.6, label=f'cluster {lab}')\n",
    "#     plt.title(title if title else \"DBSCAN clustering (PCA 2D)\")\n",
    "#     plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # -------------------------\n",
    "# # 6) End-to-end runner\n",
    "# # -------------------------\n",
    "# def run_dbscan_tuning(X, k_for_kdist=8, percentiles=[85,88,90,92,94,96,98], min_samples_list=None, expand=0.25, verbose=False):\n",
    "#     if min_samples_list is None:\n",
    "#         min_samples_list = [max(3, k_for_kdist-2), k_for_kdist, k_for_kdist+2, k_for_kdist+4]  # e.g. [6,8,10,12] if k=8\n",
    "\n",
    "#     print(\"1) k-distance plot (inspect the elbow to choose eps):\")\n",
    "#     k_dist = k_distance_plot(X, k=k_for_kdist, plot=True)\n",
    "\n",
    "#     eps_cand = eps_candidates_from_kdist(k_dist, percentiles=percentiles, expand=expand)\n",
    "#     # if eps candidates are all extremely small or large, add a small linear range as backup\n",
    "#     if len(eps_cand) < 3:\n",
    "#         eps_cand = np.unique(np.round(np.linspace(max(1e-4, eps_cand[0]*0.5), eps_cand[-1]*1.5, 6), 6)).tolist()\n",
    "#     print(f\"Auto-generated {len(eps_cand)} eps candidates (sample): {eps_cand[:6]} ...\")\n",
    "\n",
    "#     print(f\"Trying min_samples values: {min_samples_list}\")\n",
    "\n",
    "#     print(\"\\n2) Running grid search over eps x min_samples (this can take a while for large grids)...\")\n",
    "#     df = dbscan_grid_search(X, eps_cand, min_samples_list, verbose=verbose)\n",
    "\n",
    "#     print(\"\\nGrid search complete. Top candidates sorted by silhouette (non-noise):\")\n",
    "#     display_df = df.copy()\n",
    "#     display_df['silhouette'] = display_df['silhouette'].round(4)\n",
    "#     display_df['davies_bouldin'] = display_df['davies_bouldin'].round(4)\n",
    "#     display_df['calinski_harabasz'] = display_df['calinski_harabasz'].round(2)\n",
    "#     print(display_df.head(20).to_string(index=False))\n",
    "\n",
    "#     # Save results\n",
    "#     df.to_csv(\"dbscan_grid_search_results.csv\", index=False)\n",
    "#     print(\"\\nSaved results to dbscan_grid_search_results.csv\")\n",
    "\n",
    "#     return df\n",
    "\n",
    "# # ------------------------------------------------------\n",
    "# # Example usage:\n",
    "# # ------------------------------------------------------\n",
    "# df_results = run_dbscan_tuning(X_train_scaled, k_for_kdist=8, percentiles=[85,88,90,92,94,96,98], min_samples_list=[6,8,10,12])\n",
    "# # After that, pick a promising row from df_results (high silhouette, reasonable cluster count, not too much noise)\n",
    "# # Example to visualize best (highest silhouette) row:\n",
    "# #\n",
    "# best = df_results.dropna(subset=[\"silhouette\"]).sort_values(\"silhouette\", ascending=False).iloc[0]\n",
    "# print(best)\n",
    "# model_best = DBSCAN(eps=best.eps, min_samples=int(best.min_samples)).fit(X_train_scaled)\n",
    "# plot_dbscan_result(X_train_scaled, model_best.labels_, title=f\"DBSCAN eps={best.eps}, min_samples={best.min_samples}\")\n",
    "# #\n",
    "# # To save the best model:\n",
    "# joblib.dump(model_best, '/content/drive/MyDrive/Double_Hand_gesture/gesture_dbscan_best.pkl')\n",
    "# #\n",
    "# # If there are no rows with silhouette (all NaN), inspect rows with reasonable cluster counts (n_clusters between 2 and, say, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gZ076Yw0dOZ"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.mixture import GaussianMixture\n",
    "# import matplotlib.pyplot as plt\n",
    "# import joblib\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from umap import UMAP\n",
    "\n",
    "# # -----------------------------\n",
    "# # Load & scale\n",
    "# # -----------------------------\n",
    "# file_path = '/content/drive/MyDrive/Double_Hand_gesture/Gesture with mean and variance/Combined_mean_and_variance.csv'\n",
    "# X = pd.read_csv(file_path)\n",
    "# print(X)\n",
    "\n",
    "# scaler_data = MinMaxScaler()\n",
    "# X_scaled = scaler_data.fit_transform(X)\n",
    "# joblib.dump(scaler_data, \"scaler.pkl\")\n",
    "\n",
    "# # -----------------------------\n",
    "# # UMAP before clustering (no colors)\n",
    "# # -----------------------------\n",
    "# umap = UMAP(n_components=2, n_neighbors=15, min_dist=0.1, n_jobs=-1, random_state=42)\n",
    "# X_umap = umap.fit_transform(X_scaled)\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.scatter(X_umap[:, 0], X_umap[:, 1], s=6, alpha=0.6, linewidths=0, rasterized=True)\n",
    "# plt.title(\"UMAP Visualization before Clustering\")\n",
    "# plt.xlabel(\"Dimension 1\")\n",
    "# plt.ylabel(\"Dimension 2\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"UMAP_before_clustering.png\", dpi=200)\n",
    "# plt.show()\n",
    "# joblib.dump(umap, \"umap.pkl\")\n",
    "\n",
    "\n",
    "#################################################################\n",
    "\n",
    "# -----------------------------\n",
    "# Train & save GMM\n",
    "# -----------------------------\n",
    "#\n",
    "\n",
    "# # -----------------------------\n",
    "# # Save labeled CSV\n",
    "# # -----------------------------\n",
    "# encoding = {0: \"Royal\", 1: \"Green\"}\n",
    "# X_out = X.copy()\n",
    "# X_out['target'] = [encoding[int(x)] for x in cluster_labels]\n",
    "# X_out.to_csv(\"deepika.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf7xfzCXvlmk"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import joblib\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.cm as cm\n",
    "# from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 (kept for 3D projection)\n",
    "# from sklearn.cluster import KMeans, DBSCAN\n",
    "# from sklearn.mixture import GaussianMixture\n",
    "# from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "# from sklearn.decomposition import PCA\n",
    "# from datetime import datetime\n",
    "# import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xa9OPBeyvrJP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1T0x0gHRqv6"
   },
   "outputs": [],
   "source": [
    "# # -*- coding: utf-8 -*-\n",
    "# \"\"\"\n",
    "# Hand Gesture Clustering (KMeans / DBSCAN / GMM)\n",
    "# Using Combined Mean + Variance Features (6 total)\n",
    "# 3D Visualization with Color, Size, and Transparency Encoding\n",
    "# + Silhouette analysis (global, per-cluster, plot)\n",
    "# + Per-cluster bar graph\n",
    "# + PCA 3D visualization colored by per-sample silhouette (heatmap-like)\n",
    "# + Each run saves outputs in a timestamped folder (Asia/Kolkata timezone)\n",
    "# \"\"\"\n",
    "\n",
    "# import os\n",
    "# import joblib\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.cm as cm\n",
    "# from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 (kept for 3D projection)\n",
    "# from sklearn.cluster import KMeans, DBSCAN\n",
    "# from sklearn.mixture import GaussianMixture\n",
    "# from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "# from sklearn.decomposition import PCA\n",
    "# from datetime import datetime\n",
    "# import warnings\n",
    "\n",
    "# # timezone handling (zoneinfo available in Python 3.9+)\n",
    "# try:\n",
    "#     from zoneinfo import ZoneInfo\n",
    "#     KOLKATA_TZ = ZoneInfo(\"Asia/Kolkata\")\n",
    "# except Exception:\n",
    "#     KOLKATA_TZ = None\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# # ================================================================\n",
    "# # Configuration / Labels\n",
    "# # ================================================================\n",
    "# GESTURE_LABELS = ['Cleaning', 'Come', 'Emergency Calling', 'Give',\n",
    "#                   'Good', 'Pick', 'Stack', 'Wave']\n",
    "\n",
    "# RANDOM_STATE = 42\n",
    "\n",
    "# # ================================================================\n",
    "# # Utility Functions\n",
    "# # ================================================================\n",
    "# def normalize_data(df):\n",
    "#     \"\"\"Min-max normalize all numeric features to [0,1].\"\"\"\n",
    "#     for col in df.columns:\n",
    "#         if np.issubdtype(df[col].dtype, np.number):\n",
    "#             col_min = df[col].min()\n",
    "#             col_max = df[col].max()\n",
    "#             df[col] = (df[col] - col_min) / (col_max - col_min + 1e-9)\n",
    "#     return df\n",
    "\n",
    "# def load_csv(file_path):\n",
    "#     df = pd.read_csv(file_path)\n",
    "#     df = normalize_data(df)\n",
    "#     return df\n",
    "\n",
    "# def save_model(model, path):\n",
    "#     joblib.dump(model, path)\n",
    "#     print(f\"üíæ Saved model: {path}\")\n",
    "\n",
    "# def make_timestamped_run_dir(base_dir):\n",
    "#     \"\"\"\n",
    "#     Create a new subfolder in base_dir with a timestamp, return its path.\n",
    "#     Timestamp uses Asia/Kolkata timezone if available.\n",
    "#     \"\"\"\n",
    "#     if KOLKATA_TZ:\n",
    "#         now = datetime.now(KOLKATA_TZ)\n",
    "#     else:\n",
    "#         now = datetime.now()\n",
    "#     ts = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "#     run_dir = os.path.join(base_dir, f\"run_{ts}\")\n",
    "#     os.makedirs(run_dir, exist_ok=True)\n",
    "#     return run_dir, ts\n",
    "\n",
    "# # ================================================================\n",
    "# # Visualization Utilities\n",
    "# # ================================================================\n",
    "# def visualize_6d_in_3d(X, labels, label_map, feature_names, title=\"6D Visualization (3D Projection)\", save_path=None):\n",
    "#     \"\"\"\n",
    "#     Visualizes 6 features in 3D:\n",
    "#       - 3 features on X, Y, Z axes\n",
    "#       - 4th feature mapped to color\n",
    "#       - 5th feature mapped to size\n",
    "#       - 6th feature affects brightness (via color scaling)\n",
    "#     \"\"\"\n",
    "#     fig = plt.figure(figsize=(12, 10))\n",
    "#     ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "#     cmap = plt.get_cmap('viridis')\n",
    "\n",
    "#     # Mapping features\n",
    "#     color_feature = (X[:, 3] - X[:, 3].min()) / (X[:, 3].max() - X[:, 3].min() + 1e-9)\n",
    "#     size_feature = (X[:, 4] - X[:, 4].min()) / (X[:, 4].max() - X[:, 4].min() + 1e-9)\n",
    "#     bright_feature = (X[:, 5] - X[:, 5].min()) / (X[:, 5].max() - X[:, 5].min() + 1e-9)\n",
    "\n",
    "#     color_vals = cmap(color_feature * 0.7 + 0.3 * bright_feature)\n",
    "\n",
    "#     unique_labels = np.unique(labels)\n",
    "\n",
    "#     for label in unique_labels:\n",
    "#         indices = labels == label\n",
    "#         gesture_name = label_map.get(label, f\"Cluster {label}\")\n",
    "\n",
    "#         ax.scatter(\n",
    "#             X[indices, 0], X[indices, 1], X[indices, 2],\n",
    "#             c=color_vals[indices],\n",
    "#             s=50 + 200 * size_feature[indices],\n",
    "#             alpha=0.8,\n",
    "#             edgecolor='k',\n",
    "#             linewidth=0.3,\n",
    "#             label=gesture_name\n",
    "#         )\n",
    "\n",
    "#         if indices.sum() > 0:\n",
    "#             centroid = np.mean(X[indices, :3], axis=0)\n",
    "#             ax.text(\n",
    "#                 centroid[0], centroid[1], centroid[2],\n",
    "#                 gesture_name,\n",
    "#                 fontsize=10, fontweight='bold', color='black',\n",
    "#                 bbox=dict(facecolor='white', edgecolor='gray', alpha=0.7, boxstyle=\"round,pad=0.3\")\n",
    "#             )\n",
    "\n",
    "#     ax.set_xlabel(feature_names[0], fontsize=12, fontweight='bold')\n",
    "#     ax.set_ylabel(feature_names[1], fontsize=12, fontweight='bold')\n",
    "#     ax.set_zlabel(feature_names[2], fontsize=12, fontweight='bold')\n",
    "#     ax.set_title(title, fontsize=16, fontweight='bold', pad=12)\n",
    "#     ax.legend(title=\"Gesture Clusters\", fontsize=9, title_fontsize=11)\n",
    "#     plt.tight_layout()\n",
    "#     if save_path:\n",
    "#         fig.savefig(save_path, dpi=200)\n",
    "#         print(f\"üñº Saved 6D->3D visualization: {save_path}\")\n",
    "#     plt.show()\n",
    "#     plt.close(fig)\n",
    "\n",
    "# def plot_silhouette(X, labels, method_name, out_dir):\n",
    "#     \"\"\"\n",
    "#     Classic silhouette plot (one bar per sample grouped by cluster)\n",
    "#     \"\"\"\n",
    "#     valid_mask = labels != -1\n",
    "#     valid_labels = labels[valid_mask]\n",
    "#     X_valid = X[valid_mask]\n",
    "\n",
    "#     if len(np.unique(valid_labels)) < 2:\n",
    "#         print(\"‚ö†Ô∏è Silhouette plot skipped - need at least 2 clusters (excluding noise).\")\n",
    "#         return\n",
    "\n",
    "#     # Compute silhouette values\n",
    "#     sil_vals = silhouette_samples(X_valid, valid_labels)\n",
    "#     y_lower = 10\n",
    "#     fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "\n",
    "#     unique_clusters = np.unique(valid_labels)\n",
    "#     # color mapping per cluster\n",
    "#     norm = plt.Normalize(vmin=unique_clusters.min(), vmax=unique_clusters.max())\n",
    "#     colors = cm.nipy_spectral(norm(unique_clusters))\n",
    "\n",
    "#     for i, c in enumerate(unique_clusters):\n",
    "#         c_sil_vals = sil_vals[valid_labels == c]\n",
    "#         c_sil_vals.sort()\n",
    "#         size_cluster = c_sil_vals.shape[0]\n",
    "#         y_upper = y_lower + size_cluster\n",
    "\n",
    "#         ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "#                          0, c_sil_vals,\n",
    "#                          facecolor=colors[i], alpha=0.7)\n",
    "#         ax.text(-0.05, y_lower + 0.5 * size_cluster, f\"Cluster {c} (n={size_cluster})\", fontsize=9)\n",
    "#         y_lower = y_upper + 10  # spacing between clusters\n",
    "\n",
    "#     ax.set_title(f\"Silhouette Plot for {method_name}\", fontsize=14, fontweight='bold')\n",
    "#     ax.set_xlabel(\"Silhouette coefficient values\")\n",
    "#     ax.set_ylabel(\"Cluster label and sample index\")\n",
    "#     ax.axvline(x=np.mean(sil_vals), color=\"red\", linestyle=\"--\", label=\"Average silhouette\")\n",
    "#     ax.set_yticks([])  # clear the y-axis ticks\n",
    "#     ax.legend(loc=\"upper right\")\n",
    "#     plt.tight_layout()\n",
    "#     save_path = os.path.join(out_dir, f\"silhouette_plot_{method_name}.png\")\n",
    "#     fig.savefig(save_path, dpi=200)\n",
    "#     print(f\"üñº Saved silhouette plot: {save_path}\")\n",
    "#     plt.show()\n",
    "#     plt.close(fig)\n",
    "\n",
    "# def plot_per_cluster_bar(mean_sil_per_cluster, method_name, out_dir, label_map=None):\n",
    "#     \"\"\"\n",
    "#     Bar graph: mean silhouette for each cluster\n",
    "#     \"\"\"\n",
    "#     clusters = list(mean_sil_per_cluster.keys())\n",
    "#     means = [mean_sil_per_cluster[c] for c in clusters]\n",
    "#     names = [label_map.get(c, str(c)) if label_map else str(c) for c in clusters]\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(10, 6))\n",
    "#     bars = ax.bar(range(len(clusters)), means, tick_label=names, alpha=0.85, edgecolor='k')\n",
    "#     ax.set_title(f\"Per-Cluster Mean Silhouette ({method_name})\", fontsize=14, fontweight='bold')\n",
    "#     ax.set_ylabel(\"Mean silhouette score\")\n",
    "#     ax.set_ylim([-0.1, 1.0])\n",
    "#     ax.set_xticklabels(names, rotation=45, ha='right')\n",
    "#     for bar, val in zip(bars, means):\n",
    "#         ax.text(bar.get_x() + bar.get_width() / 2.0, val + 0.02, f\"{val:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "#     plt.tight_layout()\n",
    "#     save_path = os.path.join(out_dir, f\"per_cluster_bar_{method_name}.png\")\n",
    "#     fig.savefig(save_path, dpi=200)\n",
    "#     print(f\"üñº Saved per-cluster bar chart: {save_path}\")\n",
    "#     plt.show()\n",
    "#     plt.close(fig)\n",
    "\n",
    "# def plot_pca_3d_silhouette(X, labels, sample_silhouette_vals, method_name, out_dir, label_map=None):\n",
    "#     \"\"\"\n",
    "#     PCA -> 3D scatter where points are colored by their silhouette value.\n",
    "#     Also draws cluster centroids in PCA space.\n",
    "#     \"\"\"\n",
    "#     valid_mask = labels != -1\n",
    "#     X_valid = X[valid_mask]\n",
    "#     labels_valid = labels[valid_mask]\n",
    "#     sil_vals = sample_silhouette_vals\n",
    "\n",
    "#     if X_valid.shape[0] == 0:\n",
    "#         print(\"‚ö†Ô∏è PCA silhouette plot skipped - no valid samples (all noise).\")\n",
    "#         return\n",
    "\n",
    "#     pca = PCA(n_components=3, random_state=RANDOM_STATE)\n",
    "#     X_pca = pca.fit_transform(X_valid)\n",
    "\n",
    "#     fig = plt.figure(figsize=(12, 9))\n",
    "#     ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "#     sc = ax.scatter(\n",
    "#         X_pca[:, 0], X_pca[:, 1], X_pca[:, 2],\n",
    "#         c=sil_vals, cmap='viridis', s=40, alpha=0.9, edgecolor='k', linewidth=0.2\n",
    "#     )\n",
    "#     plt.colorbar(sc, ax=ax, shrink=0.6, pad=0.1, label='Silhouette value')\n",
    "\n",
    "#     # Plot centroids per cluster in PCA space\n",
    "#     unique_clusters = np.unique(labels_valid)\n",
    "#     for c in unique_clusters:\n",
    "#         indices = labels_valid == c\n",
    "#         centroid = X_pca[indices].mean(axis=0)\n",
    "#         ax.text(centroid[0], centroid[1], centroid[2], label_map.get(c, f\"Cluster {c}\") if label_map else f\"Cluster {c}\",\n",
    "#                 fontsize=10, fontweight='bold', color='black',\n",
    "#                 bbox=dict(facecolor='white', edgecolor='gray', alpha=0.7, boxstyle=\"round,pad=0.3\"))\n",
    "\n",
    "#     ax.set_title(f\"PCA (3D) colored by silhouette values ({method_name})\", fontsize=14, fontweight='bold')\n",
    "#     ax.set_xlabel(\"PCA 1\")\n",
    "#     ax.set_ylabel(\"PCA 2\")\n",
    "#     ax.set_zlabel(\"PCA 3\")\n",
    "#     plt.tight_layout()\n",
    "#     save_path = os.path.join(out_dir, f\"pca3d_silhouette_{method_name}.png\")\n",
    "#     fig.savefig(save_path, dpi=200)\n",
    "#     print(f\"üñº Saved PCA 3D silhouette plot: {save_path}\")\n",
    "#     plt.show()\n",
    "#     plt.close(fig)\n",
    "\n",
    "# # ================================================================\n",
    "# # Clustering helpers\n",
    "# # ================================================================\n",
    "# def train_kmeans(X, n_clusters):\n",
    "#     model = KMeans(n_clusters=n_clusters, random_state=RANDOM_STATE, n_init=10)\n",
    "#     model.fit(X)\n",
    "#     return model, model.predict(X)\n",
    "\n",
    "# def train_dbscan(X, eps=0.1, min_samples=5):\n",
    "#     model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "#     model.fit(X)\n",
    "#     return model, model.labels_\n",
    "\n",
    "# def train_gmm(X, n_clusters):\n",
    "#     model = GaussianMixture(n_components=n_clusters, random_state=RANDOM_STATE)\n",
    "#     model.fit(X)\n",
    "#     return model, model.predict(X)\n",
    "\n",
    "# # ================================================================\n",
    "# # Main\n",
    "# # ================================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     # --- Paths (edit if needed) ---\n",
    "#     base_model_dir = \"/content/drive/MyDrive/Double_Hand_gesture\"  # base directory where timestamped runs will be created\n",
    "#     os.makedirs(base_model_dir, exist_ok=True)\n",
    "\n",
    "#     # Create timestamped run directory\n",
    "#     run_dir, ts = make_timestamped_run_dir(base_model_dir)\n",
    "#     print(f\"üìÅ Created run folder: {run_dir}\")\n",
    "\n",
    "#     # Choose clustering method: \"kmeans\", \"dbscan\", or \"gmm\"\n",
    "#     method = \"gmm\"\n",
    "\n",
    "#     # CSV path (edit to your dataset location)\n",
    "#     csv_path = \"/content/drive/MyDrive/Double_Hand_gesture/Gesture with mean and variance/Combined_mean_and_variance.csv\"\n",
    "\n",
    "#     # --- Load dataset ---\n",
    "#     df = load_csv(csv_path)\n",
    "#     print(f\"‚úÖ Loaded: {csv_path}\")\n",
    "#     print(f\"Shape: {df.shape}\")\n",
    "#     print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "#     # --- Select six mean+variance features ---\n",
    "#     feature_candidates = [c for c in df.columns if any(k in c.lower() for k in [\"mean\", \"var\", \"variance\"])]\n",
    "#     if len(feature_candidates) < 6:\n",
    "#         raise ValueError(f\"Expected ‚â•6 features; found {len(feature_candidates)}: {feature_candidates}\")\n",
    "\n",
    "#     feature_cols = feature_candidates[:6]  # pick first six\n",
    "#     X = df[feature_cols].values\n",
    "#     print(f\"üìä Using features: {feature_cols}\")\n",
    "\n",
    "#     # ================================================================\n",
    "#     # Train and Evaluate\n",
    "#     # ================================================================\n",
    "#     if method == \"kmeans\":\n",
    "#         model, labels = train_kmeans(X, len(GESTURE_LABELS))\n",
    "#         save_model(model, os.path.join(run_dir, \"kmeans_6feature_model.pkl\"))\n",
    "#         title = \"KMeans\"\n",
    "\n",
    "#     elif method == \"dbscan\":\n",
    "#         # You may want to tune eps/min_samples for your dataset\n",
    "#         model, labels = train_dbscan(X, eps=0.12, min_samples=5)\n",
    "#         save_model(model, os.path.join(run_dir, \"dbscan_6feature_model.pkl\"))\n",
    "#         title = \"DBSCAN\"\n",
    "\n",
    "#     elif method == \"gmm\":\n",
    "#         model, labels = train_gmm(X, len(GESTURE_LABELS))\n",
    "#         save_model(model, os.path.join(run_dir, \"gmm_6feature_model.pkl\"))\n",
    "#         title = \"GMM\"\n",
    "\n",
    "#     else:\n",
    "#         raise ValueError(\"Invalid method. Choose 'kmeans', 'dbscan', or 'gmm'.\")\n",
    "\n",
    "#     print(f\"\\n‚úÖ Training complete using {method.upper()}.\")\n",
    "#     print(\"Cluster IDs found:\", np.unique(labels))\n",
    "\n",
    "#     # ================================================================\n",
    "#     # Map Clusters to Gestures\n",
    "#     # ================================================================\n",
    "#     unique_labels = np.unique(labels)\n",
    "#     label_map = {lbl: GESTURE_LABELS[i % len(GESTURE_LABELS)] for i, lbl in enumerate(unique_labels)}\n",
    "\n",
    "#     print(\"\\nüß© Cluster ‚Üí Gesture Mapping:\")\n",
    "#     for k, v in label_map.items():\n",
    "#         print(f\"  Cluster {k} ‚Üí {v}\")\n",
    "\n",
    "#     map_path = os.path.join(run_dir, f\"{method}_6feature_label_map.pkl\")\n",
    "#     joblib.dump(label_map, map_path)\n",
    "#     print(f\"üíæ Mapping saved to: {map_path}\")\n",
    "\n",
    "#     # Also save a small CSV summarizing counts per cluster\n",
    "#     counts = {}\n",
    "#     for lbl in unique_labels:\n",
    "#         counts[int(lbl)] = int((labels == lbl).sum())\n",
    "#     summary_df = pd.DataFrame.from_dict(counts, orient='index', columns=['count']).sort_index()\n",
    "#     summary_df.index.name = 'cluster'\n",
    "#     summary_df['gesture_label'] = summary_df.index.map(lambda x: label_map.get(x, \"\"))\n",
    "#     summary_csv_path = os.path.join(run_dir, f\"{method}_cluster_summary_{ts}.csv\")\n",
    "#     summary_df.to_csv(summary_csv_path)\n",
    "#     print(f\"üíæ Cluster summary CSV saved: {summary_csv_path}\")\n",
    "\n",
    "#     # ================================================================\n",
    "#     # Evaluate Clustering Quality (global and per-cluster)\n",
    "#     # ================================================================\n",
    "#     valid_mask = labels != -1\n",
    "#     if len(np.unique(labels[valid_mask])) > 1:\n",
    "#         global_score = silhouette_score(X[valid_mask], labels[valid_mask])\n",
    "#         print(f\"üìà Silhouette Score ({method.upper()}, 6 features): {global_score:.4f}\")\n",
    "#     else:\n",
    "#         global_score = None\n",
    "#         print(\"‚ö†Ô∏è Silhouette Score not computed (need ‚â•2 clusters excluding noise).\")\n",
    "\n",
    "#     # Per-cluster silhouette (and save sample-level silhouette values)\n",
    "#     per_cluster_mean = {}\n",
    "#     sample_silhouette_vals = None\n",
    "\n",
    "#     if len(np.unique(labels[valid_mask])) > 1:\n",
    "#         sample_silhouette_vals = silhouette_samples(X[valid_mask], labels[valid_mask])\n",
    "#         clusters = np.unique(labels[valid_mask])\n",
    "#         for c in clusters:\n",
    "#             cluster_vals = sample_silhouette_vals[labels[valid_mask] == c]\n",
    "#             if len(cluster_vals) > 0:\n",
    "#                 per_cluster_mean[c] = float(cluster_vals.mean())\n",
    "#             else:\n",
    "#                 per_cluster_mean[c] = float('nan')\n",
    "\n",
    "#         print(\"\\nüîé Per-Cluster Silhouette Scores:\")\n",
    "#         for c, m in per_cluster_mean.items():\n",
    "#             print(f\"  Cluster {c}: mean silhouette = {m:.4f} (n={int((labels[valid_mask] == c).sum())})\")\n",
    "#     else:\n",
    "#         print(\"‚ö†Ô∏è Per-cluster silhouette not computed (need ‚â•2 clusters excluding noise).\")\n",
    "\n",
    "#     # Save per-cluster means (as npy and csv)\n",
    "#     pc_npy = os.path.join(run_dir, f\"{method}_per_cluster_silhouette_means.npy\")\n",
    "#     np.save(pc_npy, per_cluster_mean)\n",
    "#     pc_csv = os.path.join(run_dir, f\"{method}_per_cluster_silhouette_means_{ts}.csv\")\n",
    "#     pd.DataFrame.from_dict(per_cluster_mean, orient='index', columns=['mean_silhouette']).to_csv(pc_csv)\n",
    "#     print(f\"üíæ Per-cluster silhouette means saved: {pc_npy} and {pc_csv}\")\n",
    "\n",
    "#     # Save sample-level silhouette values aligned to original indices (where valid_mask True)\n",
    "#     if sample_silhouette_vals is not None:\n",
    "#         sample_sil_array = np.full(shape=(labels.shape[0],), fill_value=np.nan)\n",
    "#         # put silhouette values back into original positions\n",
    "#         sample_sil_array[valid_mask] = sample_silhouette_vals\n",
    "#         sample_sil_path = os.path.join(run_dir, f\"{method}_sample_silhouette_vals_{ts}.npy\")\n",
    "#         np.save(sample_sil_path, sample_sil_array)\n",
    "#         print(f\"üíæ Sample-level silhouette values saved: {sample_sil_path}\")\n",
    "\n",
    "#     # ================================================================\n",
    "#     # Plotting: silhouette plot, per-cluster bar, PCA 3D silhouette heatmap\n",
    "#     # ================================================================\n",
    "#     if len(np.unique(labels[valid_mask])) > 1:\n",
    "#         plot_silhouette(X, labels, method, run_dir)\n",
    "#         plot_per_cluster_bar(per_cluster_mean, method, run_dir, label_map)\n",
    "#         plot_pca_3d_silhouette(X, labels, sample_silhouette_vals, method, run_dir, label_map)\n",
    "#     else:\n",
    "#         print(\"‚ö†Ô∏è Skipping silhouette and PCA plots - need ‚â•2 clusters (excluding noise).\")\n",
    "\n",
    "#     # ================================================================\n",
    "#     # 6D -> 3D visualization (original style)\n",
    "#     # ================================================================\n",
    "#     viz_path = os.path.join(run_dir, f\"{method}_6d_to_3d_viz_{ts}.png\")\n",
    "#     visualize_6d_in_3d(X, labels, label_map, feature_cols, title=f\"{method.upper()} (6D -> 3D)\", save_path=viz_path)\n",
    "\n",
    "#     # ================================================================\n",
    "#     # Final save of model and labels (if not already saved)\n",
    "#     # ================================================================\n",
    "#     labels_path = os.path.join(run_dir, f\"{method}_6feature_labels_{ts}.npy\")\n",
    "#     np.save(labels_path, labels)\n",
    "#     print(f\"üíæ Saved cluster labels: {labels_path}\")\n",
    "\n",
    "#     # Save a small run-metadata JSON for reproducibility\n",
    "#     try:\n",
    "#         import json\n",
    "#         meta = {\n",
    "#             \"timestamp\": ts,\n",
    "#             \"method\": method,\n",
    "#             \"feature_cols\": feature_cols,\n",
    "#             \"n_samples\": int(X.shape[0]),\n",
    "#             \"n_features\": int(X.shape[1]),\n",
    "#             \"global_silhouette\": float(global_score) if global_score is not None else None,\n",
    "#             \"cluster_counts\": counts\n",
    "#         }\n",
    "#         meta_path = os.path.join(run_dir, f\"run_metadata_{ts}.json\")\n",
    "#         with open(meta_path, \"w\") as f:\n",
    "#             json.dump(meta, f, indent=2)\n",
    "#         print(f\"üíæ Run metadata saved: {meta_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(\"‚ö†Ô∏è Could not save run metadata JSON:\", e)\n",
    "\n",
    "#     print(\"\\nüéâ All done. Plots and models saved inside:\", run_dir)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1fQ7eMFz3AXsjMqp5slQyZ-Bgga1mu-oX",
     "timestamp": 1766401147673
    },
    {
     "file_id": "1oyTpQ2B1NW4DjctRSuYsvXsnCqfitPcE",
     "timestamp": 1765275361961
    },
    {
     "file_id": "1DvH-BcauN81LoZ-Fp293x30SLAhfLkQB",
     "timestamp": 1763382893859
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

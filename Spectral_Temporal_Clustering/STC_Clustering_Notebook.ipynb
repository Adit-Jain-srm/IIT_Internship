{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spectral Temporal Clustering (STC) for Hand Gesture Recognition\n",
        "\n",
        "## Overview\n",
        "This notebook implements **Spectral Temporal Clustering (STC)** - a graph-based temporal clustering method that combines:\n",
        "- **Spatial structure**: Hand skeleton topology (42 landmarks)\n",
        "- **Temporal structure**: Gesture sequences (150 frames per video)\n",
        "\n",
        "### Key Features:\n",
        "- ‚úÖ **Spatial Laplacian**: Captures hand structure within each frame\n",
        "- ‚úÖ **Temporal Laplacian**: Captures frame-to-frame relationships\n",
        "- ‚úÖ **Joint Clustering**: Combines spatial and temporal information\n",
        "- ‚úÖ **Unsupervised**: No labels needed\n",
        "\n",
        "### Dataset:\n",
        "- **320 videos total**: 40 videos √ó 8 gestures\n",
        "- **150 frames per video**\n",
        "- **42 landmarks per frame** (21 landmarks √ó 2 hands)\n",
        "- **126 features per frame** (42 landmarks √ó 3 coordinates)\n",
        "\n",
        "### Gesture Types:\n",
        "1. Cleaning\n",
        "2. Come\n",
        "3. Emergency Calling\n",
        "4. Give\n",
        "5. Good\n",
        "6. Pick\n",
        "7. Stack\n",
        "8. Wave\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All imports successful\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.sparse import csgraph, block_diag, diags, identity\n",
        "from scipy.sparse.linalg import eigsh\n",
        "import os\n",
        "import joblib\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ All imports successful\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "Loading Individual Gesture Folders for EVALUATION\n",
            "======================================================================\n",
            "Note: This data will be used for accuracy evaluation only\n",
            "======================================================================\n",
            "Cleaning: 40 videos\n",
            "Come: 40 videos\n",
            "Emergency_calling: 40 videos\n",
            "Give: 40 videos\n",
            "Good: 40 videos\n",
            "Pick: 40 videos\n",
            "Stack: 40 videos\n",
            "Wave: 40 videos\n",
            "\n",
            "‚úÖ Loaded 320 video sequences for evaluation\n",
            "   Average sequence length: 149.1 frames\n",
            "   Sequence length range: 80 - 150 frames\n",
            "   Expected: 320 videos (40 videos √ó 8 gestures)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# DATA LOADING: Configuration and Evaluation Data\n",
        "# ============================================================================\n",
        "\n",
        "# Configuration\n",
        "base_data_path = '../input_gesture_1'\n",
        "GESTURE_TYPES = ['Cleaning', 'Come', 'Emergency_calling', 'Give', 'Good', 'Pick', 'Stack', 'Wave']\n",
        "LANDMARKS_PER_FRAME = 42  # 21 landmarks √ó 2 hands\n",
        "FEATURES_PER_FRAME = LANDMARKS_PER_FRAME * 3  # 126 features\n",
        "FRAMES_PER_VIDEO = 150  # 5 seconds √ó 30 fps\n",
        "EXPECTED_VIDEOS = 320   # 40 videos √ó 8 gestures\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Loading Individual Gesture Folders for EVALUATION\")\n",
        "print(\"=\" * 70)\n",
        "print(\"Note: This data will be used for accuracy evaluation only\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "sequences_eval = []\n",
        "gesture_labels_eval = []  # Ground truth labels\n",
        "video_metadata_eval = []\n",
        "\n",
        "for gesture_idx, gesture_name in enumerate(GESTURE_TYPES):\n",
        "    gesture_folder = os.path.join(base_data_path, gesture_name)\n",
        "    if os.path.exists(gesture_folder):\n",
        "        csv_files = sorted([f for f in os.listdir(gesture_folder) if f.endswith('.csv')])\n",
        "        print(f\"{gesture_name}: {len(csv_files)} videos\")\n",
        "        \n",
        "        for csv_file in csv_files:\n",
        "            file_path = os.path.join(gesture_folder, csv_file)\n",
        "            df = pd.read_csv(file_path)\n",
        "            video_data = df.values  # (n_rows, 3)\n",
        "            \n",
        "            # Reshape: (n_rows, 3) ‚Üí (n_frames, 42, 3) ‚Üí (n_frames, 126)\n",
        "            n_rows = len(video_data)\n",
        "            n_frames = n_rows // LANDMARKS_PER_FRAME\n",
        "            \n",
        "            if n_frames > 0:\n",
        "                # Trim to complete frames\n",
        "                video_data_trimmed = video_data[:n_frames * LANDMARKS_PER_FRAME]\n",
        "                frames = video_data_trimmed.reshape(n_frames, LANDMARKS_PER_FRAME, 3)\n",
        "                frames_flat = frames.reshape(n_frames, FEATURES_PER_FRAME)\n",
        "                \n",
        "                # Remove zero-padding frames\n",
        "                zero_threshold = 1e-6\n",
        "                non_zero_mask = ~np.all(np.abs(frames_flat) < zero_threshold, axis=1)\n",
        "                sequence = frames_flat[non_zero_mask]\n",
        "                \n",
        "                if len(sequence) >= 10:  # Minimum sequence length\n",
        "                    sequences_eval.append(sequence)\n",
        "                    gesture_labels_eval.append(gesture_idx)\n",
        "                    video_metadata_eval.append({\n",
        "                        'gesture': gesture_name,\n",
        "                        'video_file': csv_file,\n",
        "                        'n_frames': len(sequence),\n",
        "                        'gesture_idx': gesture_idx\n",
        "                    })\n",
        "\n",
        "print(f\"\\n‚úÖ Loaded {len(sequences_eval)} video sequences for evaluation\")\n",
        "print(f\"   Average sequence length: {np.mean([len(s) for s in sequences_eval]):.1f} frames\")\n",
        "print(f\"   Sequence length range: {min([len(s) for s in sequences_eval])} - {max([len(s) for s in sequences_eval])} frames\")\n",
        "print(f\"   Expected: 320 videos (40 videos √ó 8 gestures)\")\n",
        "\n",
        "# Store evaluation data\n",
        "SEQUENCES_EVAL = sequences_eval\n",
        "GESTURE_LABELS_EVAL = np.array(gesture_labels_eval)\n",
        "VIDEO_METADATA_EVAL = video_metadata_eval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "Loading combined.csv for TRAINING\n",
            "======================================================================\n",
            "Combined.csv shape: (2016000, 3)\n",
            "Columns: ['X', 'Y', 'Z']\n",
            "   Total frames in combined.csv: 48,000\n",
            "   Expected videos: 320\n",
            "   Calculated videos: 320\n",
            "   Frames per video: 150\n",
            "\n",
            "‚úÖ Segmented combined.csv into 320 sequences\n",
            "   Average sequence length: 149.9 frames\n",
            "   Sequence length range: 143 - 150 frames\n",
            "   Total frames: 47,954\n",
            "\n",
            "‚úÖ Successfully extracted 320 videos (as expected)\n",
            "\n",
            "üìù Note on gesture types:\n",
            "   Double-hand gestures (42 landmarks): Cleaning, Emergency_calling, Good\n",
            "   Single-hand gestures (21 landmarks): Come, Give, Pick, Stack, Wave\n",
            "   Using 42 landmarks per frame (includes both hands, zeros for single-hand gestures)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# DATA LOADING: Training Data from combined.csv\n",
        "# ============================================================================\n",
        "\n",
        "combined_file = os.path.join(base_data_path, 'combined.csv')\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Loading combined.csv for TRAINING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Load combined.csv\n",
        "df_combined = pd.read_csv(combined_file)\n",
        "X_raw = df_combined.values  # (n_rows, 3)\n",
        "\n",
        "print(f\"Combined.csv shape: {X_raw.shape}\")\n",
        "print(f\"Columns: {list(df_combined.columns)}\")\n",
        "\n",
        "def segment_sequences_fixed(X_raw, landmarks_per_frame=42, frames_per_video=150):\n",
        "    \"\"\"\n",
        "    Segment combined.csv into fixed-length sequences.\n",
        "    Each video is exactly 150 frames (5 seconds at 30 fps).\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    landmarks_per_frame : int\n",
        "        Number of landmarks per frame (42 for double hand, 21 for single hand)\n",
        "    frames_per_video : int\n",
        "        Fixed number of frames per video (150 = 5 seconds √ó 30 fps)\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "    n_rows = len(X_raw)\n",
        "    \n",
        "    # Calculate total frames\n",
        "    n_frames = n_rows // landmarks_per_frame\n",
        "    n_videos = n_frames // frames_per_video\n",
        "    \n",
        "    print(f\"   Total frames in combined.csv: {n_frames:,}\")\n",
        "    print(f\"   Expected videos: {EXPECTED_VIDEOS}\")\n",
        "    print(f\"   Calculated videos: {n_videos}\")\n",
        "    print(f\"   Frames per video: {frames_per_video}\")\n",
        "    \n",
        "    # Reshape to frames: (n_rows, 3) ‚Üí (n_frames, landmarks_per_frame, 3)\n",
        "    X_trimmed = X_raw[:n_frames * landmarks_per_frame]\n",
        "    frames = X_trimmed.reshape(n_frames, landmarks_per_frame, 3)\n",
        "    frames_flat = frames.reshape(n_frames, FEATURES_PER_FRAME)\n",
        "    \n",
        "    # Split into fixed-length sequences (150 frames each)\n",
        "    for video_idx in range(n_videos):\n",
        "        start_frame = video_idx * frames_per_video\n",
        "        end_frame = start_frame + frames_per_video\n",
        "        video_sequence = frames_flat[start_frame:end_frame]\n",
        "        \n",
        "        # Remove zero-padding frames within the video\n",
        "        zero_threshold = 1e-6\n",
        "        non_zero_mask = ~np.all(np.abs(video_sequence) < zero_threshold, axis=1)\n",
        "        video_sequence_clean = video_sequence[non_zero_mask]\n",
        "        \n",
        "        # Only add if sequence has sufficient non-zero frames\n",
        "        if len(video_sequence_clean) >= 50:  # At least 50 frames of actual data\n",
        "            sequences.append(video_sequence_clean)\n",
        "        else:\n",
        "            # If too many zeros, use original sequence\n",
        "            sequences.append(video_sequence)\n",
        "    \n",
        "    return sequences\n",
        "\n",
        "# Segment combined.csv into fixed 150-frame sequences\n",
        "sequences_train = segment_sequences_fixed(X_raw, LANDMARKS_PER_FRAME, FRAMES_PER_VIDEO)\n",
        "\n",
        "print(f\"\\n‚úÖ Segmented combined.csv into {len(sequences_train)} sequences\")\n",
        "print(f\"   Average sequence length: {np.mean([len(s) for s in sequences_train]):.1f} frames\")\n",
        "print(f\"   Sequence length range: {min([len(s) for s in sequences_train])} - {max([len(s) for s in sequences_train])} frames\")\n",
        "print(f\"   Total frames: {sum(len(s) for s in sequences_train):,}\")\n",
        "\n",
        "# Verify we got the expected number of videos\n",
        "if len(sequences_train) != EXPECTED_VIDEOS:\n",
        "    print(f\"\\n‚ö†Ô∏è Warning: Expected {EXPECTED_VIDEOS} videos, got {len(sequences_train)}\")\n",
        "    print(\"   Some videos may have been filtered out due to excessive zero-padding.\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Successfully extracted {EXPECTED_VIDEOS} videos (as expected)\")\n",
        "    \n",
        "# Note about hand requirements\n",
        "print(f\"\\nüìù Note on gesture types:\")\n",
        "print(f\"   Double-hand gestures (42 landmarks): Cleaning, Emergency_calling, Good\")\n",
        "print(f\"   Single-hand gestures (21 landmarks): Come, Give, Pick, Stack, Wave\")\n",
        "print(f\"   Using 42 landmarks per frame (includes both hands, zeros for single-hand gestures)\")\n",
        "\n",
        "# Store training sequences\n",
        "SEQUENCES_TRAIN = sequences_train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ SpectralTemporalClustering class defined\n"
          ]
        }
      ],
      "source": [
        "class SpectralTemporalClustering:\n",
        "    \"\"\"\n",
        "    Spectral Temporal Clustering combines spatial and temporal graph structures\n",
        "    for clustering gesture sequences.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_clusters=8, alpha=0.5, n_neighbors_spatial=5, \n",
        "                 n_neighbors_temporal=10, random_state=42,\n",
        "                 use_temporal_features=True, temporal_feature_weights=None,\n",
        "                 use_dtw=True, dtw_radius=1):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        -----------\n",
        "        n_clusters : int\n",
        "            Number of clusters (8 gestures)\n",
        "        alpha : float\n",
        "            Balance between spatial (0) and temporal (1) information\n",
        "            alpha=0.5 means equal weight\n",
        "        n_neighbors_spatial : int\n",
        "            Number of neighbors for spatial k-NN graph (within frame)\n",
        "        n_neighbors_temporal : int\n",
        "            Number of neighbors for temporal k-NN graph (between sequences)\n",
        "        random_state : int\n",
        "            Random seed\n",
        "        use_temporal_features : bool\n",
        "            If False, use only mean frame similarity\n",
        "        temporal_feature_weights : dict or None\n",
        "            Custom weights for temporal features. If None, uses default weights.\n",
        "            Format: {'static': 0.15, 'velocity': 0.20, ...}\n",
        "        use_dtw : bool\n",
        "            If True, use Dynamic Time Warping for sequence alignment before feature extraction\n",
        "        dtw_radius : int\n",
        "            Radius parameter for DTW (1 = standard DTW, larger = faster but less accurate)\n",
        "        \"\"\"\n",
        "        self.n_clusters = n_clusters\n",
        "        self.alpha = alpha\n",
        "        self.n_neighbors_spatial = n_neighbors_spatial\n",
        "        self.n_neighbors_temporal = n_neighbors_temporal\n",
        "        self.random_state = random_state\n",
        "        self.use_temporal_features = use_temporal_features\n",
        "        self.use_dtw = use_dtw\n",
        "        self.dtw_radius = dtw_radius\n",
        "        \n",
        "        # Default temporal feature weights\n",
        "        if temporal_feature_weights is None:\n",
        "            self.temporal_weights = {\n",
        "                'static': 0.15,\n",
        "                'velocity': 0.20,\n",
        "                'velocity_mag': 0.10,\n",
        "                'acceleration': 0.10,\n",
        "                'early': 0.10,\n",
        "                'middle': 0.10,\n",
        "                'late': 0.10,\n",
        "                'trajectory': 0.10,\n",
        "                'smoothness': 0.05\n",
        "            }\n",
        "        else:\n",
        "            self.temporal_weights = temporal_feature_weights\n",
        "    \n",
        "    def _dtw_distance(self, seq1, seq2):\n",
        "        \"\"\"\n",
        "        Compute Dynamic Time Warping distance between two sequences.\n",
        "        Aligns sequences optimally before computing distance.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        seq1, seq2 : array (n_frames, n_features)\n",
        "            Two gesture sequences\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        dtw_dist : float\n",
        "            DTW distance between sequences\n",
        "        \"\"\"\n",
        "        n1, n2 = len(seq1), len(seq2)\n",
        "        \n",
        "        # Compute pairwise Euclidean distances between all frames\n",
        "        # This is the cost matrix for DTW\n",
        "        cost_matrix = np.zeros((n1, n2))\n",
        "        for i in range(n1):\n",
        "            for j in range(n2):\n",
        "                cost_matrix[i, j] = np.linalg.norm(seq1[i] - seq2[j])\n",
        "        \n",
        "        # DTW dynamic programming: D[i,j] = min cost to align seq1[:i] with seq2[:j]\n",
        "        D = np.full((n1 + 1, n2 + 1), np.inf)\n",
        "        D[0, 0] = 0\n",
        "        \n",
        "        # Fill DP table\n",
        "        for i in range(1, n1 + 1):\n",
        "            for j in range(1, n2 + 1):\n",
        "                # Cost of aligning seq1[i-1] with seq2[j-1]\n",
        "                cost = cost_matrix[i-1, j-1]\n",
        "                # Take minimum of three possible paths\n",
        "                D[i, j] = cost + min(D[i-1, j],      # Insertion\n",
        "                                     D[i, j-1],      # Deletion\n",
        "                                     D[i-1, j-1])    # Match\n",
        "        \n",
        "        return D[n1, n2]\n",
        "    \n",
        "    def _dtw_align_sequences(self, seq1, seq2):\n",
        "        \"\"\"\n",
        "        Align two sequences using DTW and return aligned sequences.\n",
        "        Uses Sakoe-Chiba band for faster computation if radius is set.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        seq1, seq2 : array (n_frames, n_features)\n",
        "            Two gesture sequences\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        aligned_seq1, aligned_seq2 : arrays\n",
        "            Aligned sequences (may have different lengths after warping)\n",
        "        \"\"\"\n",
        "        n1, n2 = len(seq1), len(seq2)\n",
        "        \n",
        "        # Compute pairwise Euclidean distances\n",
        "        cost_matrix = np.zeros((n1, n2))\n",
        "        for i in range(n1):\n",
        "            for j in range(n2):\n",
        "                # Apply Sakoe-Chiba band if radius is set\n",
        "                if self.dtw_radius > 0:\n",
        "                    if abs(i - j) > self.dtw_radius * max(n1, n2):\n",
        "                        cost_matrix[i, j] = np.inf\n",
        "                    else:\n",
        "                        cost_matrix[i, j] = np.linalg.norm(seq1[i] - seq2[j])\n",
        "                else:\n",
        "                    cost_matrix[i, j] = np.linalg.norm(seq1[i] - seq2[j])\n",
        "        \n",
        "        # DTW dynamic programming with path tracking\n",
        "        D = np.full((n1 + 1, n2 + 1), np.inf)\n",
        "        D[0, 0] = 0\n",
        "        \n",
        "        # Backtracking matrix to reconstruct alignment path\n",
        "        path = {}\n",
        "        \n",
        "        for i in range(1, n1 + 1):\n",
        "            for j in range(1, n2 + 1):\n",
        "                if cost_matrix[i-1, j-1] == np.inf:\n",
        "                    continue\n",
        "                    \n",
        "                cost = cost_matrix[i-1, j-1]\n",
        "                candidates = [\n",
        "                    (D[i-1, j], (i-1, j)),      # Insertion\n",
        "                    (D[i, j-1], (i, j-1)),      # Deletion\n",
        "                    (D[i-1, j-1], (i-1, j-1))   # Match\n",
        "                ]\n",
        "                min_val, prev = min(candidates, key=lambda x: x[0])\n",
        "                D[i, j] = cost + min_val\n",
        "                path[(i, j)] = prev\n",
        "        \n",
        "        # Reconstruct alignment path (backtracking)\n",
        "        alignment = []\n",
        "        i, j = n1, n2\n",
        "        while i > 0 and j > 0:\n",
        "            alignment.append((i-1, j-1))\n",
        "            if (i, j) in path:\n",
        "                i, j = path[(i, j)]\n",
        "            else:\n",
        "                # Fallback: move diagonally\n",
        "                i -= 1\n",
        "                j -= 1\n",
        "        \n",
        "        # Handle remaining frames\n",
        "        while i > 0:\n",
        "            alignment.append((i-1, 0))\n",
        "            i -= 1\n",
        "        while j > 0:\n",
        "            alignment.append((0, j-1))\n",
        "            j -= 1\n",
        "        \n",
        "        alignment.reverse()\n",
        "        \n",
        "        # Build aligned sequences\n",
        "        aligned_seq1 = []\n",
        "        aligned_seq2 = []\n",
        "        for i_idx, j_idx in alignment:\n",
        "            aligned_seq1.append(seq1[i_idx])\n",
        "            aligned_seq2.append(seq2[j_idx])\n",
        "        \n",
        "        return np.array(aligned_seq1), np.array(aligned_seq2)\n",
        "    \n",
        "    def _compute_laplacian(self, W):\n",
        "        \"\"\"\n",
        "        Compute normalized graph Laplacian: L = I - D^(-1/2) W D^(-1/2)\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        W : sparse matrix\n",
        "            Adjacency/affinity matrix\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        L : sparse matrix\n",
        "            Normalized Laplacian\n",
        "        \"\"\"\n",
        "        # Degree matrix\n",
        "        D = np.array(W.sum(axis=1)).flatten()\n",
        "        D_sqrt_inv = 1.0 / np.sqrt(D + 1e-10)  # Avoid division by zero\n",
        "        \n",
        "        # Normalized Laplacian: L = I - D^(-1/2) W D^(-1/2)\n",
        "        D_sqrt_inv_diag = diags(D_sqrt_inv, format=W.format)\n",
        "        I = identity(W.shape[0], format=W.format)\n",
        "        L = I - D_sqrt_inv_diag @ W @ D_sqrt_inv_diag\n",
        "        \n",
        "        return L\n",
        "    \n",
        "    def _build_spatial_graph(self, frame, landmarks_per_frame=42):\n",
        "        \"\"\"\n",
        "        Build spatial graph for one frame (hand structure)\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        frame : array (126,)\n",
        "            One frame with 42 landmarks √ó 3 coordinates\n",
        "        landmarks_per_frame : int\n",
        "            Number of landmarks per frame (default 42)\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        W_spatial : sparse matrix\n",
        "            Spatial affinity matrix (42 √ó 42)\n",
        "        \"\"\"\n",
        "        # Reshape to landmarks: (126,) ‚Üí (42, 3)\n",
        "        landmarks = frame.reshape(landmarks_per_frame, 3)\n",
        "        \n",
        "        # Build k-NN graph of landmarks\n",
        "        W_spatial = kneighbors_graph(\n",
        "            landmarks, \n",
        "            n_neighbors=self.n_neighbors_spatial,\n",
        "            mode='connectivity',\n",
        "            include_self=False\n",
        "        )\n",
        "        \n",
        "        # Make symmetric\n",
        "        W_spatial = (W_spatial + W_spatial.T) / 2\n",
        "        \n",
        "        return W_spatial\n",
        "    \n",
        "    def _extract_temporal_features(self, sequence):\n",
        "        \"\"\"\n",
        "        Extract temporal features for 5-second video at 30fps.\n",
        "        Captures motion dynamics, velocity, acceleration, and temporal phases.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        sequence : array (n_frames, n_features)\n",
        "            Gesture sequence\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        features : dict\n",
        "            Dictionary of temporal features\n",
        "        \"\"\"\n",
        "        n_frames = len(sequence)\n",
        "        \n",
        "        # 1. Velocity (frame-to-frame differences) - captures motion speed\n",
        "        velocity = np.diff(sequence, axis=0)  # (n_frames-1, n_features)\n",
        "        mean_velocity = np.mean(velocity, axis=0)\n",
        "        velocity_magnitude = np.linalg.norm(mean_velocity)\n",
        "        \n",
        "        # 2. Acceleration (second-order differences) - captures motion changes\n",
        "        if len(velocity) > 1:\n",
        "            acceleration = np.diff(velocity, axis=0)  # (n_frames-2, n_features)\n",
        "            mean_acceleration = np.mean(acceleration, axis=0)\n",
        "            acceleration_magnitude = np.linalg.norm(mean_acceleration)\n",
        "        else:\n",
        "            mean_acceleration = np.zeros(sequence.shape[1])\n",
        "            acceleration_magnitude = 0.0\n",
        "        \n",
        "        # 3. Temporal phases (early, middle, late) - captures gesture progression\n",
        "        # Divide 5-second video into 3 phases: 0-1.67s, 1.67-3.33s, 3.33-5s\n",
        "        phase_size = n_frames // 3\n",
        "        early_phase = sequence[:phase_size] if phase_size > 0 else sequence[:1]\n",
        "        middle_phase = sequence[phase_size:2*phase_size] if 2*phase_size <= n_frames else sequence[phase_size:]\n",
        "        late_phase = sequence[2*phase_size:] if 2*phase_size < n_frames else sequence[-1:]\n",
        "        \n",
        "        early_mean = np.mean(early_phase, axis=0)\n",
        "        middle_mean = np.mean(middle_phase, axis=0)\n",
        "        late_mean = np.mean(late_phase, axis=0)\n",
        "        \n",
        "        # 4. Motion direction (trajectory)\n",
        "        # Start to end vector\n",
        "        trajectory = sequence[-1] - sequence[0]\n",
        "        trajectory_magnitude = np.linalg.norm(trajectory)\n",
        "        \n",
        "        # 5. Motion smoothness (variance of velocity)\n",
        "        velocity_variance = np.var(velocity, axis=0).mean() if len(velocity) > 0 else 0.0\n",
        "        \n",
        "        return {\n",
        "            'mean_frame': np.mean(sequence, axis=0),\n",
        "            'velocity': mean_velocity,\n",
        "            'velocity_magnitude': velocity_magnitude,\n",
        "            'acceleration': mean_acceleration,\n",
        "            'acceleration_magnitude': acceleration_magnitude,\n",
        "            'early_phase': early_mean,\n",
        "            'middle_phase': middle_mean,\n",
        "            'late_phase': late_mean,\n",
        "            'trajectory': trajectory,\n",
        "            'trajectory_magnitude': trajectory_magnitude,\n",
        "            'velocity_variance': velocity_variance,\n",
        "            'sequence_length': n_frames\n",
        "        }\n",
        "    \n",
        "    def _compute_temporal_similarity(self, seq1, seq2):\n",
        "        \"\"\"\n",
        "        Compute temporal similarity between two sequences\n",
        "        IMPROVED: Uses temporal features for 5-second videos at 30fps\n",
        "        Supports A/B testing with configurable weights\n",
        "        NEW: Supports DTW alignment for optimal temporal matching\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        seq1, seq2 : array (n_frames, n_features)\n",
        "            Two gesture sequences (5 seconds at 30fps = 150 frames)\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        similarity : float\n",
        "            Similarity score (0 to 1)\n",
        "        \"\"\"\n",
        "        # If temporal features disabled, use only mean frame\n",
        "        if not self.use_temporal_features:\n",
        "            mean1 = np.mean(seq1, axis=0)\n",
        "            mean2 = np.mean(seq2, axis=0)\n",
        "            dist = np.linalg.norm(mean1 - mean2)\n",
        "            return 1.0 / (1.0 + dist)\n",
        "        \n",
        "        # Apply DTW alignment if enabled\n",
        "        if self.use_dtw:\n",
        "            # Align sequences using DTW before feature extraction\n",
        "            aligned_seq1, aligned_seq2 = self._dtw_align_sequences(seq1, seq2)\n",
        "            # Extract features from aligned sequences\n",
        "            feat1 = self._extract_temporal_features(aligned_seq1)\n",
        "            feat2 = self._extract_temporal_features(aligned_seq2)\n",
        "            # Also compute DTW distance as an additional feature\n",
        "            dtw_dist = self._dtw_distance(seq1, seq2)\n",
        "        else:\n",
        "            # Extract temporal features without alignment\n",
        "            feat1 = self._extract_temporal_features(seq1)\n",
        "            feat2 = self._extract_temporal_features(seq2)\n",
        "            dtw_dist = None\n",
        "        \n",
        "        # Compute distances for each feature type\n",
        "        distances = {}\n",
        "        \n",
        "        # 1. Mean frame similarity (static pose)\n",
        "        distances['mean'] = np.linalg.norm(feat1['mean_frame'] - feat2['mean_frame'])\n",
        "        \n",
        "        # 2. Velocity similarity (motion speed)\n",
        "        distances['velocity'] = np.linalg.norm(feat1['velocity'] - feat2['velocity'])\n",
        "        distances['velocity_mag'] = abs(feat1['velocity_magnitude'] - feat2['velocity_magnitude'])\n",
        "        \n",
        "        # 3. Acceleration similarity (motion changes)\n",
        "        distances['acceleration'] = np.linalg.norm(feat1['acceleration'] - feat2['acceleration'])\n",
        "        distances['acceleration_mag'] = abs(feat1['acceleration_magnitude'] - feat2['acceleration_magnitude'])\n",
        "        \n",
        "        # 4. Temporal phase similarity (gesture progression)\n",
        "        distances['early'] = np.linalg.norm(feat1['early_phase'] - feat2['early_phase'])\n",
        "        distances['middle'] = np.linalg.norm(feat1['middle_phase'] - feat2['middle_phase'])\n",
        "        distances['late'] = np.linalg.norm(feat1['late_phase'] - feat2['late_phase'])\n",
        "        \n",
        "        # 5. Trajectory similarity (motion direction)\n",
        "        distances['trajectory'] = np.linalg.norm(feat1['trajectory'] - feat2['trajectory'])\n",
        "        distances['trajectory_mag'] = abs(feat1['trajectory_magnitude'] - feat2['trajectory_magnitude'])\n",
        "        \n",
        "        # 6. Motion smoothness\n",
        "        distances['smoothness'] = abs(feat1['velocity_variance'] - feat2['velocity_variance'])\n",
        "        \n",
        "        # 7. DTW distance (if DTW is enabled)\n",
        "        if dtw_dist is not None:\n",
        "            # Normalize DTW distance by average sequence length for comparability\n",
        "            avg_length = (len(seq1) + len(seq2)) / 2.0\n",
        "            distances['dtw'] = dtw_dist / (avg_length + 1e-10)\n",
        "        else:\n",
        "            distances['dtw'] = 0.0\n",
        "        \n",
        "        # Weighted combination using configurable weights\n",
        "        # If DTW is enabled, add DTW distance to the combination\n",
        "        dtw_weight = self.temporal_weights.get('dtw', 0.0) if self.use_dtw else 0.0\n",
        "        \n",
        "        # Normalize weights if DTW is added (to keep total weight = 1.0)\n",
        "        if dtw_weight > 0:\n",
        "            weight_sum = sum(self.temporal_weights.values()) + dtw_weight\n",
        "            scale = 1.0 / weight_sum if weight_sum > 0 else 1.0\n",
        "        else:\n",
        "            scale = 1.0\n",
        "        \n",
        "        combined_dist = (\n",
        "            scale * self.temporal_weights.get('static', 0.15) * distances['mean'] +\n",
        "            scale * self.temporal_weights.get('velocity', 0.20) * distances['velocity'] +\n",
        "            scale * self.temporal_weights.get('velocity_mag', 0.10) * distances['velocity_mag'] +\n",
        "            scale * self.temporal_weights.get('acceleration', 0.10) * distances['acceleration'] +\n",
        "            scale * self.temporal_weights.get('early', 0.10) * distances['early'] +\n",
        "            scale * self.temporal_weights.get('middle', 0.10) * distances['middle'] +\n",
        "            scale * self.temporal_weights.get('late', 0.10) * distances['late'] +\n",
        "            scale * self.temporal_weights.get('trajectory', 0.10) * distances['trajectory'] +\n",
        "            scale * self.temporal_weights.get('smoothness', 0.05) * distances['smoothness']\n",
        "        )\n",
        "        \n",
        "        # Add DTW distance if enabled\n",
        "        if dtw_weight > 0:\n",
        "            combined_dist += scale * dtw_weight * distances['dtw']\n",
        "        \n",
        "        # Convert to similarity\n",
        "        similarity = 1.0 / (1.0 + combined_dist)\n",
        "        \n",
        "        return similarity\n",
        "    \n",
        "    def _build_temporal_graph(self, sequences, similarity_threshold=None, top_k=None, percentile_threshold=None):\n",
        "        \"\"\"\n",
        "        Build temporal graph connecting sequences with optional sparsification\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        sequences : list of arrays\n",
        "            List of gesture sequences\n",
        "        similarity_threshold : float or None\n",
        "            If provided, only connect sequences with similarity >= threshold\n",
        "        top_k : int or None\n",
        "            If provided, keep only top-k most similar sequences per node\n",
        "        percentile_threshold : float or None\n",
        "            If provided, use percentile-based threshold (0-100)\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        W_temporal : array (n_sequences, n_sequences)\n",
        "            Temporal affinity matrix (sparse if thresholding applied)\n",
        "        \"\"\"\n",
        "        n_sequences = len(sequences)\n",
        "        W_temporal = np.zeros((n_sequences, n_sequences))\n",
        "        \n",
        "        print(f\"Building temporal graph for {n_sequences} sequences...\")\n",
        "        \n",
        "        # Compute all similarities first\n",
        "        all_similarities = []\n",
        "        for i in range(n_sequences):\n",
        "            if i % max(1, n_sequences // 20) == 0:\n",
        "                print(f\"  Processing sequence {i}/{n_sequences} ({100*i/n_sequences:.1f}%)\")\n",
        "            \n",
        "            for j in range(i+1, n_sequences):\n",
        "                similarity = self._compute_temporal_similarity(sequences[i], sequences[j])\n",
        "                all_similarities.append((i, j, similarity))\n",
        "        \n",
        "        # Determine threshold if percentile-based\n",
        "        if percentile_threshold is not None:\n",
        "            sim_values = [s[2] for s in all_similarities]\n",
        "            threshold = np.percentile(sim_values, percentile_threshold)\n",
        "            print(f\"  Percentile threshold ({percentile_threshold}%): {threshold:.4f}\")\n",
        "        elif similarity_threshold is not None:\n",
        "            threshold = similarity_threshold\n",
        "        else:\n",
        "            threshold = 0.0\n",
        "        \n",
        "        # Build graph with thresholding\n",
        "        for i, j, similarity in all_similarities:\n",
        "            if similarity >= threshold:\n",
        "                W_temporal[i, j] = similarity\n",
        "                W_temporal[j, i] = similarity  # Symmetric\n",
        "        \n",
        "        # Apply top-k sparsification if requested (after thresholding)\n",
        "        if top_k is not None and top_k < n_sequences - 1:\n",
        "            print(f\"  Applying top-{top_k} sparsification...\")\n",
        "            W_temporal_sparse = np.zeros_like(W_temporal)\n",
        "            for i in range(n_sequences):\n",
        "                # Get top-k neighbors for node i\n",
        "                similarities = W_temporal[i, :]\n",
        "                top_k_indices = np.argsort(similarities)[-top_k-1:-1]  # Exclude self\n",
        "                for j in top_k_indices:\n",
        "                    if W_temporal[i, j] > 0:\n",
        "                        W_temporal_sparse[i, j] = W_temporal[i, j]\n",
        "                        W_temporal_sparse[j, i] = W_temporal[i, j]\n",
        "            W_temporal = W_temporal_sparse\n",
        "        \n",
        "        # Normalize (only if max > 0)\n",
        "        max_sim = W_temporal.max()\n",
        "        if max_sim > 0:\n",
        "            W_temporal = W_temporal / max_sim\n",
        "        \n",
        "        return W_temporal\n",
        "    \n",
        "    def fit_predict(self, sequences):\n",
        "        \"\"\"\n",
        "        Fit STC and return cluster labels\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        sequences : list of arrays\n",
        "            List of gesture sequences, each is (n_frames, n_features)\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        labels : array (n_sequences,)\n",
        "            Cluster assignments\n",
        "        \"\"\"\n",
        "        n_sequences = len(sequences)\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"Step 1: Building Spatial Graphs\")\n",
        "        print(\"=\" * 70)\n",
        "        \n",
        "        # Use mean frame representation per sequence (per-frame spatial doesn't improve results)\n",
        "        print(\"Computing mean frame representation for each sequence...\")\n",
        "        mean_frames = []\n",
        "        for seq in sequences:\n",
        "            mean_frame = np.mean(seq, axis=0)\n",
        "            mean_frames.append(mean_frame)\n",
        "        \n",
        "        mean_frames = np.array(mean_frames)  # (n_sequences, 126)\n",
        "        \n",
        "        print(\"Building spatial k-NN graph...\")\n",
        "        W_spatial = kneighbors_graph(\n",
        "            mean_frames,\n",
        "            n_neighbors=self.n_neighbors_spatial,\n",
        "            mode='connectivity',\n",
        "            include_self=False\n",
        "        )\n",
        "        W_spatial = (W_spatial + W_spatial.T) / 2  # Make symmetric\n",
        "        \n",
        "        print(f\"Spatial graph: {W_spatial.shape}, density: {W_spatial.nnz / (W_spatial.shape[0] * W_spatial.shape[1]):.4f}\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"Step 2: Building Temporal Graph\")\n",
        "        print(\"=\" * 70)\n",
        "        \n",
        "        # Get sparsification parameters from instance (set during initialization or optimization)\n",
        "        similarity_threshold = getattr(self, '_temporal_threshold', None)\n",
        "        top_k = getattr(self, '_temporal_top_k', None)\n",
        "        percentile_threshold = getattr(self, '_temporal_percentile', None)\n",
        "        \n",
        "        W_temporal = self._build_temporal_graph(\n",
        "            sequences,\n",
        "            similarity_threshold=similarity_threshold,\n",
        "            top_k=top_k,\n",
        "            percentile_threshold=percentile_threshold\n",
        "        )\n",
        "        \n",
        "        # Store parameters for use in predict method\n",
        "        self._temporal_threshold = similarity_threshold\n",
        "        self._temporal_top_k = top_k\n",
        "        self._temporal_percentile = percentile_threshold\n",
        "        \n",
        "        graph_density = (W_temporal > 0).sum() / (W_temporal.shape[0] * W_temporal.shape[1])\n",
        "        print(f\"Temporal graph: {W_temporal.shape}, density: {graph_density:.4f} ({graph_density*100:.2f}%)\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"Step 3: Computing Laplacians\")\n",
        "        print(\"=\" * 70)\n",
        "        L_spatial = self._compute_laplacian(W_spatial)\n",
        "        print(\"Spatial Laplacian computed\")\n",
        "        \n",
        "        # Convert temporal to sparse for consistency\n",
        "        from scipy.sparse import csr_matrix\n",
        "        W_temporal_sparse = csr_matrix(W_temporal)\n",
        "        L_temporal = self._compute_laplacian(W_temporal_sparse)\n",
        "        print(\"Temporal Laplacian computed\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"Step 4: Combining Laplacians\")\n",
        "        print(\"=\" * 70)\n",
        "        L_joint = self.alpha * L_spatial + (1 - self.alpha) * L_temporal\n",
        "        print(f\"Joint Laplacian: Œ±={self.alpha} (spatial) + {1-self.alpha} (temporal)\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"Step 5: Spectral Decomposition\")\n",
        "        print(\"=\" * 70)\n",
        "        try:\n",
        "            eigenvalues, eigenvectors = eigsh(\n",
        "                L_joint, \n",
        "                k=self.n_clusters, \n",
        "                which='SM',  # Smallest eigenvalues\n",
        "                maxiter=5000,\n",
        "                tol=1e-6\n",
        "            )\n",
        "            print(f\"Computed {self.n_clusters} smallest eigenvalues\")\n",
        "            print(f\"Eigenvalue range: [{eigenvalues.min():.6f}, {eigenvalues.max():.6f}]\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è eigsh failed: {e}\")\n",
        "            print(\"Falling back to dense eigendecomposition...\")\n",
        "            L_joint_dense = L_joint.toarray()\n",
        "            eigenvalues, eigenvectors = np.linalg.eigh(L_joint_dense)\n",
        "            eigenvectors = eigenvectors[:, :self.n_clusters]\n",
        "            eigenvalues = eigenvalues[:self.n_clusters]\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"Step 6: K-Means Clustering in Spectral Space\")\n",
        "        print(\"=\" * 70)\n",
        "        kmeans = KMeans(\n",
        "            n_clusters=self.n_clusters,\n",
        "            random_state=self.random_state,\n",
        "            n_init=10\n",
        "        )\n",
        "        labels = kmeans.fit_predict(eigenvectors)\n",
        "        \n",
        "        # Store results\n",
        "        self.sequences_ = sequences\n",
        "        self.W_spatial_ = W_spatial\n",
        "        self.W_temporal_ = W_temporal\n",
        "        self.L_spatial_ = L_spatial\n",
        "        self.L_temporal_ = L_temporal\n",
        "        self.L_joint_ = L_joint\n",
        "        self.eigenvalues_ = eigenvalues\n",
        "        self.eigenvectors_ = eigenvectors\n",
        "        self.labels_ = labels\n",
        "        \n",
        "        print(f\"‚úÖ Clustering complete!\")\n",
        "        print(f\"   Found {len(np.unique(labels))} clusters\")\n",
        "        print(f\"   Cluster distribution: {np.bincount(labels)}\")\n",
        "        \n",
        "        # Store kmeans for prediction\n",
        "        self.kmeans_ = kmeans\n",
        "        \n",
        "        return labels\n",
        "    \n",
        "    def predict(self, eval_sequences):\n",
        "        \"\"\"\n",
        "        Predict cluster labels for evaluation sequences by projecting into learned spectral space.\n",
        "        IMPROVED: Uses spectral projection instead of naive nearest neighbor.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        eval_sequences : list of arrays\n",
        "            List of evaluation gesture sequences, each is (n_frames, n_features)\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        labels : array (n_eval_sequences,)\n",
        "            Predicted cluster assignments\n",
        "        \"\"\"\n",
        "        if not hasattr(self, 'eigenvectors_'):\n",
        "            raise ValueError(\"Model must be fitted before prediction. Call fit_predict() first.\")\n",
        "        \n",
        "        n_eval = len(eval_sequences)\n",
        "        n_train = len(self.sequences_)\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"Predicting Evaluation Sequences (Spectral Projection)\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"Evaluation sequences: {n_eval}\")\n",
        "        print(f\"Training sequences: {n_train}\")\n",
        "        \n",
        "        # Step 1: Build spatial graph for evaluation sequences\n",
        "        print(\"\\nStep 1: Building spatial graph for evaluation sequences...\")\n",
        "        eval_spatial_features = np.array([np.mean(seq, axis=0) for seq in eval_sequences])\n",
        "        \n",
        "        # Build spatial graph connecting eval to training\n",
        "        # Combine training and eval features\n",
        "        train_spatial_features = np.array([np.mean(seq, axis=0) for seq in self.sequences_])\n",
        "        combined_features = np.vstack([train_spatial_features, eval_spatial_features])\n",
        "        \n",
        "        # Build k-NN graph on combined features\n",
        "        W_spatial_combined = kneighbors_graph(\n",
        "            combined_features,\n",
        "            n_neighbors=self.n_neighbors_spatial,\n",
        "            mode='connectivity',\n",
        "            include_self=False\n",
        "        )\n",
        "        W_spatial_combined = (W_spatial_combined + W_spatial_combined.T) / 2\n",
        "        \n",
        "        # Extract eval-to-train and eval-to-eval connections\n",
        "        W_spatial_eval_train = W_spatial_combined[n_train:, :n_train]  # (n_eval, n_train)\n",
        "        W_spatial_eval_eval = W_spatial_combined[n_train:, n_train:]  # (n_eval, n_eval)\n",
        "        \n",
        "        # Step 2: Build temporal graph (eval sequences vs training sequences)\n",
        "        print(\"Step 2: Building temporal graph (eval vs training)...\")\n",
        "        W_temporal_eval = np.zeros((n_eval, n_train))\n",
        "        \n",
        "        # Get threshold parameters from training (if used)\n",
        "        # These are stored when _build_temporal_graph is called during training\n",
        "        temporal_threshold = getattr(self, '_temporal_threshold', None)\n",
        "        temporal_top_k = getattr(self, '_temporal_top_k', None)\n",
        "        \n",
        "        all_similarities = []\n",
        "        for i in range(n_eval):\n",
        "            if i % max(1, n_eval // 20) == 0:\n",
        "                print(f\"  Processing eval sequence {i}/{n_eval} ({100*i/n_eval:.1f}%)\")\n",
        "            for j in range(n_train):\n",
        "                similarity = self._compute_temporal_similarity(eval_sequences[i], self.sequences_[j])\n",
        "                all_similarities.append((i, j, similarity))\n",
        "        \n",
        "        # Apply thresholding if used during training\n",
        "        threshold = temporal_threshold if temporal_threshold is not None else 0.0\n",
        "        for i, j, similarity in all_similarities:\n",
        "            if similarity >= threshold:\n",
        "                W_temporal_eval[i, j] = similarity\n",
        "        \n",
        "        # Apply top-k if used during training\n",
        "        if temporal_top_k is not None:\n",
        "            for i in range(n_eval):\n",
        "                similarities = W_temporal_eval[i, :]\n",
        "                top_k_indices = np.argsort(similarities)[-temporal_top_k:]\n",
        "                W_temporal_eval_sparse = np.zeros_like(W_temporal_eval)\n",
        "                for j in top_k_indices:\n",
        "                    if W_temporal_eval[i, j] > 0:\n",
        "                        W_temporal_eval_sparse[i, j] = W_temporal_eval[i, j]\n",
        "                W_temporal_eval = W_temporal_eval_sparse\n",
        "        \n",
        "        # Normalize (only if max > 0)\n",
        "        max_sim = W_temporal_eval.max()\n",
        "        if max_sim > 0:\n",
        "            W_temporal_eval = W_temporal_eval / max_sim\n",
        "        \n",
        "        # Step 3: Project into spectral space using Nystr√∂m extension\n",
        "        print(\"Step 3: Projecting into spectral space (Nystr√∂m extension)...\")\n",
        "        \n",
        "        # Compute Laplacians\n",
        "        from scipy.sparse import csr_matrix, vstack, hstack\n",
        "        \n",
        "        # Spatial Laplacian for eval (connect to training)\n",
        "        L_spatial_eval = self._compute_laplacian(csr_matrix(W_spatial_eval_eval))\n",
        "        \n",
        "        # Temporal Laplacian for eval (connect to training)\n",
        "        L_temporal_eval = self._compute_laplacian(csr_matrix(W_temporal_eval))\n",
        "        \n",
        "        # Joint Laplacian for eval\n",
        "        L_joint_eval = self.alpha * L_spatial_eval + (1 - self.alpha) * L_temporal_eval\n",
        "        \n",
        "        # Nystr√∂m extension: Project eval sequences using training eigenvectors\n",
        "        # Approximate: eval_embedding ‚âà L_eval_train @ train_eigenvectors\n",
        "        # For simplicity, use temporal similarity to project\n",
        "        eval_embedding = W_temporal_eval @ self.eigenvectors_  # (n_eval, n_clusters)\n",
        "        \n",
        "        # Normalize embedding\n",
        "        eval_embedding = eval_embedding / (np.linalg.norm(eval_embedding, axis=1, keepdims=True) + 1e-10)\n",
        "        \n",
        "        # Step 4: Assign to nearest cluster in spectral space\n",
        "        print(\"Step 4: Assigning to clusters in spectral space...\")\n",
        "        labels = self.kmeans_.predict(eval_embedding)\n",
        "        \n",
        "        print(f\"‚úÖ Prediction complete!\")\n",
        "        print(f\"   Cluster distribution: {np.bincount(labels)}\")\n",
        "        \n",
        "        return labels\n",
        "\n",
        "print(\"‚úÖ SpectralTemporalClustering class defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "Scaling TRAINING Sequences (from combined.csv)\n",
            "======================================================================\n",
            "Scaled 320 training sequences\n",
            "Total frames: 47,954\n",
            "Features per frame: 126\n",
            "\n",
            "======================================================================\n",
            "Scaling EVALUATION Sequences (using training scaler)\n",
            "======================================================================\n",
            "Scaled 320 evaluation sequences\n",
            "Total frames: 47,715\n"
          ]
        }
      ],
      "source": [
        "# Scale TRAINING sequences for clustering\n",
        "print(\"=\" * 70)\n",
        "print(\"Scaling TRAINING Sequences (from combined.csv)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Flatten all training sequences for scaling\n",
        "all_frames_train = np.vstack(SEQUENCES_TRAIN)\n",
        "scaler = StandardScaler()\n",
        "all_frames_scaled_train = scaler.fit_transform(all_frames_train)\n",
        "\n",
        "# Reconstruct scaled training sequences\n",
        "sequences_scaled_train = []\n",
        "current_idx = 0\n",
        "for seq in SEQUENCES_TRAIN:\n",
        "    seq_len = len(seq)\n",
        "    seq_scaled = all_frames_scaled_train[current_idx:current_idx + seq_len]\n",
        "    sequences_scaled_train.append(seq_scaled)\n",
        "    current_idx += seq_len\n",
        "\n",
        "print(f\"Scaled {len(sequences_scaled_train)} training sequences\")\n",
        "print(f\"Total frames: {len(all_frames_scaled_train):,}\")\n",
        "print(f\"Features per frame: {all_frames_scaled_train.shape[1]}\")\n",
        "\n",
        "# Store training data\n",
        "SEQUENCES_SCALED_TRAIN = sequences_scaled_train\n",
        "SCALER = scaler\n",
        "\n",
        "# Also scale evaluation sequences using the SAME scaler (fit on training data)\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Scaling EVALUATION Sequences (using training scaler)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "all_frames_eval = np.vstack(SEQUENCES_EVAL)\n",
        "all_frames_scaled_eval = scaler.transform(all_frames_eval)  # Use transform, not fit_transform\n",
        "\n",
        "sequences_scaled_eval = []\n",
        "current_idx = 0\n",
        "for seq in SEQUENCES_EVAL:\n",
        "    seq_len = len(seq)\n",
        "    seq_scaled = all_frames_scaled_eval[current_idx:current_idx + seq_len]\n",
        "    sequences_scaled_eval.append(seq_scaled)\n",
        "    current_idx += seq_len\n",
        "\n",
        "print(f\"Scaled {len(sequences_scaled_eval)} evaluation sequences\")\n",
        "print(f\"Total frames: {len(all_frames_scaled_eval):,}\")\n",
        "\n",
        "# Store evaluation data\n",
        "SEQUENCES_SCALED_EVAL = sequences_scaled_eval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Loaded optimized temporal graph configuration:\n",
            "   Method: similarity_threshold\n",
            "   Accuracy: 0.4562 (45.62%)\n",
            "   Graph Density: 0.1818 (18.18%)\n",
            "   Parameters: Threshold=0.30\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# NOTE: Temporal Graph Sparsification Optimization (COMPLETED)\n",
        "# ============================================================================\n",
        "#\n",
        "# Optimization has been completed. Results saved to:\n",
        "# STC_Results/temporal_graph_sparsification_optimization.json\n",
        "#\n",
        "# Best Configuration Found:\n",
        "# - Method: similarity_threshold\n",
        "# - Threshold: 0.3\n",
        "# - Accuracy: 45.625% (improvement from 44.0625%)\n",
        "# - Graph Density: 18.18% (reduced from 99.69%)\n",
        "#\n",
        "# Key Findings:\n",
        "# - Similarity threshold = 0.3 is optimal (45.625% accuracy)\n",
        "# - Percentile methods work well (90th percentile = 42.8125%)\n",
        "# - Top-k and combined methods fail (12.5% - likely disconnected graphs)\n",
        "#\n",
        "# This configuration is now applied automatically in Cell 8.\n",
        "#\n",
        "\n",
        "# Optimization completed. Results loaded from saved file.\n",
        "# Best configuration: similarity_threshold=0.3 (45.625% accuracy, 18.18% graph density)\n",
        "\n",
        "# Load results from saved optimization\n",
        "import json\n",
        "output_dir = 'STC_Results'\n",
        "sparsification_results_path = os.path.join(output_dir, 'temporal_graph_sparsification_optimization.json')\n",
        "if os.path.exists(sparsification_results_path):\n",
        "    with open(sparsification_results_path, 'r') as f:\n",
        "        opt_results = json.load(f)\n",
        "    \n",
        "    BEST_TEMPORAL_GRAPH_CONFIG = opt_results.get('best_configuration')\n",
        "    \n",
        "    if BEST_TEMPORAL_GRAPH_CONFIG:\n",
        "        print(f\"\\n‚úÖ Loaded optimized temporal graph configuration:\")\n",
        "        print(f\"   Method: {BEST_TEMPORAL_GRAPH_CONFIG['method']}\")\n",
        "        print(f\"   Accuracy: {BEST_TEMPORAL_GRAPH_CONFIG['accuracy']:.4f} ({BEST_TEMPORAL_GRAPH_CONFIG['accuracy']*100:.2f}%)\")\n",
        "        print(f\"   Graph Density: {BEST_TEMPORAL_GRAPH_CONFIG['graph_density']:.4f} ({BEST_TEMPORAL_GRAPH_CONFIG['graph_density']*100:.2f}%)\")\n",
        "        print(f\"   Parameters: {BEST_TEMPORAL_GRAPH_CONFIG['name']}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No best configuration found in results file\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Optimization results file not found. Using default configuration.\")\n",
        "    BEST_TEMPORAL_GRAPH_CONFIG = None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# NOTE: A/B Testing (Completed)\n",
        "# ============================================================================\n",
        "# \n",
        "# A/B testing has been completed and results saved.\n",
        "# See ANALYSIS_AND_RECOMMENDATIONS.md for detailed results.\n",
        "#\n",
        "# Key finding: Balanced Weights (50% static, 50% temporal) performs best.\n",
        "# Results saved to: STC_Results/ab_test_results.json\n",
        "#\n",
        "\n",
        "pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "Testing Lower Alpha Values (More Temporal Weight)\n",
            "======================================================================\n",
            "Current best: alpha=0.3 (70% temporal)\n",
            "Testing: alpha=0.2, 0.1, 0.0\n",
            "======================================================================\n",
            "\n",
            "Testing alpha=0.3 (70% temporal, 30% spatial)...\n",
            "\n",
            "======================================================================\n",
            "Step 1: Building Spatial Graphs\n",
            "======================================================================\n",
            "Computing mean frame representation for each sequence...\n",
            "Building spatial k-NN graph...\n",
            "Spatial graph: (320, 320), density: 0.0206\n",
            "\n",
            "======================================================================\n",
            "Step 2: Building Temporal Graph\n",
            "======================================================================\n",
            "Building temporal graph for 320 sequences...\n",
            "  Processing sequence 0/320 (0.0%)\n",
            "  Processing sequence 16/320 (5.0%)\n",
            "  Processing sequence 32/320 (10.0%)\n",
            "  Processing sequence 48/320 (15.0%)\n",
            "  Processing sequence 64/320 (20.0%)\n",
            "  Processing sequence 80/320 (25.0%)\n",
            "  Processing sequence 96/320 (30.0%)\n",
            "  Processing sequence 112/320 (35.0%)\n",
            "  Processing sequence 128/320 (40.0%)\n",
            "  Processing sequence 144/320 (45.0%)\n",
            "  Processing sequence 160/320 (50.0%)\n",
            "  Processing sequence 176/320 (55.0%)\n",
            "  Processing sequence 192/320 (60.0%)\n",
            "  Processing sequence 208/320 (65.0%)\n",
            "  Processing sequence 224/320 (70.0%)\n",
            "  Processing sequence 240/320 (75.0%)\n",
            "  Processing sequence 256/320 (80.0%)\n",
            "  Processing sequence 272/320 (85.0%)\n",
            "  Processing sequence 288/320 (90.0%)\n",
            "  Processing sequence 304/320 (95.0%)\n",
            "Temporal graph: (320, 320), density: 0.1818 (18.18%)\n",
            "\n",
            "======================================================================\n",
            "Step 3: Computing Laplacians\n",
            "======================================================================\n",
            "Spatial Laplacian computed\n",
            "Temporal Laplacian computed\n",
            "\n",
            "======================================================================\n",
            "Step 4: Combining Laplacians\n",
            "======================================================================\n",
            "Joint Laplacian: Œ±=0.3 (spatial) + 0.7 (temporal)\n",
            "\n",
            "======================================================================\n",
            "Step 5: Spectral Decomposition\n",
            "======================================================================\n",
            "Computed 8 smallest eigenvalues\n",
            "Eigenvalue range: [0.002715, 0.293780]\n",
            "\n",
            "======================================================================\n",
            "Step 6: K-Means Clustering in Spectral Space\n",
            "======================================================================\n",
            "‚úÖ Clustering complete!\n",
            "   Found 8 clusters\n",
            "   Cluster distribution: [ 2 56 82 78 38 38  2 24]\n",
            "\n",
            "======================================================================\n",
            "Predicting Evaluation Sequences (Spectral Projection)\n",
            "======================================================================\n",
            "Evaluation sequences: 320\n",
            "Training sequences: 320\n",
            "\n",
            "Step 1: Building spatial graph for evaluation sequences...\n",
            "Step 2: Building temporal graph (eval vs training)...\n",
            "  Processing eval sequence 0/320 (0.0%)\n",
            "  Processing eval sequence 16/320 (5.0%)\n",
            "  Processing eval sequence 32/320 (10.0%)\n",
            "  Processing eval sequence 48/320 (15.0%)\n",
            "  Processing eval sequence 64/320 (20.0%)\n",
            "  Processing eval sequence 80/320 (25.0%)\n",
            "  Processing eval sequence 96/320 (30.0%)\n",
            "  Processing eval sequence 112/320 (35.0%)\n",
            "  Processing eval sequence 128/320 (40.0%)\n",
            "  Processing eval sequence 144/320 (45.0%)\n",
            "  Processing eval sequence 160/320 (50.0%)\n",
            "  Processing eval sequence 176/320 (55.0%)\n",
            "  Processing eval sequence 192/320 (60.0%)\n",
            "  Processing eval sequence 208/320 (65.0%)\n",
            "  Processing eval sequence 224/320 (70.0%)\n",
            "  Processing eval sequence 240/320 (75.0%)\n",
            "  Processing eval sequence 256/320 (80.0%)\n",
            "  Processing eval sequence 272/320 (85.0%)\n",
            "  Processing eval sequence 288/320 (90.0%)\n",
            "  Processing eval sequence 304/320 (95.0%)\n",
            "Step 3: Projecting into spectral space (Nystr√∂m extension)...\n",
            "Step 4: Assigning to clusters in spectral space...\n",
            "‚úÖ Prediction complete!\n",
            "   Cluster distribution: [  1  24 148  39  30  66   1  11]\n",
            "  Accuracy: 0.4562 (45.62%)\n",
            "\n",
            "Testing alpha=0.2 (80% temporal, 20% spatial)...\n",
            "\n",
            "======================================================================\n",
            "Step 1: Building Spatial Graphs\n",
            "======================================================================\n",
            "Computing mean frame representation for each sequence...\n",
            "Building spatial k-NN graph...\n",
            "Spatial graph: (320, 320), density: 0.0206\n",
            "\n",
            "======================================================================\n",
            "Step 2: Building Temporal Graph\n",
            "======================================================================\n",
            "Building temporal graph for 320 sequences...\n",
            "  Processing sequence 0/320 (0.0%)\n",
            "  Processing sequence 16/320 (5.0%)\n",
            "  Processing sequence 32/320 (10.0%)\n",
            "  Processing sequence 48/320 (15.0%)\n",
            "  Processing sequence 64/320 (20.0%)\n",
            "  Processing sequence 80/320 (25.0%)\n",
            "  Processing sequence 96/320 (30.0%)\n",
            "  Processing sequence 112/320 (35.0%)\n",
            "  Processing sequence 128/320 (40.0%)\n",
            "  Processing sequence 144/320 (45.0%)\n",
            "  Processing sequence 160/320 (50.0%)\n",
            "  Processing sequence 176/320 (55.0%)\n",
            "  Processing sequence 192/320 (60.0%)\n",
            "  Processing sequence 208/320 (65.0%)\n",
            "  Processing sequence 224/320 (70.0%)\n",
            "  Processing sequence 240/320 (75.0%)\n",
            "  Processing sequence 256/320 (80.0%)\n",
            "  Processing sequence 272/320 (85.0%)\n",
            "  Processing sequence 288/320 (90.0%)\n",
            "  Processing sequence 304/320 (95.0%)\n",
            "Temporal graph: (320, 320), density: 0.1818 (18.18%)\n",
            "\n",
            "======================================================================\n",
            "Step 3: Computing Laplacians\n",
            "======================================================================\n",
            "Spatial Laplacian computed\n",
            "Temporal Laplacian computed\n",
            "\n",
            "======================================================================\n",
            "Step 4: Combining Laplacians\n",
            "======================================================================\n",
            "Joint Laplacian: Œ±=0.2 (spatial) + 0.8 (temporal)\n",
            "\n",
            "======================================================================\n",
            "Step 5: Spectral Decomposition\n",
            "======================================================================\n",
            "Computed 8 smallest eigenvalues\n",
            "Eigenvalue range: [0.001990, 0.317343]\n",
            "\n",
            "======================================================================\n",
            "Step 6: K-Means Clustering in Spectral Space\n",
            "======================================================================\n",
            "‚úÖ Clustering complete!\n",
            "   Found 8 clusters\n",
            "   Cluster distribution: [ 2 80 80 78 36 38  2  4]\n",
            "\n",
            "======================================================================\n",
            "Predicting Evaluation Sequences (Spectral Projection)\n",
            "======================================================================\n",
            "Evaluation sequences: 320\n",
            "Training sequences: 320\n",
            "\n",
            "Step 1: Building spatial graph for evaluation sequences...\n",
            "Step 2: Building temporal graph (eval vs training)...\n",
            "  Processing eval sequence 0/320 (0.0%)\n",
            "  Processing eval sequence 16/320 (5.0%)\n",
            "  Processing eval sequence 32/320 (10.0%)\n",
            "  Processing eval sequence 48/320 (15.0%)\n",
            "  Processing eval sequence 64/320 (20.0%)\n",
            "  Processing eval sequence 80/320 (25.0%)\n",
            "  Processing eval sequence 96/320 (30.0%)\n",
            "  Processing eval sequence 112/320 (35.0%)\n",
            "  Processing eval sequence 128/320 (40.0%)\n",
            "  Processing eval sequence 144/320 (45.0%)\n",
            "  Processing eval sequence 160/320 (50.0%)\n",
            "  Processing eval sequence 176/320 (55.0%)\n",
            "  Processing eval sequence 192/320 (60.0%)\n",
            "  Processing eval sequence 208/320 (65.0%)\n",
            "  Processing eval sequence 224/320 (70.0%)\n",
            "  Processing eval sequence 240/320 (75.0%)\n",
            "  Processing eval sequence 256/320 (80.0%)\n",
            "  Processing eval sequence 272/320 (85.0%)\n",
            "  Processing eval sequence 288/320 (90.0%)\n",
            "  Processing eval sequence 304/320 (95.0%)\n",
            "Step 3: Projecting into spectral space (Nystr√∂m extension)...\n",
            "Step 4: Assigning to clusters in spectral space...\n",
            "‚úÖ Prediction complete!\n",
            "   Cluster distribution: [  1 163  20  39  27  66   1   3]\n",
            "  Accuracy: 0.4562 (45.62%)\n",
            "\n",
            "Testing alpha=0.1 (90% temporal, 10% spatial)...\n",
            "\n",
            "======================================================================\n",
            "Step 1: Building Spatial Graphs\n",
            "======================================================================\n",
            "Computing mean frame representation for each sequence...\n",
            "Building spatial k-NN graph...\n",
            "Spatial graph: (320, 320), density: 0.0206\n",
            "\n",
            "======================================================================\n",
            "Step 2: Building Temporal Graph\n",
            "======================================================================\n",
            "Building temporal graph for 320 sequences...\n",
            "  Processing sequence 0/320 (0.0%)\n",
            "  Processing sequence 16/320 (5.0%)\n",
            "  Processing sequence 32/320 (10.0%)\n",
            "  Processing sequence 48/320 (15.0%)\n",
            "  Processing sequence 64/320 (20.0%)\n",
            "  Processing sequence 80/320 (25.0%)\n",
            "  Processing sequence 96/320 (30.0%)\n",
            "  Processing sequence 112/320 (35.0%)\n",
            "  Processing sequence 128/320 (40.0%)\n",
            "  Processing sequence 144/320 (45.0%)\n",
            "  Processing sequence 160/320 (50.0%)\n",
            "  Processing sequence 176/320 (55.0%)\n",
            "  Processing sequence 192/320 (60.0%)\n",
            "  Processing sequence 208/320 (65.0%)\n",
            "  Processing sequence 224/320 (70.0%)\n",
            "  Processing sequence 240/320 (75.0%)\n",
            "  Processing sequence 256/320 (80.0%)\n",
            "  Processing sequence 272/320 (85.0%)\n",
            "  Processing sequence 288/320 (90.0%)\n",
            "  Processing sequence 304/320 (95.0%)\n",
            "Temporal graph: (320, 320), density: 0.1818 (18.18%)\n",
            "\n",
            "======================================================================\n",
            "Step 3: Computing Laplacians\n",
            "======================================================================\n",
            "Spatial Laplacian computed\n",
            "Temporal Laplacian computed\n",
            "\n",
            "======================================================================\n",
            "Step 4: Combining Laplacians\n",
            "======================================================================\n",
            "Joint Laplacian: Œ±=0.1 (spatial) + 0.9 (temporal)\n",
            "\n",
            "======================================================================\n",
            "Step 5: Spectral Decomposition\n",
            "======================================================================\n",
            "Computed 8 smallest eigenvalues\n",
            "Eigenvalue range: [0.001083, 0.306971]\n",
            "\n",
            "======================================================================\n",
            "Step 6: K-Means Clustering in Spectral Space\n",
            "======================================================================\n",
            "‚úÖ Clustering complete!\n",
            "   Found 8 clusters\n",
            "   Cluster distribution: [ 2 80 80 78 38 38  2  2]\n",
            "\n",
            "======================================================================\n",
            "Predicting Evaluation Sequences (Spectral Projection)\n",
            "======================================================================\n",
            "Evaluation sequences: 320\n",
            "Training sequences: 320\n",
            "\n",
            "Step 1: Building spatial graph for evaluation sequences...\n",
            "Step 2: Building temporal graph (eval vs training)...\n",
            "  Processing eval sequence 0/320 (0.0%)\n",
            "  Processing eval sequence 16/320 (5.0%)\n",
            "  Processing eval sequence 32/320 (10.0%)\n",
            "  Processing eval sequence 48/320 (15.0%)\n",
            "  Processing eval sequence 64/320 (20.0%)\n",
            "  Processing eval sequence 80/320 (25.0%)\n",
            "  Processing eval sequence 96/320 (30.0%)\n",
            "  Processing eval sequence 112/320 (35.0%)\n",
            "  Processing eval sequence 128/320 (40.0%)\n",
            "  Processing eval sequence 144/320 (45.0%)\n",
            "  Processing eval sequence 160/320 (50.0%)\n",
            "  Processing eval sequence 176/320 (55.0%)\n",
            "  Processing eval sequence 192/320 (60.0%)\n",
            "  Processing eval sequence 208/320 (65.0%)\n",
            "  Processing eval sequence 224/320 (70.0%)\n",
            "  Processing eval sequence 240/320 (75.0%)\n",
            "  Processing eval sequence 256/320 (80.0%)\n",
            "  Processing eval sequence 272/320 (85.0%)\n",
            "  Processing eval sequence 288/320 (90.0%)\n",
            "  Processing eval sequence 304/320 (95.0%)\n",
            "Step 3: Projecting into spectral space (Nystr√∂m extension)...\n",
            "Step 4: Assigning to clusters in spectral space...\n",
            "‚úÖ Prediction complete!\n",
            "   Cluster distribution: [  1 163  20  39  27  66   1   3]\n",
            "  Accuracy: 0.4562 (45.62%)\n",
            "\n",
            "Testing alpha=0.0 (100% temporal, 0% spatial)...\n",
            "\n",
            "======================================================================\n",
            "Step 1: Building Spatial Graphs\n",
            "======================================================================\n",
            "Computing mean frame representation for each sequence...\n",
            "Building spatial k-NN graph...\n",
            "Spatial graph: (320, 320), density: 0.0206\n",
            "\n",
            "======================================================================\n",
            "Step 2: Building Temporal Graph\n",
            "======================================================================\n",
            "Building temporal graph for 320 sequences...\n",
            "  Processing sequence 0/320 (0.0%)\n",
            "  Processing sequence 16/320 (5.0%)\n",
            "  Processing sequence 32/320 (10.0%)\n",
            "  Processing sequence 48/320 (15.0%)\n",
            "  Processing sequence 64/320 (20.0%)\n",
            "  Processing sequence 80/320 (25.0%)\n",
            "  Processing sequence 96/320 (30.0%)\n",
            "  Processing sequence 112/320 (35.0%)\n",
            "  Processing sequence 128/320 (40.0%)\n",
            "  Processing sequence 144/320 (45.0%)\n",
            "  Processing sequence 160/320 (50.0%)\n",
            "  Processing sequence 176/320 (55.0%)\n",
            "  Processing sequence 192/320 (60.0%)\n",
            "  Processing sequence 208/320 (65.0%)\n",
            "  Processing sequence 224/320 (70.0%)\n",
            "  Processing sequence 240/320 (75.0%)\n",
            "  Processing sequence 256/320 (80.0%)\n",
            "  Processing sequence 272/320 (85.0%)\n",
            "  Processing sequence 288/320 (90.0%)\n",
            "  Processing sequence 304/320 (95.0%)\n",
            "Temporal graph: (320, 320), density: 0.1818 (18.18%)\n",
            "\n",
            "======================================================================\n",
            "Step 3: Computing Laplacians\n",
            "======================================================================\n",
            "Spatial Laplacian computed\n",
            "Temporal Laplacian computed\n",
            "\n",
            "======================================================================\n",
            "Step 4: Combining Laplacians\n",
            "======================================================================\n",
            "Joint Laplacian: Œ±=0.0 (spatial) + 1.0 (temporal)\n",
            "\n",
            "======================================================================\n",
            "Step 5: Spectral Decomposition\n",
            "======================================================================\n",
            "Computed 8 smallest eigenvalues\n",
            "Eigenvalue range: [0.000000, 0.292704]\n",
            "\n",
            "======================================================================\n",
            "Step 6: K-Means Clustering in Spectral Space\n",
            "======================================================================\n",
            "‚úÖ Clustering complete!\n",
            "   Found 8 clusters\n",
            "   Cluster distribution: [ 2 80 80 78 38 38  2  2]\n",
            "\n",
            "======================================================================\n",
            "Predicting Evaluation Sequences (Spectral Projection)\n",
            "======================================================================\n",
            "Evaluation sequences: 320\n",
            "Training sequences: 320\n",
            "\n",
            "Step 1: Building spatial graph for evaluation sequences...\n",
            "Step 2: Building temporal graph (eval vs training)...\n",
            "  Processing eval sequence 0/320 (0.0%)\n",
            "  Processing eval sequence 16/320 (5.0%)\n",
            "  Processing eval sequence 32/320 (10.0%)\n",
            "  Processing eval sequence 48/320 (15.0%)\n",
            "  Processing eval sequence 64/320 (20.0%)\n",
            "  Processing eval sequence 80/320 (25.0%)\n",
            "  Processing eval sequence 96/320 (30.0%)\n",
            "  Processing eval sequence 112/320 (35.0%)\n",
            "  Processing eval sequence 128/320 (40.0%)\n",
            "  Processing eval sequence 144/320 (45.0%)\n",
            "  Processing eval sequence 160/320 (50.0%)\n",
            "  Processing eval sequence 176/320 (55.0%)\n",
            "  Processing eval sequence 192/320 (60.0%)\n",
            "  Processing eval sequence 208/320 (65.0%)\n",
            "  Processing eval sequence 224/320 (70.0%)\n",
            "  Processing eval sequence 240/320 (75.0%)\n",
            "  Processing eval sequence 256/320 (80.0%)\n",
            "  Processing eval sequence 272/320 (85.0%)\n",
            "  Processing eval sequence 288/320 (90.0%)\n",
            "  Processing eval sequence 304/320 (95.0%)\n",
            "Step 3: Projecting into spectral space (Nystr√∂m extension)...\n",
            "Step 4: Assigning to clusters in spectral space...\n",
            "‚úÖ Prediction complete!\n",
            "   Cluster distribution: [  1 163  20  39  27  66   1   3]\n",
            "  Accuracy: 0.4562 (45.62%)\n",
            "\n",
            "======================================================================\n",
            "Alpha Optimization Results\n",
            "======================================================================\n",
            "Alpha 0.3 ( 70% temporal): 0.4562 (45.62%) ‚úÖ BEST\n",
            "Alpha 0.2 ( 80% temporal): 0.4562 (45.62%) \n",
            "Alpha 0.1 ( 90% temporal): 0.4562 (45.62%) \n",
            "Alpha 0.0 (100% temporal): 0.4562 (45.62%) \n",
            "\n",
            "‚úÖ Best alpha: 0.3 (accuracy: 0.4562 = 45.62%)\n",
            "\n",
            "‚úÖ Current alpha=0.3 remains optimal\n",
            "\n",
            "üíæ Alpha optimization results saved to: STC_Results\\alpha_optimization_results.json\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PRIORITY 3: Test Lower Alpha Values (More Temporal Weight)\n",
        "# ============================================================================\n",
        "# \n",
        "# Current best: alpha=0.3 (70% temporal, 30% spatial)\n",
        "# Hypothesis: With sparse temporal graph, even more temporal weight may help\n",
        "# Test: alpha=0.2, 0.1, 0.0 (temporal only)\n",
        "# Expected: May help distinguish gestures that fail due to similar static poses\n",
        "#\n",
        "# NOTE: This is a quick test. For full optimization, run grid search.\n",
        "\n",
        "# Balanced weights (50% static, 50% temporal) - found optimal from A/B testing\n",
        "# Define here in case Cell 8 hasn't been run yet\n",
        "if 'balanced_weights' not in globals():\n",
        "    balanced_weights = {\n",
        "        'static': 0.50,\n",
        "        'velocity': 0.10,\n",
        "        'velocity_mag': 0.05,\n",
        "        'acceleration': 0.05,\n",
        "        'early': 0.10,\n",
        "        'middle': 0.10,\n",
        "        'late': 0.10,\n",
        "        'trajectory': 0.00,\n",
        "        'smoothness': 0.00\n",
        "    }\n",
        "\n",
        "# Import required functions (in case evaluation cell hasn't been run)\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Define map_clusters_to_labels function if not already defined\n",
        "if 'map_clusters_to_labels' not in globals():\n",
        "    def map_clusters_to_labels(predicted_labels, true_labels, n_clusters=8):\n",
        "        \"\"\"\n",
        "        Map cluster labels to ground truth labels using Hungarian algorithm\n",
        "        \"\"\"\n",
        "        cm = confusion_matrix(true_labels, predicted_labels, labels=range(n_clusters))\n",
        "        cost_matrix = -cm  # Negative because linear_sum_assignment minimizes\n",
        "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
        "        mapping = {col_ind[i]: row_ind[i] for i in range(len(row_ind))}\n",
        "        mapped_labels = np.array([mapping.get(label, label) for label in predicted_labels])\n",
        "        return mapped_labels, mapping\n",
        "\n",
        "# Ensure output_dir is defined\n",
        "if 'output_dir' not in globals():\n",
        "    output_dir = 'STC_Results'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Check if required data is available\n",
        "if 'SEQUENCES_SCALED_TRAIN' not in globals() or 'SEQUENCES_SCALED_EVAL' not in globals() or 'GESTURE_LABELS_EVAL' not in globals():\n",
        "    print(\"‚ö†Ô∏è WARNING: Required data not found. Please run Cells 1-5 first to load and scale data.\")\n",
        "    print(\"   Required variables: SEQUENCES_SCALED_TRAIN, SEQUENCES_SCALED_EVAL, GESTURE_LABELS_EVAL\")\n",
        "    raise NameError(\"Required data variables not found. Run data loading cells first.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Testing Lower Alpha Values (More Temporal Weight)\")\n",
        "print(\"=\" * 70)\n",
        "print(\"Current best: alpha=0.3 (70% temporal)\")\n",
        "print(\"Testing: alpha=0.2, 0.1, 0.0\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Test different alpha values\n",
        "alpha_values = [0.3, 0.2, 0.1, 0.0]  # Include current best for comparison\n",
        "alpha_results = {}\n",
        "\n",
        "for alpha in alpha_values:\n",
        "    print(f\"\\nTesting alpha={alpha} ({int((1-alpha)*100)}% temporal, {int(alpha*100)}% spatial)...\")\n",
        "    \n",
        "    # Create STC with different alpha\n",
        "    stc_test = SpectralTemporalClustering(\n",
        "        n_clusters=8,\n",
        "        alpha=alpha,\n",
        "        n_neighbors_spatial=5,\n",
        "        n_neighbors_temporal=5,\n",
        "        random_state=42,\n",
        "        use_temporal_features=True,\n",
        "        temporal_feature_weights=balanced_weights,\n",
        "        use_dtw=False  # DTW disabled\n",
        "    )\n",
        "    \n",
        "    # Apply temporal graph threshold\n",
        "    stc_test._temporal_threshold = 0.3\n",
        "    stc_test._temporal_top_k = None\n",
        "    stc_test._temporal_percentile = None\n",
        "    \n",
        "    # Fit on training data\n",
        "    stc_labels_train_test = stc_test.fit_predict(SEQUENCES_SCALED_TRAIN)\n",
        "    \n",
        "    # Predict on evaluation data\n",
        "    stc_labels_eval_test = stc_test.predict(SEQUENCES_SCALED_EVAL)\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    stc_labels_eval_mapped_test, stc_mapping_test = map_clusters_to_labels(\n",
        "        stc_labels_eval_test, GESTURE_LABELS_EVAL, n_clusters=8\n",
        "    )\n",
        "    accuracy_test = accuracy_score(GESTURE_LABELS_EVAL, stc_labels_eval_mapped_test)\n",
        "    \n",
        "    alpha_results[alpha] = {\n",
        "        'accuracy': accuracy_test,\n",
        "        'labels': stc_labels_eval_mapped_test,\n",
        "        'mapping': stc_mapping_test\n",
        "    }\n",
        "    \n",
        "    print(f\"  Accuracy: {accuracy_test:.4f} ({accuracy_test*100:.2f}%)\")\n",
        "\n",
        "# Find best alpha\n",
        "best_alpha = max(alpha_results.keys(), key=lambda a: alpha_results[a]['accuracy'])\n",
        "best_accuracy = alpha_results[best_alpha]['accuracy']\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Alpha Optimization Results\")\n",
        "print(\"=\" * 70)\n",
        "for alpha in sorted(alpha_results.keys(), reverse=True):\n",
        "    acc = alpha_results[alpha]['accuracy']\n",
        "    marker = \"‚úÖ BEST\" if alpha == best_alpha else \"\"\n",
        "    print(f\"Alpha {alpha:3.1f} ({int((1-alpha)*100):3d}% temporal): {acc:.4f} ({acc*100:.2f}%) {marker}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Best alpha: {best_alpha} (accuracy: {best_accuracy:.4f} = {best_accuracy*100:.2f}%)\")\n",
        "\n",
        "# Update main STC if better alpha found\n",
        "if best_alpha != 0.3:\n",
        "    print(f\"\\n‚ö†Ô∏è Better alpha found: {best_alpha} (improvement: {best_accuracy - alpha_results[0.3]['accuracy']:+.4f})\")\n",
        "    print(\"   Consider updating main STC configuration with this alpha value\")\n",
        "    # Optionally update stc\n",
        "    # stc = SpectralTemporalClustering(..., alpha=best_alpha, ...)\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Current alpha=0.3 remains optimal\")\n",
        "\n",
        "# Save results\n",
        "alpha_results_path = os.path.join(output_dir, 'alpha_optimization_results.json')\n",
        "alpha_results_save = {\n",
        "    str(alpha): {\n",
        "        'accuracy': float(results['accuracy']),\n",
        "        'mapping': {str(k): int(v) for k, v in results['mapping'].items()}\n",
        "    }\n",
        "    for alpha, results in alpha_results.items()\n",
        "}\n",
        "with open(alpha_results_path, 'w') as f:\n",
        "    json.dump(alpha_results_save, f, indent=2)\n",
        "print(f\"\\nüíæ Alpha optimization results saved to: {alpha_results_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STC Training - Optimized Configuration\n",
            "======================================================================\n",
            "‚úÖ Training on combined.csv: 320 sequences\n",
            "   Total frames: 47,954\n",
            "   Features per frame: 126\n",
            "\n",
            "Optimized Parameters:\n",
            "   Alpha: 0.3 (30% spatial, 70% temporal)\n",
            "   Spatial Neighbors: 5\n",
            "   Temporal Neighbors: 5\n",
            "   Temporal Weights: Balanced (50% static, 50% temporal)\n",
            "======================================================================\n",
            "\n",
            "‚úÖ Using optimized temporal graph sparsification:\n",
            "   Method: similarity_threshold\n",
            "   Threshold: 0.3\n",
            "   Expected graph density: ~18.18% (reduced from 99.69%)\n",
            "   Expected accuracy improvement: +1.56% (45.625% vs 44.0625%)\n",
            "\n",
            "‚úÖ Using DTW for temporal alignment:\n",
            "   DTW enabled: True\n",
            "   DTW radius: 1 (standard DTW)\n",
            "   Expected to improve: Pick (0%), Come (35%), Wave (12.5%) gestures\n",
            "   Expected accuracy improvement: +5-10% (optimal temporal alignment)\n",
            "\n",
            "======================================================================\n",
            "Step 1: Building Spatial Graphs\n",
            "======================================================================\n",
            "Computing mean frame representation for each sequence...\n",
            "Building spatial k-NN graph...\n",
            "Spatial graph: (320, 320), density: 0.0206\n",
            "\n",
            "======================================================================\n",
            "Step 2: Building Temporal Graph\n",
            "======================================================================\n",
            "Building temporal graph for 320 sequences...\n",
            "  Processing sequence 0/320 (0.0%)\n",
            "  Processing sequence 16/320 (5.0%)\n",
            "  Processing sequence 32/320 (10.0%)\n",
            "  Processing sequence 48/320 (15.0%)\n",
            "  Processing sequence 64/320 (20.0%)\n",
            "  Processing sequence 80/320 (25.0%)\n",
            "  Processing sequence 96/320 (30.0%)\n",
            "  Processing sequence 112/320 (35.0%)\n",
            "  Processing sequence 128/320 (40.0%)\n",
            "  Processing sequence 144/320 (45.0%)\n",
            "  Processing sequence 160/320 (50.0%)\n",
            "  Processing sequence 176/320 (55.0%)\n",
            "  Processing sequence 192/320 (60.0%)\n",
            "  Processing sequence 208/320 (65.0%)\n",
            "  Processing sequence 224/320 (70.0%)\n",
            "  Processing sequence 240/320 (75.0%)\n",
            "  Processing sequence 256/320 (80.0%)\n",
            "  Processing sequence 272/320 (85.0%)\n",
            "  Processing sequence 288/320 (90.0%)\n",
            "  Processing sequence 304/320 (95.0%)\n",
            "Temporal graph: (320, 320), density: 0.2003 (20.03%)\n",
            "\n",
            "======================================================================\n",
            "Step 3: Computing Laplacians\n",
            "======================================================================\n",
            "Spatial Laplacian computed\n",
            "Temporal Laplacian computed\n",
            "\n",
            "======================================================================\n",
            "Step 4: Combining Laplacians\n",
            "======================================================================\n",
            "Joint Laplacian: Œ±=0.3 (spatial) + 0.7 (temporal)\n",
            "\n",
            "======================================================================\n",
            "Step 5: Spectral Decomposition\n",
            "======================================================================\n",
            "Computed 8 smallest eigenvalues\n",
            "Eigenvalue range: [0.003191, 0.504942]\n",
            "\n",
            "======================================================================\n",
            "Step 6: K-Means Clustering in Spectral Space\n",
            "======================================================================\n",
            "‚úÖ Clustering complete!\n",
            "   Found 8 clusters\n",
            "   Cluster distribution: [80 80 40 40 38  2 16 24]\n",
            "\n",
            "üíæ Model saved to: STC_Results\\stc_model.pkl\n",
            "üíæ Training labels saved to: STC_Results\\stc_train_labels.npy\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STC TRAINING: Using Optimized Parameters\n",
        "# ============================================================================\n",
        "\n",
        "# Create output directory\n",
        "output_dir = 'STC_Results'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Balanced weights (50% static, 50% temporal) - found optimal from A/B testing\n",
        "balanced_weights = {\n",
        "    'static': 0.50,\n",
        "    'velocity': 0.10,\n",
        "    'velocity_mag': 0.05,\n",
        "    'acceleration': 0.05,\n",
        "    'early': 0.10,\n",
        "    'middle': 0.10,\n",
        "    'late': 0.10,\n",
        "    'trajectory': 0.00,\n",
        "    'smoothness': 0.00\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STC Training - Optimized Configuration\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"‚úÖ Training on combined.csv: {len(SEQUENCES_SCALED_TRAIN)} sequences\")\n",
        "print(f\"   Total frames: {sum(len(seq) for seq in SEQUENCES_SCALED_TRAIN):,}\")\n",
        "print(f\"   Features per frame: {SEQUENCES_SCALED_TRAIN[0].shape[1]}\")\n",
        "print(f\"\\nOptimized Parameters:\")\n",
        "print(f\"   Alpha: 0.3 (30% spatial, 70% temporal)\")\n",
        "print(f\"   Spatial Neighbors: 5\")\n",
        "print(f\"   Temporal Neighbors: 5\")\n",
        "print(f\"   Temporal Weights: Balanced (50% static, 50% temporal)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Initialize STC with optimized parameters\n",
        "# NEW: Enable DTW for temporal alignment (Priority 1 recommendation)\n",
        "stc = SpectralTemporalClustering(\n",
        "    n_clusters=8,\n",
        "    alpha=0.3,  # Optimized: 30% spatial, 70% temporal\n",
        "    n_neighbors_spatial=5,  # Optimized\n",
        "    n_neighbors_temporal=5,  # Doesn't matter with thresholding\n",
        "    random_state=42,\n",
        "    use_temporal_features=True,\n",
        "    temporal_feature_weights=balanced_weights,\n",
        "    use_dtw=True,  # Enable DTW for optimal temporal alignment\n",
        "    dtw_radius=1  # Standard DTW (radius=1), increase for faster computation\n",
        ")\n",
        "\n",
        "# Apply optimized temporal graph sparsification (from optimization results)\n",
        "# Best configuration: similarity_threshold=0.3 (reduces density from 99.69% to 18.18%)\n",
        "stc._temporal_threshold = 0.3  # Optimized threshold\n",
        "stc._temporal_top_k = None\n",
        "stc._temporal_percentile = None\n",
        "\n",
        "print(f\"\\n‚úÖ Using optimized temporal graph sparsification:\")\n",
        "print(f\"   Method: similarity_threshold\")\n",
        "print(f\"   Threshold: 0.3\")\n",
        "print(f\"   Expected graph density: ~18.18% (reduced from 99.69%)\")\n",
        "print(f\"   Expected accuracy improvement: +1.56% (45.625% vs 44.0625%)\")\n",
        "\n",
        "print(f\"\\n‚úÖ Using DTW for temporal alignment:\")\n",
        "print(f\"   DTW enabled: True\")\n",
        "print(f\"   DTW radius: 1 (standard DTW)\")\n",
        "print(f\"   Expected to improve: Pick (0%), Come (35%), Wave (12.5%) gestures\")\n",
        "print(f\"   Expected accuracy improvement: +5-10% (optimal temporal alignment)\")\n",
        "\n",
        "# Fit on TRAINING data (combined.csv)\n",
        "stc_labels_train = stc.fit_predict(SEQUENCES_SCALED_TRAIN)\n",
        "\n",
        "# Save model\n",
        "model_path = os.path.join(output_dir, 'stc_model.pkl')\n",
        "joblib.dump(stc, model_path)\n",
        "print(f\"\\nüíæ Model saved to: {model_path}\")\n",
        "\n",
        "# Save training labels\n",
        "labels_path = os.path.join(output_dir, 'stc_train_labels.npy')\n",
        "np.save(labels_path, stc_labels_train)\n",
        "print(f\"üíæ Training labels saved to: {labels_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "ACCURACY EVALUATION: Predicting on Individual Gesture Folders\n",
            "======================================================================\n",
            "Evaluation sequences: 320\n",
            "Ground truth labels: 320\n",
            "\n",
            "Predicting STC labels for evaluation sequences...\n",
            "   Using spectral projection (predict method)...\n",
            "\n",
            "======================================================================\n",
            "Predicting Evaluation Sequences (Spectral Projection)\n",
            "======================================================================\n",
            "Evaluation sequences: 320\n",
            "Training sequences: 320\n",
            "\n",
            "Step 1: Building spatial graph for evaluation sequences...\n",
            "Step 2: Building temporal graph (eval vs training)...\n",
            "  Processing eval sequence 0/320 (0.0%)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ACCURACY EVALUATION: Predict on individual gesture folders\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ACCURACY EVALUATION: Predicting on Individual Gesture Folders\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# For STC: We need to predict on evaluation sequences\n",
        "# Since STC uses graph-based clustering, we'll use nearest neighbor approach\n",
        "# Compute mean frames for evaluation sequences\n",
        "mean_frames_eval = np.array([np.mean(seq, axis=0) for seq in SEQUENCES_SCALED_EVAL])\n",
        "\n",
        "print(f\"Evaluation sequences: {len(mean_frames_eval)}\")\n",
        "print(f\"Ground truth labels: {len(GESTURE_LABELS_EVAL)}\")\n",
        "\n",
        "# STC Prediction: IMPROVED - Use spectral projection\n",
        "print(\"\\nPredicting STC labels for evaluation sequences...\")\n",
        "print(\"   Using spectral projection (predict method)...\")\n",
        "\n",
        "# Use the new predict method that projects into learned spectral space\n",
        "stc_labels_eval = stc.predict(SEQUENCES_SCALED_EVAL)\n",
        "print(f\"   ‚úÖ Completed prediction for all {len(SEQUENCES_SCALED_EVAL)} sequences\")\n",
        "\n",
        "# GMM Baseline: Train if needed (for comparison)\n",
        "print(\"\\nPredicting GMM labels for evaluation sequences...\")\n",
        "mean_frames_train = np.array([np.mean(seq, axis=0) for seq in SEQUENCES_SCALED_TRAIN])\n",
        "\n",
        "# Train GMM if not already trained\n",
        "if 'gmm' not in globals() or 'gmm_labels_train' not in globals():\n",
        "    print(\"   Training GMM baseline...\")\n",
        "    gmm = GaussianMixture(\n",
        "        n_components=8,\n",
        "        covariance_type='full',\n",
        "        init_params='k-means++',\n",
        "        n_init=20,\n",
        "        max_iter=200,\n",
        "        random_state=42\n",
        "    )\n",
        "    gmm.fit(mean_frames_train)\n",
        "    gmm_labels_train = gmm.predict(mean_frames_train)\n",
        "    \n",
        "    # Save GMM model\n",
        "    gmm_model_path = os.path.join(output_dir, 'gmm_baseline_model.pkl')\n",
        "    joblib.dump(gmm, gmm_model_path)\n",
        "    gmm_labels_path = os.path.join(output_dir, 'gmm_train_labels.npy')\n",
        "    np.save(gmm_labels_path, gmm_labels_train)\n",
        "    print(f\"   ‚úÖ GMM training complete! (Converged: {gmm.converged_}, Iterations: {gmm.n_iter_})\")\n",
        "else:\n",
        "    print(\"   ‚úÖ Using existing GMM model\")\n",
        "\n",
        "gmm_labels_eval = gmm.predict(mean_frames_eval)\n",
        "\n",
        "# Calculate accuracy with optimal cluster-to-gesture mapping\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "def map_clusters_to_labels(predicted_labels, true_labels, n_clusters=8):\n",
        "    \"\"\"\n",
        "    Map cluster labels to ground truth labels using Hungarian algorithm\n",
        "    to maximize accuracy.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    predicted_labels : array\n",
        "        Cluster assignments (0 to n_clusters-1)\n",
        "    true_labels : array\n",
        "        Ground truth gesture labels (0 to n_clusters-1)\n",
        "    n_clusters : int\n",
        "        Number of clusters/gestures\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    mapped_labels : array\n",
        "        Predicted labels mapped to gesture labels\n",
        "    mapping : dict\n",
        "        Dictionary mapping cluster_id -> gesture_id\n",
        "    \"\"\"\n",
        "    # Build confusion matrix: rows = true labels, cols = predicted clusters\n",
        "    cm = confusion_matrix(true_labels, predicted_labels, labels=range(n_clusters))\n",
        "    \n",
        "    # Use Hungarian algorithm to find optimal assignment\n",
        "    # We want to maximize matches, so use negative of confusion matrix\n",
        "    cost_matrix = -cm  # Negative because linear_sum_assignment minimizes\n",
        "    \n",
        "    # Find optimal assignment: row_ind = true labels, col_ind = predicted clusters\n",
        "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
        "    \n",
        "    # Create mapping: cluster_id -> gesture_id\n",
        "    mapping = {col_ind[i]: row_ind[i] for i in range(len(row_ind))}\n",
        "    \n",
        "    # Map predicted labels to gesture labels\n",
        "    mapped_labels = np.array([mapping.get(label, label) for label in predicted_labels])\n",
        "    \n",
        "    return mapped_labels, mapping\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Mapping Clusters to Gesture Labels (Hungarian Algorithm)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Map STC clusters to gestures\n",
        "stc_labels_eval_mapped, stc_mapping = map_clusters_to_labels(\n",
        "    stc_labels_eval, GESTURE_LABELS_EVAL, n_clusters=8\n",
        ")\n",
        "print(f\"\\nSTC Cluster-to-Gesture Mapping:\")\n",
        "for cluster_id, gesture_id in sorted(stc_mapping.items()):\n",
        "    print(f\"  Cluster {cluster_id} -> Gesture {gesture_id} ({GESTURE_TYPES[gesture_id]})\")\n",
        "\n",
        "# Map GMM clusters to gestures\n",
        "gmm_labels_eval_mapped, gmm_mapping = map_clusters_to_labels(\n",
        "    gmm_labels_eval, GESTURE_LABELS_EVAL, n_clusters=8\n",
        ")\n",
        "print(f\"\\nGMM Cluster-to-Gesture Mapping:\")\n",
        "for cluster_id, gesture_id in sorted(gmm_mapping.items()):\n",
        "    print(f\"  Cluster {cluster_id} -> Gesture {gesture_id} ({GESTURE_TYPES[gesture_id]})\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ACCURACY RESULTS (After Optimal Mapping)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# STC Accuracy (after mapping)\n",
        "stc_accuracy = accuracy_score(GESTURE_LABELS_EVAL, stc_labels_eval_mapped)\n",
        "print(f\"\\n‚úÖ STC Accuracy: {stc_accuracy:.4f} ({stc_accuracy*100:.2f}%)\")\n",
        "\n",
        "# GMM Accuracy (after mapping)\n",
        "gmm_accuracy = accuracy_score(GESTURE_LABELS_EVAL, gmm_labels_eval_mapped)\n",
        "print(f\"‚úÖ GMM Accuracy: {gmm_accuracy:.4f} ({gmm_accuracy*100:.2f}%)\")\n",
        "\n",
        "# Update labels to mapped versions for confusion matrices and reports\n",
        "stc_labels_eval = stc_labels_eval_mapped\n",
        "gmm_labels_eval = gmm_labels_eval_mapped\n",
        "\n",
        "# Confusion matrices\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STC Confusion Matrix\")\n",
        "print(\"=\" * 70)\n",
        "stc_cm = confusion_matrix(GESTURE_LABELS_EVAL, stc_labels_eval)\n",
        "print(stc_cm)\n",
        "print(\"\\nGesture Types:\", GESTURE_TYPES)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"GMM Confusion Matrix\")\n",
        "print(\"=\" * 70)\n",
        "gmm_cm = confusion_matrix(GESTURE_LABELS_EVAL, gmm_labels_eval)\n",
        "print(gmm_cm)\n",
        "\n",
        "# Classification reports\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STC Classification Report\")\n",
        "print(\"=\" * 70)\n",
        "print(classification_report(GESTURE_LABELS_EVAL, stc_labels_eval, \n",
        "                            target_names=GESTURE_TYPES, zero_division=0))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"GMM Classification Report\")\n",
        "print(\"=\" * 70)\n",
        "print(classification_report(GESTURE_LABELS_EVAL, gmm_labels_eval, \n",
        "                            target_names=GESTURE_TYPES, zero_division=0))\n",
        "\n",
        "# Save evaluation results\n",
        "eval_results = {\n",
        "    'stc_accuracy': float(stc_accuracy),\n",
        "    'gmm_accuracy': float(gmm_accuracy),\n",
        "    'stc_labels_eval': stc_labels_eval.tolist(),\n",
        "    'gmm_labels_eval': gmm_labels_eval.tolist(),\n",
        "    'ground_truth_labels': GESTURE_LABELS_EVAL.tolist(),\n",
        "    'stc_confusion_matrix': stc_cm.tolist(),\n",
        "    'gmm_confusion_matrix': gmm_cm.tolist(),\n",
        "    'stc_cluster_mapping': {str(k): int(v) for k, v in stc_mapping.items()},\n",
        "    'gmm_cluster_mapping': {str(k): int(v) for k, v in gmm_mapping.items()},\n",
        "    'gesture_types': GESTURE_TYPES\n",
        "}\n",
        "\n",
        "eval_results_path = os.path.join(output_dir, 'accuracy_evaluation.json')\n",
        "with open(eval_results_path, 'w') as f:\n",
        "    json.dump(eval_results, f, indent=2)\n",
        "print(f\"\\nüíæ Evaluation results saved to: {eval_results_path}\")\n",
        "\n",
        "# Save evaluation labels\n",
        "np.save(os.path.join(output_dir, 'stc_eval_labels.npy'), stc_labels_eval)\n",
        "np.save(os.path.join(output_dir, 'gmm_eval_labels.npy'), gmm_labels_eval)\n",
        "print(\"üíæ Evaluation labels saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# NOTE: GMM Baseline Training (Moved to Cell 9)\n",
        "# ============================================================================\n",
        "# \n",
        "# GMM training is now handled in Cell 9 (Accuracy Evaluation) to avoid redundancy.\n",
        "# GMM is trained automatically if not already available.\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clustering Quality Evaluation (on TRAINING data)\n",
        "def evaluate_clustering(X, labels, name=\"\"):\n",
        "    \"\"\"Evaluate clustering quality\"\"\"\n",
        "    if len(np.unique(labels)) < 2:\n",
        "        print(f\"{name}: Cannot compute metrics (only 1 cluster)\")\n",
        "        return {\n",
        "            'silhouette': np.nan,\n",
        "            'davies_bouldin': np.nan,\n",
        "            'calinski_harabasz': np.nan\n",
        "        }\n",
        "    \n",
        "    sil_score = silhouette_score(X, labels)\n",
        "    db_score = davies_bouldin_score(X, labels)\n",
        "    ch_score = calinski_harabasz_score(X, labels)\n",
        "    \n",
        "    print(f\"\\n{name} Metrics:\")\n",
        "    print(f\"  Silhouette Score: {sil_score:.6f} (higher is better, range: -1 to 1)\")\n",
        "    print(f\"  Davies-Bouldin Score: {db_score:.6f} (lower is better)\")\n",
        "    print(f\"  Calinski-Harabasz Score: {ch_score:.2f} (higher is better)\")\n",
        "    \n",
        "    return {\n",
        "        'silhouette': sil_score,\n",
        "        'davies_bouldin': db_score,\n",
        "        'calinski_harabasz': ch_score\n",
        "    }\n",
        "\n",
        "# Evaluate STC on TRAINING data\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STC Clustering Quality (Training Data)\")\n",
        "print(\"=\" * 70)\n",
        "mean_frames_train = np.array([np.mean(seq, axis=0) for seq in SEQUENCES_SCALED_TRAIN])\n",
        "stc_metrics = evaluate_clustering(mean_frames_train, stc_labels_train, \"STC\")\n",
        "\n",
        "# Evaluate GMM on TRAINING data\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"GMM Baseline Clustering Quality (Training Data)\")\n",
        "print(\"=\" * 70)\n",
        "gmm_metrics = evaluate_clustering(mean_frames_train, gmm_labels_train, \"GMM\")\n",
        "\n",
        "# Comparison\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Comparison: STC vs GMM (Training Data)\")\n",
        "print(\"=\" * 70)\n",
        "comparison_df = pd.DataFrame({\n",
        "    'STC': [stc_metrics['silhouette'], stc_metrics['davies_bouldin'], stc_metrics['calinski_harabasz']],\n",
        "    'GMM': [gmm_metrics['silhouette'], gmm_metrics['davies_bouldin'], gmm_metrics['calinski_harabasz']]\n",
        "}, index=['Silhouette Score', 'Davies-Bouldin Score', 'Calinski-Harabasz Score'])\n",
        "\n",
        "print(comparison_df)\n",
        "\n",
        "# Improvement percentages\n",
        "sil_improvement = ((stc_metrics['silhouette'] - gmm_metrics['silhouette']) / abs(gmm_metrics['silhouette']) * 100) if gmm_metrics['silhouette'] != 0 else np.nan\n",
        "db_improvement = ((gmm_metrics['davies_bouldin'] - stc_metrics['davies_bouldin']) / gmm_metrics['davies_bouldin'] * 100) if gmm_metrics['davies_bouldin'] != 0 else np.nan\n",
        "ch_improvement = ((stc_metrics['calinski_harabasz'] - gmm_metrics['calinski_harabasz']) / gmm_metrics['calinski_harabasz'] * 100) if gmm_metrics['calinski_harabasz'] != 0 else np.nan\n",
        "\n",
        "print(f\"\\nImprovement:\")\n",
        "print(f\"  Silhouette Score: {sil_improvement:+.2f}%\")\n",
        "print(f\"  Davies-Bouldin Score: {db_improvement:+.2f}% (lower is better)\")\n",
        "print(f\"  Calinski-Harabasz Score: {ch_improvement:+.2f}%\")\n",
        "\n",
        "# Save comparison\n",
        "comparison_path = os.path.join(output_dir, 'clustering_quality_comparison.json')\n",
        "results = {\n",
        "    'stc': stc_metrics,\n",
        "    'gmm': gmm_metrics,\n",
        "    'improvement': {\n",
        "        'silhouette_pct': float(sil_improvement),\n",
        "        'davies_bouldin_pct': float(db_improvement),\n",
        "        'calinski_harabasz_pct': float(ch_improvement)\n",
        "    }\n",
        "}\n",
        "with open(comparison_path, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "print(f\"\\nüíæ Clustering quality comparison saved to: {comparison_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VISUALIZATION: Results and Comparisons\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Creating Visualizations\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Prepare data for visualization\n",
        "mean_frames_train = np.array([np.mean(seq, axis=0) for seq in SEQUENCES_SCALED_TRAIN])\n",
        "mean_frames_eval = np.array([np.mean(seq, axis=0) for seq in SEQUENCES_SCALED_EVAL])\n",
        "\n",
        "# PCA for visualization (fit on training data)\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "mean_frames_pca_train = pca.fit_transform(mean_frames_train)\n",
        "mean_frames_pca_eval = pca.transform(mean_frames_eval)\n",
        "\n",
        "print(f\"PCA explained variance: {pca.explained_variance_ratio_.sum():.4f}\")\n",
        "\n",
        "# Figure 1: Training data comparison (STC vs GMM)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "ax = axes[0]\n",
        "scatter = ax.scatter(mean_frames_pca_train[:, 0], mean_frames_pca_train[:, 1], \n",
        "                     c=stc_labels_train, cmap='tab10', s=50, alpha=0.6, edgecolors='k', linewidth=0.5)\n",
        "ax.set_title('STC Clustering - Training Data', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "plt.colorbar(scatter, ax=ax, label='Cluster')\n",
        "\n",
        "ax = axes[1]\n",
        "scatter = ax.scatter(mean_frames_pca_train[:, 0], mean_frames_pca_train[:, 1], \n",
        "                     c=gmm_labels_train, cmap='tab10', s=50, alpha=0.6, edgecolors='k', linewidth=0.5)\n",
        "ax.set_title('GMM Baseline - Training Data', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "plt.colorbar(scatter, ax=ax, label='Cluster')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, 'stc_vs_gmm_pca_comparison_train.png'), dpi=200, bbox_inches='tight')\n",
        "print(f\"üíæ Training visualization saved\")\n",
        "plt.show()\n",
        "\n",
        "# Figure 2: Evaluation data comparison (Ground Truth, STC, GMM)\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "ax = axes[0]\n",
        "scatter = ax.scatter(mean_frames_pca_eval[:, 0], mean_frames_pca_eval[:, 1], \n",
        "                     c=GESTURE_LABELS_EVAL, cmap='tab10', s=50, alpha=0.6, edgecolors='k', linewidth=0.5)\n",
        "ax.set_title('Ground Truth Labels', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "plt.colorbar(scatter, ax=ax, label='Gesture')\n",
        "\n",
        "ax = axes[1]\n",
        "scatter = ax.scatter(mean_frames_pca_eval[:, 0], mean_frames_pca_eval[:, 1], \n",
        "                     c=stc_labels_eval, cmap='tab10', s=50, alpha=0.6, edgecolors='k', linewidth=0.5)\n",
        "ax.set_title(f'STC Predictions (Accuracy: {stc_accuracy:.2%})', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "plt.colorbar(scatter, ax=ax, label='Cluster')\n",
        "\n",
        "ax = axes[2]\n",
        "scatter = ax.scatter(mean_frames_pca_eval[:, 0], mean_frames_pca_eval[:, 1], \n",
        "                     c=gmm_labels_eval, cmap='tab10', s=50, alpha=0.6, edgecolors='k', linewidth=0.5)\n",
        "ax.set_title(f'GMM Predictions (Accuracy: {gmm_accuracy:.2%})', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "plt.colorbar(scatter, ax=ax, label='Cluster')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, 'stc_vs_gmm_pca_comparison_eval.png'), dpi=200, bbox_inches='tight')\n",
        "print(f\"üíæ Evaluation visualization saved\")\n",
        "plt.show()\n",
        "\n",
        "# Figure 3: Cluster distribution and eigenvalue spectrum\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Cluster distribution\n",
        "ax = axes[0]\n",
        "stc_counts = np.bincount(stc_labels_train)\n",
        "ax.bar(range(len(stc_counts)), stc_counts, color='steelblue', alpha=0.7, edgecolor='black')\n",
        "ax.set_xlabel('Cluster ID')\n",
        "ax.set_ylabel('Number of Sequences')\n",
        "ax.set_title('STC Cluster Distribution (Training)', fontsize=12, fontweight='bold')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "for i, count in enumerate(stc_counts):\n",
        "    ax.text(i, count + 1, str(count), ha='center', va='bottom')\n",
        "\n",
        "# Eigenvalue spectrum\n",
        "ax = axes[1]\n",
        "ax.plot(range(1, len(stc.eigenvalues_) + 1), stc.eigenvalues_, 'o-', linewidth=2, markersize=8)\n",
        "ax.set_xlabel('Eigenvalue Index', fontsize=12)\n",
        "ax.set_ylabel('Eigenvalue', fontsize=12)\n",
        "ax.set_title('STC Eigenvalue Spectrum', fontsize=12, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.axhline(y=0, color='r', linestyle='--', linewidth=1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, 'cluster_distribution_and_spectrum.png'), dpi=200, bbox_inches='tight')\n",
        "print(f\"üíæ Cluster distribution and eigenvalue spectrum saved\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization cells have been consolidated into Cell 12\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization cells have been consolidated into Cell 12\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Final Summary\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Training sequences: {len(SEQUENCES_TRAIN)}\")\n",
        "print(f\"Evaluation sequences: {len(SEQUENCES_EVAL)}\")\n",
        "print(f\"Average training sequence length: {np.mean([len(s) for s in SEQUENCES_TRAIN]):.1f} frames\")\n",
        "print(f\"Average evaluation sequence length: {np.mean([len(s) for s in SEQUENCES_EVAL]):.1f} frames\")\n",
        "print(f\"\\nSTC Parameters:\")\n",
        "print(f\"  Œ± (spatial weight): {stc.alpha}\")\n",
        "print(f\"  Spatial neighbors: {stc.n_neighbors_spatial}\")\n",
        "print(f\"  Temporal neighbors: {stc.n_neighbors_temporal}\")\n",
        "print(f\"\\nSTC Performance:\")\n",
        "print(f\"  Silhouette Score: {stc_metrics['silhouette']:.6f}\")\n",
        "print(f\"  Davies-Bouldin Score: {stc_metrics['davies_bouldin']:.6f}\")\n",
        "print(f\"  Calinski-Harabasz Score: {stc_metrics['calinski_harabasz']:.2f}\")\n",
        "print(f\"\\nGMM Baseline Performance:\")\n",
        "print(f\"  Silhouette Score: {gmm_metrics['silhouette']:.6f}\")\n",
        "print(f\"  Davies-Bouldin Score: {gmm_metrics['davies_bouldin']:.6f}\")\n",
        "print(f\"  Calinski-Harabasz Score: {gmm_metrics['calinski_harabasz']:.2f}\")\n",
        "\n",
        "print(\"\\n‚úÖ All analysis complete!\")\n",
        "print(f\"üìÅ Results saved in: {output_dir}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

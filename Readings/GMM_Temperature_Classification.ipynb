{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM Temperature Classification - Cold_normal vs Hot\n",
    "\n",
    "This notebook implements:\n",
    "- Loading sensor data from Readings/Cold_normal and Readings/Hot folders\n",
    "- Using folder labels as ground truth\n",
    "- Statistical feature engineering (mean, std, min, max, percentiles, etc.)\n",
    "- Train-test split for evaluation\n",
    "- GMM model training and evaluation\n",
    "- Visualizations and performance metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except:\n",
    "    plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data(data_dir=None):\n",
    "    \"\"\"Load all sensor data from Cold_normal and Hot folders.\n",
    "    Returns file-level data: each file becomes one data point with aggregated features.\"\"\"\n",
    "    # Auto-detect the correct path\n",
    "    if data_dir is None:\n",
    "        # Try current directory first (if notebook is in Readings folder)\n",
    "        current_dir = Path('.')\n",
    "        if (current_dir / 'Cold_normal').exists() and (current_dir / 'Hot').exists():\n",
    "            data_path = current_dir\n",
    "        # Try Readings subdirectory (if notebook is in project root)\n",
    "        elif (current_dir / 'Readings' / 'Cold_normal').exists() and (current_dir / 'Readings' / 'Hot').exists():\n",
    "            data_path = current_dir / 'Readings'\n",
    "        else:\n",
    "            # Try going up one level and then into Readings\n",
    "            parent_dir = Path('..')\n",
    "            if (parent_dir / 'Readings' / 'Cold_normal').exists() and (parent_dir / 'Readings' / 'Hot').exists():\n",
    "                data_path = parent_dir / 'Readings'\n",
    "            else:\n",
    "                # Last resort: try absolute path\n",
    "                import os\n",
    "                cwd = Path(os.getcwd())\n",
    "                if (cwd / 'Readings' / 'Cold_normal').exists() and (cwd / 'Readings' / 'Hot').exists():\n",
    "                    data_path = cwd / 'Readings'\n",
    "                else:\n",
    "                    data_path = Path('Readings')  # Default fallback\n",
    "    else:\n",
    "        data_path = Path(data_dir)\n",
    "    \n",
    "    X_file_level = []  # One feature vector per file\n",
    "    y_file_level = []  # One label per file\n",
    "    \n",
    "    print(f\"Searching for data in: {data_path.absolute()}\")\n",
    "    \n",
    "    # Load Cold_normal data\n",
    "    cold_folder = data_path / 'Cold_normal'\n",
    "    if cold_folder.exists():\n",
    "        csv_files = sorted(cold_folder.glob('raw_sensor_log*.csv'))\n",
    "        print(f\"Loading {len(csv_files)} files from {cold_folder}...\")\n",
    "        for csv_file in csv_files:\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file)\n",
    "                # Extract sensor readings (4 sensors) - all rows from this file\n",
    "                sensor_data = df[['Sensor_1', 'Sensor_2', 'Sensor_3', 'Sensor_4']].values.astype(float)\n",
    "                X_file_level.append(sensor_data)  # Store all rows for this file\n",
    "                y_file_level.append('Cold_normal')\n",
    "            except Exception as e:\n",
    "                print(f\"  Error loading {csv_file.name}: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: Cold_normal folder not found at {cold_folder.absolute()}\")\n",
    "    \n",
    "    # Load Hot data\n",
    "    hot_folder = data_path / 'Hot'\n",
    "    if hot_folder.exists():\n",
    "        csv_files = sorted(hot_folder.glob('raw_sensor_log*.csv'))\n",
    "        print(f\"Loading {len(csv_files)} files from {hot_folder}...\")\n",
    "        for csv_file in csv_files:\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file)\n",
    "                sensor_data = df[['Sensor_1', 'Sensor_2', 'Sensor_3', 'Sensor_4']].values.astype(float)\n",
    "                X_file_level.append(sensor_data)  # Store all rows for this file\n",
    "                y_file_level.append('Hot')\n",
    "            except Exception as e:\n",
    "                print(f\"  Error loading {csv_file.name}: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: Hot folder not found at {hot_folder.absolute()}\")\n",
    "    \n",
    "    if not X_file_level:\n",
    "        print(f\"\\nError: No data files found. Searched in: {data_path}\")\n",
    "        print(f\"Current working directory: {Path.cwd()}\")\n",
    "        return None, None\n",
    "    \n",
    "    # X_file_level is a list of arrays, where each array contains all rows from one file\n",
    "    # y_file_level is the labels (one per file)\n",
    "    y = np.array(y_file_level)\n",
    "    return X_file_level, y\n",
    "\n",
    "# Load data at file level\n",
    "# Try to auto-detect the correct path\n",
    "# If auto-detection fails, you can manually specify the path:\n",
    "# X_file_data, y = load_all_data(data_dir='../Readings')  # If running from Readings folder\n",
    "# X_file_data, y = load_all_data(data_dir='Readings')     # If running from project root\n",
    "# X_file_data, y = load_all_data(data_dir='.')          # If data folders are in current directory\n",
    "\n",
    "X_file_data, y = load_all_data()  # X_file_data is list of arrays (one array per file)\n",
    "\n",
    "if X_file_data is not None:\n",
    "    print(f\"\\nTotal files: {len(X_file_data)}\")\n",
    "    print(f\"Rows per file: {[len(x) for x in X_file_data[:5]]}... (showing first 5)\")\n",
    "    \n",
    "    print(f\"\\nFile-level class distribution:\")\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    for cls, cnt in zip(unique, counts):\n",
    "        print(f\"  {cls}: {cnt} files ({cnt/len(y)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\nERROR: Failed to load data. Please check:\")\n",
    "    print(\"  1. The Cold_normal and Hot folders exist\")\n",
    "    print(\"  2. The folders contain CSV files starting with 'raw_sensor_log'\")\n",
    "    print(\"  3. The current working directory is correct\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistical Feature Engineering\n",
    "\n",
    "Aggregate all rows from each file into a single feature vector using statistical features:\n",
    "- Per-sensor statistics: mean, std, min, max (16 features)\n",
    "- Global statistics: mean, std, variance, min, max, range, median, Q25, Q75, IQR, CV, skew, kurtosis (13 features)\n",
    "- **Total: 29 features per file**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_file_to_features(file_sensor_data):\n",
    "    \"\"\"Aggregate all rows from a file into statistical features.\n",
    "    Input: file_sensor_data - array of shape (n_rows, 4_sensors)\n",
    "    Output: single feature vector with 29 statistical features\"\"\"\n",
    "    X = np.array(file_sensor_data)\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(1, -1)\n",
    "    \n",
    "    # Per-sensor statistics (4 sensors × 4 stats = 16 features)\n",
    "    sensor_means = X.mean(axis=0)\n",
    "    sensor_stds = X.std(axis=0)\n",
    "    sensor_maxs = X.max(axis=0)\n",
    "    sensor_mins = X.min(axis=0)\n",
    "    \n",
    "    # Global statistics (13 features)\n",
    "    all_values = X.flatten()\n",
    "    global_mean = all_values.mean()\n",
    "    global_std = all_values.std()\n",
    "    global_var = all_values.var()\n",
    "    global_max = all_values.max()\n",
    "    global_min = all_values.min()\n",
    "    global_range = global_max - global_min\n",
    "    global_median = np.median(all_values)\n",
    "    \n",
    "    # Percentiles\n",
    "    q25 = np.percentile(all_values, 25)\n",
    "    q75 = np.percentile(all_values, 75)\n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    # Higher moments\n",
    "    mean_centered = all_values - global_mean\n",
    "    global_skew = (mean_centered ** 3).mean() / (global_std ** 3 + 1e-8)\n",
    "    global_kurtosis = (mean_centered ** 4).mean() / (global_std ** 4 + 1e-8)\n",
    "    cv = global_std / (global_mean + 1e-8)\n",
    "    \n",
    "    # Combine all features (16 + 13 = 29 features)\n",
    "    features = np.concatenate([\n",
    "        sensor_means,      # 4 features (indices 0-3)\n",
    "        sensor_stds,       # 4 features (indices 4-7)\n",
    "        sensor_maxs,       # 4 features (indices 8-11)\n",
    "        sensor_mins,       # 4 features (indices 12-15)\n",
    "        [global_mean, global_std, global_var, global_max, global_min, global_range, \n",
    "         global_median, q25, q75, iqr, cv, global_skew, global_kurtosis]  # 13 features (indices 16-28)\n",
    "    ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Feature names for reference\n",
    "feature_names = [\n",
    "    'sensor_1_mean', 'sensor_2_mean', 'sensor_3_mean', 'sensor_4_mean',  # 0-3\n",
    "    'sensor_1_std', 'sensor_2_std', 'sensor_3_std', 'sensor_4_std',      # 4-7\n",
    "    'sensor_1_max', 'sensor_2_max', 'sensor_3_max', 'sensor_4_max',      # 8-11\n",
    "    'sensor_1_min', 'sensor_2_min', 'sensor_3_min', 'sensor_4_min',      # 12-15\n",
    "    'global_mean', 'global_std', 'global_var', 'global_max', 'global_min',  # 16-20\n",
    "    'global_range', 'global_median', 'q25', 'q75', 'iqr',                   # 21-25\n",
    "    'cv', 'global_skew', 'global_kurtosis'                                  # 26-28\n",
    "]\n",
    "\n",
    "# Convert file-level data to feature vectors\n",
    "print(\"Extracting statistical features from each file...\")\n",
    "X_features = []\n",
    "for file_data in X_file_data:\n",
    "    features = aggregate_file_to_features(file_data)\n",
    "    X_features.append(features)\n",
    "\n",
    "X = np.array(X_features)  # Shape: (n_files, 29_features)\n",
    "print(f\"\\nFeature matrix shape: {X.shape} (files × features)\")\n",
    "print(f\"Number of features per file: {X.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} files ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(X_test)} files ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"Features per file: {X_train.shape[1]}\")\n",
    "\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "for cls, cnt in zip(unique_train, counts_train):\n",
    "    print(f\"  {cls}: {cnt} files ({cnt/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "for cls, cnt in zip(unique_test, counts_test):\n",
    "    print(f\"  {cls}: {cnt} files ({cnt/len(y_test)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GMM Training and Evaluation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_clusters_to_labels(clusters, ground_truth):\n",
    "    \"\"\"Map cluster IDs to class labels using majority voting.\"\"\"\n",
    "    cluster_to_label = {}\n",
    "    unique_clusters = np.unique(clusters)\n",
    "    \n",
    "    for cluster_id in unique_clusters:\n",
    "        cluster_mask = clusters == cluster_id\n",
    "        labels_in_cluster = pd.Series(ground_truth[cluster_mask]).value_counts()\n",
    "        \n",
    "        if len(labels_in_cluster) > 0:\n",
    "            dominant_label = labels_in_cluster.idxmax()\n",
    "            cluster_to_label[int(cluster_id)] = dominant_label\n",
    "        else:\n",
    "            cluster_to_label[int(cluster_id)] = 'Cold_normal'\n",
    "    \n",
    "    return cluster_to_label\n",
    "\n",
    "def evaluate_gmm_config(X_train, y_train, X_test=None, y_test=None, \n",
    "                        covariance_type='diag', n_init=30, random_state=42):\n",
    "    \"\"\"Train and evaluate a GMM configuration.\"\"\"\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Train GMM with 2 clusters\n",
    "    gmm = GaussianMixture(\n",
    "        n_components=2,\n",
    "        covariance_type=covariance_type,\n",
    "        random_state=random_state,\n",
    "        max_iter=300,\n",
    "        n_init=n_init,\n",
    "        init_params='kmeans'\n",
    "    )\n",
    "    gmm.fit(X_train_scaled)\n",
    "    \n",
    "    if not gmm.converged_:\n",
    "        return None\n",
    "    \n",
    "    # Predict clusters on training data\n",
    "    clusters_train = gmm.predict(X_train_scaled)\n",
    "    \n",
    "    # Map clusters to labels\n",
    "    cluster_to_label = map_clusters_to_labels(clusters_train, y_train)\n",
    "    \n",
    "    # Convert clusters to predictions\n",
    "    pred_train = np.array([cluster_to_label.get(c, 'Cold_normal') for c in clusters_train])\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_acc = accuracy_score(y_train, pred_train)\n",
    "    train_precision = precision_score(y_train, pred_train, average='weighted', zero_division=0)\n",
    "    train_recall = recall_score(y_train, pred_train, average='weighted', zero_division=0)\n",
    "    train_f1 = f1_score(y_train, pred_train, average='weighted', zero_division=0)\n",
    "    conf_matrix_train = confusion_matrix(y_train, pred_train, labels=['Cold_normal', 'Hot'])\n",
    "    \n",
    "    result = {\n",
    "        'n_features': X_train.shape[1],\n",
    "        'covariance_type': covariance_type,\n",
    "        'train_accuracy': float(train_acc),\n",
    "        'train_precision': float(train_precision),\n",
    "        'train_recall': float(train_recall),\n",
    "        'train_f1': float(train_f1),\n",
    "        'gmm': gmm,\n",
    "        'scaler': scaler,\n",
    "        'cluster_mapping': cluster_to_label,\n",
    "        'confusion_matrix': conf_matrix_train,\n",
    "        'predictions': pred_train,\n",
    "        'clusters': clusters_train\n",
    "    }\n",
    "    \n",
    "    # Evaluate on test data if provided\n",
    "    if X_test is not None and y_test is not None:\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        clusters_test = gmm.predict(X_test_scaled)\n",
    "        pred_test = np.array([cluster_to_label.get(c, 'Cold_normal') for c in clusters_test])\n",
    "        \n",
    "        test_acc = accuracy_score(y_test, pred_test)\n",
    "        test_precision = precision_score(y_test, pred_test, average='weighted', zero_division=0)\n",
    "        test_recall = recall_score(y_test, pred_test, average='weighted', zero_division=0)\n",
    "        test_f1 = f1_score(y_test, pred_test, average='weighted', zero_division=0)\n",
    "        conf_matrix_test = confusion_matrix(y_test, pred_test, labels=['Cold_normal', 'Hot'])\n",
    "        \n",
    "        result['test_accuracy'] = float(test_acc)\n",
    "        result['test_precision'] = float(test_precision)\n",
    "        result['test_recall'] = float(test_recall)\n",
    "        result['test_f1'] = float(test_f1)\n",
    "        result['test_confusion_matrix'] = conf_matrix_test\n",
    "        result['test_predictions'] = pred_test\n",
    "        result['test_clusters'] = clusters_test\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"GMM training functions defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Selection - Find Best Features\n",
    "\n",
    "Test different feature counts (4-12) using Mutual Information scoring to find the optimal feature set for >60% accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest, f_classif\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE SELECTION - Finding Best Features\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total features available: {len(feature_names)}\")\n",
    "print(f\"Testing feature counts: 4 to 12\")\n",
    "print()\n",
    "\n",
    "# Convert labels to numeric for feature selection\n",
    "y_train_numeric = (y_train == 'Hot').astype(int)\n",
    "\n",
    "# Compute Mutual Information scores\n",
    "print(\"Computing Mutual Information scores...\")\n",
    "mi_scores = mutual_info_classif(X_train, y_train_numeric, random_state=42)\n",
    "mi_ranked = np.argsort(mi_scores)[::-1]\n",
    "\n",
    "print(\"\\nTop 12 features by Mutual Information:\")\n",
    "for i, idx in enumerate(mi_ranked[:12]):\n",
    "    print(f\"  {i+1:2d}. {feature_names[idx]:20s} (MI={mi_scores[idx]:.4f})\")\n",
    "\n",
    "# Test different feature counts\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Testing Different Feature Counts\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "\n",
    "for n_features in range(4, 13):\n",
    "    selected_indices = mi_ranked[:n_features]\n",
    "    X_train_selected = X_train[:, selected_indices]\n",
    "    X_test_selected = X_test[:, selected_indices]\n",
    "    \n",
    "    result = evaluate_gmm_config(\n",
    "        X_train_selected, y_train,\n",
    "        X_test=X_test_selected, y_test=y_test,\n",
    "        covariance_type='diag',\n",
    "        n_init=30,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    if result is not None:\n",
    "        results.append({\n",
    "            'n_features': n_features,\n",
    "            'indices': selected_indices.tolist(),\n",
    "            'feature_names': [feature_names[i] for i in selected_indices],\n",
    "            'train_acc': result['train_accuracy'],\n",
    "            'test_acc': result.get('test_accuracy', 0),\n",
    "            'train_f1': result['train_f1'],\n",
    "            'test_f1': result.get('test_f1', 0),\n",
    "            'result': result\n",
    "        })\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n{'Features':<10} {'Train Acc':<12} {'Test Acc':<12} {'Train F1':<12} {'Test F1':<12}\")\n",
    "print(\"=\"*60)\n",
    "for r in results:\n",
    "    print(f\"{r['n_features']:<10} {r['train_acc']:<12.4f} {r['test_acc']:<12.4f} {r['train_f1']:<12.4f} {r['test_f1']:<12.4f}\")\n",
    "\n",
    "# Find best configuration (prioritize test accuracy >= 60%)\n",
    "valid_results = [r for r in results if r['test_acc'] >= 0.6]\n",
    "if valid_results:\n",
    "    best_result_feat = max(valid_results, key=lambda x: x['test_acc'])\n",
    "else:\n",
    "    best_result_feat = max(results, key=lambda x: x['test_acc'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST FEATURE CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Number of features: {best_result_feat['n_features']}\")\n",
    "print(f\"Test Accuracy: {best_result_feat['test_acc']:.4f} ({best_result_feat['test_acc']*100:.2f}%)\")\n",
    "print(f\"Train Accuracy: {best_result_feat['train_acc']:.4f} ({best_result_feat['train_acc']*100:.2f}%)\")\n",
    "print(f\"Test F1-Score: {best_result_feat['test_f1']:.4f}\")\n",
    "print(f\"\\nSelected features:\")\n",
    "for i, (idx, name) in enumerate(zip(best_result_feat['indices'], best_result_feat['feature_names']), 1):\n",
    "    print(f\"  {i:2d}. [{idx:2d}] {name}\")\n",
    "\n",
    "# Store best configuration\n",
    "best_feature_indices = best_result_feat['indices']\n",
    "best_n_features = best_result_feat['n_features']\n",
    "best_result = best_result_feat['result']\n",
    "\n",
    "# Accuracy check\n",
    "if best_result_feat['test_acc'] >= 0.60:\n",
    "    print(f\"\\n✓ Test accuracy ({best_result_feat['test_acc']*100:.2f}%) meets 60% threshold!\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Test accuracy ({best_result_feat['test_acc']*100:.2f}%) below 60% threshold\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Classification Reports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DETAILED CLASSIFICATION REPORT - TRAINING SET\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_train, best_result['predictions'], target_names=['Cold_normal', 'Hot']))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED CLASSIFICATION REPORT - TEST SET\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_test, best_result['test_predictions'], target_names=['Cold_normal', 'Hot']))\n",
    "\n",
    "# Confusion matrices\n",
    "print(\"\\nTraining Confusion Matrix:\")\n",
    "print(f\"                    Pred Cold_normal  Pred Hot\")\n",
    "cm_train = best_result['confusion_matrix']\n",
    "print(f\"True Cold_normal:   {cm_train[0, 0]:12}  {cm_train[0, 1]:8}\")\n",
    "print(f\"True Hot:           {cm_train[1, 0]:12}  {cm_train[1, 1]:8}\")\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(f\"                    Pred Cold_normal  Pred Hot\")\n",
    "cm_test = best_result['test_confusion_matrix']\n",
    "print(f\"True Cold_normal:   {cm_test[0, 0]:12}  {cm_test[0, 1]:8}\")\n",
    "print(f\"True Hot:           {cm_test[1, 0]:12}  {cm_test[1, 1]:8}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "X_train_best = X_train[:, best_feature_indices]\n",
    "X_test_best = X_test[:, best_feature_indices]\n",
    "X_train_scaled = best_result['scaler'].transform(X_train_best)\n",
    "X_test_scaled = best_result['scaler'].transform(X_test_best)\n",
    "\n",
    "# PCA for 2D visualization\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Training Set - Ground Truth\n",
    "ax1 = axes[0, 0]\n",
    "cold_mask_train = y_train == 'Cold_normal'\n",
    "hot_mask_train = y_train == 'Hot'\n",
    "ax1.scatter(X_train_pca[cold_mask_train, 0], X_train_pca[cold_mask_train, 1], \n",
    "           label='Cold_normal', alpha=0.7, s=50, color='blue', edgecolors='darkblue', linewidths=0.5)\n",
    "ax1.scatter(X_train_pca[hot_mask_train, 0], X_train_pca[hot_mask_train, 1], \n",
    "           label='Hot', alpha=0.7, s=50, color='red', edgecolors='darkred', linewidths=0.5)\n",
    "ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "ax1.set_title('Training Set - Ground Truth', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='best', fontsize=11)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# 2. Training Set - Predictions\n",
    "ax2 = axes[0, 1]\n",
    "pred_cold_train = best_result['predictions'] == 'Cold_normal'\n",
    "pred_hot_train = best_result['predictions'] == 'Hot'\n",
    "ax2.scatter(X_train_pca[pred_cold_train, 0], X_train_pca[pred_cold_train, 1], \n",
    "           label='Predicted Cold_normal', alpha=0.7, s=50, color='cyan', edgecolors='darkcyan', linewidths=0.5)\n",
    "ax2.scatter(X_train_pca[pred_hot_train, 0], X_train_pca[pred_hot_train, 1], \n",
    "           label='Predicted Hot', alpha=0.7, s=50, color='magenta', edgecolors='darkmagenta', linewidths=0.5)\n",
    "ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "ax2.set_title('Training Set - Predictions', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='best', fontsize=11)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# 3. Test Set - Correct vs Incorrect\n",
    "ax3 = axes[0, 2]\n",
    "cold_mask_test = y_test == 'Cold_normal'\n",
    "hot_mask_test = y_test == 'Hot'\n",
    "correct = (y_test == best_result['test_predictions'])\n",
    "ax3.scatter(X_test_pca[correct & cold_mask_test, 0], X_test_pca[correct & cold_mask_test, 1], \n",
    "           label='Cold_normal (correct)', alpha=0.7, s=50, color='blue', marker='o')\n",
    "ax3.scatter(X_test_pca[correct & hot_mask_test, 0], X_test_pca[correct & hot_mask_test, 1], \n",
    "           label='Hot (correct)', alpha=0.7, s=50, color='red', marker='o')\n",
    "incorrect = ~correct\n",
    "ax3.scatter(X_test_pca[incorrect, 0], X_test_pca[incorrect, 1], \n",
    "           label='Incorrect', alpha=0.9, s=80, color='black', marker='x', linewidths=2)\n",
    "ax3.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "ax3.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "ax3.set_title('Test Set - Predictions', fontsize=14, fontweight='bold')\n",
    "ax3.legend(loc='best', fontsize=11)\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# 4. Training Confusion Matrix\n",
    "ax4 = axes[1, 0]\n",
    "sns.heatmap(best_result['confusion_matrix'], annot=True, fmt='d', cmap='Blues', ax=ax4,\n",
    "            xticklabels=['Cold_normal', 'Hot'],\n",
    "            yticklabels=['Cold_normal', 'Hot'],\n",
    "            cbar_kws={'label': 'File Count'})\n",
    "ax4.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax4.set_ylabel('True Label', fontsize=12)\n",
    "ax4.set_title('Training Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 5. Test Confusion Matrix\n",
    "ax5 = axes[1, 1]\n",
    "sns.heatmap(best_result['test_confusion_matrix'], annot=True, fmt='d', cmap='Blues', ax=ax5,\n",
    "            xticklabels=['Cold_normal', 'Hot'],\n",
    "            yticklabels=['Cold_normal', 'Hot'],\n",
    "            cbar_kws={'label': 'File Count'})\n",
    "ax5.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax5.set_ylabel('True Label', fontsize=12)\n",
    "ax5.set_title('Test Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 6. Performance Comparison\n",
    "ax6 = axes[1, 2]\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "train_scores = [\n",
    "    best_result['train_accuracy'],\n",
    "    best_result['train_precision'],\n",
    "    best_result['train_recall'],\n",
    "    best_result['train_f1']\n",
    "]\n",
    "test_scores = [\n",
    "    best_result['test_accuracy'],\n",
    "    best_result['test_precision'],\n",
    "    best_result['test_recall'],\n",
    "    best_result['test_f1']\n",
    "]\n",
    "x_pos = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "ax6.bar(x_pos - width/2, train_scores, width, label='Train', color='blue', alpha=0.7)\n",
    "ax6.bar(x_pos + width/2, test_scores, width, label='Test', color='red', alpha=0.7)\n",
    "ax6.set_xticks(x_pos)\n",
    "ax6.set_xticklabels(metrics)\n",
    "ax6.set_ylabel('Score')\n",
    "ax6.set_title('Train vs Test Performance', fontsize=14, fontweight='bold')\n",
    "ax6.legend()\n",
    "ax6.set_ylim([0, 1])\n",
    "ax6.axhline(y=0.6, color='green', linestyle='--', alpha=0.5, label='60% threshold')\n",
    "ax6.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('gmm_classification_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved as 'gmm_classification_results.png'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Best Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model_package = {\n",
    "    'gmm_model': best_result['gmm'],\n",
    "    'scaler': best_result['scaler'],\n",
    "    'cluster_to_label_mapping': best_result['cluster_mapping'],\n",
    "    'feature_indices': best_feature_indices,\n",
    "    'feature_names': [feature_names[i] for i in best_feature_indices],\n",
    "    'aggregate_file_function': aggregate_file_to_features,\n",
    "    'n_clusters': 2,\n",
    "    'n_features': best_result['n_features'],\n",
    "    'covariance_type': best_result['covariance_type'],\n",
    "    'training_files': len(X_train),\n",
    "    'test_files': len(X_test),\n",
    "    'train_accuracy': best_result['train_accuracy'],\n",
    "    'train_precision': best_result['train_precision'],\n",
    "    'train_recall': best_result['train_recall'],\n",
    "    'train_f1': best_result['train_f1'],\n",
    "    'test_accuracy': best_result['test_accuracy'],\n",
    "    'test_precision': best_result['test_precision'],\n",
    "    'test_recall': best_result['test_recall'],\n",
    "    'test_f1': best_result['test_f1'],\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Save model\n",
    "model_filename = 'gmm_temperature_classifier_best.pkl'\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(model_package, f)\n",
    "\n",
    "print(f\"\\nModel saved: {model_filename}\")\n",
    "print(f\"\\nModel details:\")\n",
    "print(f\"  Features: {best_result['n_features']} selected features\")\n",
    "print(f\"  Feature indices: {best_feature_indices}\")\n",
    "print(f\"  Covariance type: {best_result['covariance_type']}\")\n",
    "print(f\"  Training accuracy: {best_result['train_accuracy']*100:.2f}%\")\n",
    "print(f\"  Test accuracy: {best_result['test_accuracy']*100:.2f}%\")\n",
    "print(f\"\\n✓ Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Total files: {len(X)}\")\n",
    "print(f\"  Training: {len(X_train)} files (70%)\")\n",
    "print(f\"  Test: {len(X_test)} files (30%)\")\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Algorithm: Gaussian Mixture Model (GMM)\")\n",
    "print(f\"  Components: 2 (Cold_normal, Hot)\")\n",
    "print(f\"  Covariance type: {best_result['covariance_type']}\")\n",
    "print(f\"  Features: {best_result['n_features']} (selected via Mutual Information)\")\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Training Accuracy: {best_result['train_accuracy']*100:.2f}%\")\n",
    "print(f\"  Test Accuracy: {best_result['test_accuracy']*100:.2f}%\")\n",
    "print(f\"  Test F1-Score: {best_result['test_f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nOutputs:\")\n",
    "print(f\"  Model file: gmm_temperature_classifier_best.pkl\")\n",
    "print(f\"  Visualization: gmm_classification_results.png\")\n",
    "\n",
    "if best_result['test_accuracy'] >= 0.60:\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"✓ SUCCESS: Test accuracy meets 60% threshold!\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"⚠ WARNING: Test accuracy below 60% threshold\")\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of notebook\n",
    "print(\"Notebook execution complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**End of Notebook**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell intentionally left blank (duplicate removed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Duplicate cell removed -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell intentionally left blank (duplicate removed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
